# -*- coding: utf-8 -*-
"""suc_translate_ai_ollama.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1d2S8oB1puDxFJTyDMlEju0I1ldtyM8TN
"""







!git clone https://github.com/mrseanryan/gpt-summarizer.git

!curl -fsSL https://ollama.com/install.sh | sh

!ollama run llama3.1:8b

!ollama list

!ollama run llama3.1:8b

print('Checking available Ollama models:')
!ollama list

!ollama pull phi3

!ollama run phi3:latest

print('Checking available Ollama models:')
!ollama list

print('Checking available Ollama models:')
!ollama list

!ollama run phi3:latest

!pip install cornsnake~=0.0.60 html2text==2024.2.26 json5==0.9.25 ollama==0.2.0 PyMuPDF==1.24.1 pyyaml==6.0.1 ruff==0.3.5

!pip3 install --upgrade ctransformers pymupdf

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/gpt-summarizer
!./go.sh

!poetry run python -m gpt_summarizer.main_cli

!python /content/gpt-summarizer/gpt_summarizer/main_cli.py --help

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/gpt-summarizer/gpt_summarizer

!python main_cli.py -h

# ÿ•ŸÜÿ¥ÿßÿ° ŸÖŸÑŸÅ ŸÜÿµŸä ÿ™ÿ¨ÿ±Ÿäÿ®Ÿä Ÿäÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ŸÜÿµ ÿ∑ŸàŸäŸÑ (ŸäŸÖŸÉŸÜŸÉ ÿ™ÿπÿØŸäŸÑ ÿßŸÑŸÜÿµ)
sample_text = """
Artificial intelligence (AI) is intelligence demonstrated by machines, as opposed to the natural intelligence displayed by animals including humans.
AI research has been defined as the field of study of intelligent agents, which refers to any system that perceives its environment and takes actions that maximize its chance of achieving its goals.
The term "artificial intelligence" had previously been used to describe machines that mimic and display "human" cognitive skills that are associated with the human mind, such as "learning" and "problem-solving".
This definition has since been rejected by major AI researchers who now describe AI in terms of rationality and acting rationally, which does not limit how intelligence can be articulated.
"""

with open("text.txt", "w") as f:
    f.write(sample_text)

print("ÿ™ŸÖ ÿ•ŸÜÿ¥ÿßÿ° ŸÖŸÑŸÅ text.txt ÿ®ŸÜÿ¨ÿßÿ≠")

!python -m gpt_summarizer --input_file text.txt

# Commented out IPython magic to ensure Python compatibility.
# %cd gpt-summarizer

import ollama

# ŸÇÿ±ÿßÿ°ÿ© ÿßŸÑŸÖŸÑŸÅ ÿßŸÑŸÜÿµŸä
with open("text.txt", "r") as f:
    text_content = f.read()

# ÿ•ÿ±ÿ≥ÿßŸÑ ÿßŸÑŸÜÿµ ÿ•ŸÑŸâ Llama 3.1 ŸÑŸÑÿ™ŸÑÿÆŸäÿµ
response = ollama.chat(model='llama3.1:8b', messages=[
  {
    'role': 'user',
    'content': f"Summarize this text: {text_content}",
  },
])

print("--- ÿßŸÑŸÖŸÑÿÆÿµ ŸÖŸÜ Llama 3.1 ---")
print(response['message']['content'])

import ollama

# ŸÇÿ±ÿßÿ°ÿ© ÿßŸÑŸÖŸÑŸÅ ÿßŸÑŸÜÿµŸä
with open("text.txt", "r") as f:
    text_content = f.read()

# ÿ•ÿ±ÿ≥ÿßŸÑ ÿßŸÑŸÜÿµ ÿ•ŸÑŸâ Llama 3.1 ŸÑŸÑÿ™ŸÑÿÆŸäÿµ
response = ollama.chat(model='llama3.1:8b', messages=[
  {
    'role': 'user',
    'content': f"ŸÑÿÆÿµ Ÿáÿ∞ÿß ÿßŸÑŸÜÿµ ÿ®ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ©: {text_content}",
  },
])

print("--- ÿßŸÑŸÖŸÑÿÆÿµ ŸÖŸÜ Llama 3.1 ---")
print(response['message']['content'])

import ollama

# ŸÇÿ±ÿßÿ°ÿ© ÿßŸÑŸÖŸÑŸÅ ÿßŸÑŸÜÿµŸä
with open("text.txt", "r") as f:
    text_content = f.read()

# ÿ•ÿ±ÿ≥ÿßŸÑ ÿßŸÑŸÜÿµ ÿ•ŸÑŸâ Llama 3.1 ŸÑŸÑÿ™ŸÑÿÆŸäÿµ
response = ollama.chat(model='llama3.1:8b', messages=[
  {
    'role': 'user',
    'content': f"ÿ™ÿ±ÿ¨ŸÖ Ÿáÿ∞ÿß ÿßŸÑŸÜÿµ ÿßŸÑŸâ ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ©: {text_content}",
  },
])

print("--- ÿßŸÑŸÖŸÑÿÆÿµ ŸÖŸÜ Llama 3.1 ---")
print(response['message']['content'])

"""--- ÿßŸÑŸÖŸÑÿÆÿµ ŸÖŸÜ Llama 3.1 ---
ÿßŸÑŸáŸÜÿØÿ≥ÿ© ÿßŸÑÿ¨ÿØŸäÿØÿ© (ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸä) ŸáŸä ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿ∞Ÿä Ÿäÿ∏Ÿáÿ± ŸÅŸä ÿßŸÑÿ¢ŸÑÿßÿ™ÿå ŸàŸÑŸäÿ≥ ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿ∑ÿ®ŸäÿπŸä ÿßŸÑŸÖŸèÿ∏ŸáŸéÿ± ŸÅŸä ÿßŸÑÿ≠ŸäŸàÿßŸÜÿßÿ™ ÿ®ŸÖÿß ŸÅŸä ÿ∞ŸÑŸÉ ÿßŸÑÿ•ŸÜÿ≥ÿßŸÜ. ŸàŸÇÿØ ÿ™ŸÖ ÿ™ÿπÿ±ŸäŸÅ ÿßŸÑÿ®ÿ≠ÿ´ ÿπŸÜ ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸä ÿπŸÑŸâ ÿ£ŸÜŸá ŸÖÿ¨ÿßŸÑ ÿØÿ±ÿßÿ≥ÿ©
"""

ÿßŸÉÿ™ÿ® Ÿáÿ∞ÿß (ŸÑŸÑÿ≠ÿµŸàŸÑ ÿπŸÑŸâ ŸÖŸÑÿÆÿµ ÿπÿ±ÿ®Ÿä):
code Python


'content': f"ŸÑÿÆÿµ Ÿáÿ∞ÿß ÿßŸÑŸÜÿµ ÿ®ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ©: {text_content}",



ÿ£Ÿà ÿ•ÿ∞ÿß ÿ£ÿ±ÿØÿ™Ÿá ŸÉŸÜŸÇÿßÿ∑ ŸÖÿ≠ÿØÿØÿ©:
code Python


'content': f"Give me a summary of this text as bullet points: {text_content}",

# Commented out IPython magic to ensure Python compatibility.
# 1. ÿßÿØÿÆŸÑ ÿ•ŸÑŸâ ÿßŸÑŸÖÿ¨ŸÑÿØ ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿä ÿ£ŸàŸÑÿßŸã
# %cd /content/gpt-summarizer

# 2. ÿ¥ÿ∫ŸÑ ÿßŸÑÿ£ŸÖÿ± Ÿàÿßÿ≥ÿ™ÿØÿπŸä ÿßŸÑŸÖŸàÿØŸäŸàŸÑ (ŸÑÿßÿ≠ÿ∏ ÿßŸÑŸÜŸÇÿ∑ÿ© ŸàÿßŸÑÿ¥ÿ±ÿ∑ÿ© ÿßŸÑÿ≥ŸÅŸÑŸäÿ©)
!python3 -m gpt_summarizer.main_cli text.txt --engine ollama --model llama3.1:8b







import ollama

# ŸÇÿ±ÿßÿ°ÿ© ÿßŸÑŸÜÿµ
with open("text.txt", "r") as f:
    text_content = f.read()

# ÿ•ÿ±ÿ≥ÿßŸÑ ÿßŸÑŸÜÿµ ŸÖÿπ ÿ™ÿπŸÑŸäŸÖÿßÿ™ ÿØŸÇŸäŸÇÿ© (System Prompt)
response = ollama.chat(model='llama3.1:8b', messages=[
  {
    'role': 'system',
    'content': 'ÿ£ŸÜÿ™ ÿÆÿ®Ÿäÿ± ŸÑÿ∫ŸàŸä ŸàŸÖÿ≠ŸÑŸÑ ŸÜÿµŸàÿµ. ŸÇŸÖ ÿ®ÿ™ŸÑÿÆŸäÿµ ÿßŸÑŸÜÿµ ÿßŸÑÿ™ÿßŸÑŸä ÿ®ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ÿßŸÑŸÅÿµÿ≠Ÿâ ÿßŸÑÿ≥ŸÑŸäŸÖÿ©ÿå Ÿàÿ±ŸÉÿ≤ ÿπŸÑŸâ ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ÿßŸÑÿ£ŸÅŸÉÿßÿ± ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿäÿ© ÿ®ÿ£ÿ≥ŸÑŸàÿ® ŸÖÿ™ÿ±ÿßÿ®ÿ∑ ŸàŸÑŸäÿ≥ ÿ™ÿ±ÿ¨ŸÖÿ© ÿ≠ÿ±ŸÅŸäÿ©.'
  },
  {
    'role': 'user',
    'content': text_content,
  },
])

print("--- ÿßŸÑŸÖŸÑÿÆÿµ ÿßŸÑŸÖÿ≠ÿ≥ŸÜ ---")
print(response['message']['content'])

import ollama
import os

# --- ÿ•ÿπÿØÿßÿØÿßÿ™ ---
input_file = "text.txt"           # ÿßÿ≥ŸÖ ÿßŸÑŸÖŸÑŸÅ ÿßŸÑÿ£ÿµŸÑŸä
output_file = "translated_text.txt" # ÿßÿ≥ŸÖ ŸÖŸÑŸÅ ÿßŸÑÿ™ÿ±ÿ¨ŸÖÿ©
chunk_size = 2000                 # ÿπÿØÿØ ÿßŸÑÿ£ÿ≠ÿ±ŸÅ ŸÅŸä ŸÉŸÑ ÿØŸÅÿπÿ© (ÿ≠ŸàÿßŸÑŸä 300-400 ŸÉŸÑŸÖÿ©)
model_name = "llama3.1:8b"        # ÿßŸÑŸÖŸàÿØŸäŸÑ ÿßŸÑŸÖÿ≥ÿ™ÿÆÿØŸÖ

# 1. ÿØÿßŸÑÿ© ŸÑÿ™ŸÇÿ≥ŸäŸÖ ÿßŸÑŸÜÿµ ÿ•ŸÑŸâ ÿ£ÿ¨ÿ≤ÿßÿ°
def split_text(text, size):
    # ŸÜŸÇÿ≥ŸÖ ÿßŸÑŸÜÿµ ÿ®ÿ≠Ÿäÿ´ ŸÑÿß ŸÜŸÇÿ∑ÿπ ÿßŸÑŸÉŸÑŸÖÿßÿ™ (ŸÜÿ≠ÿßŸàŸÑ ÿßŸÑÿ™ŸÇÿ≥ŸäŸÖ ÿπŸÜÿØ ÿßŸÑŸÖÿ≥ÿßŸÅÿßÿ™)
    chunks = []
    while len(text) > size:
        # ŸÜÿ¨ÿØ ÿ¢ÿÆÿ± ŸÖÿ≥ÿßŸÅÿ© ŸÇÿ®ŸÑ ÿßŸÑÿ≠ÿØ ÿßŸÑÿ£ŸÇÿµŸâ
        split_index = text.rfind(' ', 0, size)
        if split_index == -1: # ÿ•ÿ∞ÿß ŸÑŸÖ ÿ™Ÿàÿ¨ÿØ ŸÖÿ≥ÿßŸÅÿ© (ŸÉŸÑŸÖÿ© ÿ∑ŸàŸäŸÑÿ© ÿ¨ÿØÿßŸã)
            split_index = size

        chunks.append(text[:split_index])
        text = text[split_index:]
    chunks.append(text)
    return chunks

# 2. ŸÇÿ±ÿßÿ°ÿ© ÿßŸÑŸÖŸÑŸÅ ÿßŸÑÿ∑ŸàŸäŸÑ
if not os.path.exists(input_file):
    print(f"Error: File {input_file} not found!")
else:
    with open(input_file, "r", encoding="utf-8") as f:
        full_text = f.read()

    # ÿ™ŸÇÿ≥ŸäŸÖ ÿßŸÑŸÜÿµ
    text_chunks = split_text(full_text, chunk_size)
    print(f"ÿ™ŸÖ ÿ™ŸÇÿ≥ŸäŸÖ ÿßŸÑŸÖŸÑŸÅ ÿ•ŸÑŸâ {len(text_chunks)} ÿ¨ÿ≤ÿ° ŸÑŸÑÿ™ÿ±ÿ¨ŸÖÿ©...")

    # ÿ™ŸÜÿ∏ŸäŸÅ ŸÖŸÑŸÅ ÿßŸÑŸÖÿÆÿ±ÿ¨ÿßÿ™ ÿßŸÑŸÇÿØŸäŸÖ ÿ•ŸÜ Ÿàÿ¨ÿØ
    with open(output_file, "w", encoding="utf-8") as f:
        f.write("")

    # 3. ÿßŸÑÿ®ÿØÿ° ŸÅŸä ÿßŸÑÿ™ÿ±ÿ¨ŸÖÿ© (ÿ¨ÿ≤ÿ° ÿ®ÿ¨ÿ≤ÿ°)
    for i, chunk in enumerate(text_chunks):
        print(f"ÿ¨ÿßÿ±Ÿä ÿ™ÿ±ÿ¨ŸÖÿ© ÿßŸÑÿ¨ÿ≤ÿ° {i+1} ŸÖŸÜ {len(text_chunks)}...")

        try:
            response = ollama.chat(model=model_name, messages=[
                {
                    'role': 'system',
                    'content': 'ÿ£ŸÜÿ™ ŸÖÿ™ÿ±ÿ¨ŸÖ ŸÖÿ≠ÿ™ÿ±ŸÅ. ŸÇŸÖ ÿ®ÿ™ÿ±ÿ¨ŸÖÿ© ÿßŸÑŸÜÿµ ÿßŸÑÿ™ÿßŸÑŸä ÿ•ŸÑŸâ ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ÿ®ÿØŸÇÿ© Ÿàÿ£ÿ≥ŸÑŸàÿ® ÿ£ÿØÿ®Ÿä ŸÖÿ™ŸÖÿßÿ≥ŸÉ. ŸÑÿß ÿ™ÿ∂ŸÅ ÿ£Ÿä ŸÖŸÇÿØŸÖÿßÿ™ ÿ£Ÿà ÿ¥ÿ±Ÿàÿ≠ÿßÿ™ÿå ŸÅŸÇÿ∑ ÿ£ÿπÿ∑ŸÜŸä ÿßŸÑÿ™ÿ±ÿ¨ŸÖÿ©.'
                },
                {
                    'role': 'user',
                    'content': chunk
                },
            ])

            translated_chunk = response['message']['content']

            # ÿ≠ŸÅÿ∏ ÿßŸÑÿ™ÿ±ÿ¨ŸÖÿ© ŸÖÿ®ÿßÿ¥ÿ±ÿ© ŸÅŸä ÿßŸÑŸÖŸÑŸÅ (append)
            with open(output_file, "a", encoding="utf-8") as f:
                f.write(translated_chunk + "\n\n")

        except Exception as e:
            print(f"ÿ≠ÿØÿ´ ÿÆÿ∑ÿ£ ŸÅŸä ÿßŸÑÿ¨ÿ≤ÿ° {i+1}: {e}")

    print(f"\n‚úÖ ÿ™ŸÖÿ™ ÿßŸÑÿ™ÿ±ÿ¨ŸÖÿ© ÿ®ŸÜÿ¨ÿßÿ≠! ÿßŸÑŸÖŸÑŸÅ ÿßŸÑŸÖÿ≠ŸÅŸàÿ∏: {output_file}")

    # ÿπÿ±ÿ∂ ÿ£ŸàŸÑ ÿ¨ÿ≤ÿ° ŸÖŸÜ ÿßŸÑÿ™ÿ±ÿ¨ŸÖÿ© ŸÑŸÑÿ™ÿ£ŸÉÿØ
    print("\n--- ŸÖŸÇÿ™ÿ∑ŸÅ ŸÖŸÜ ÿßŸÑÿ™ÿ±ÿ¨ŸÖÿ© ---")
    with open(output_file, "r", encoding="utf-8") as f:
        print(f.read()[:500] + "...")

# ÿØÿßÿÆŸÑ ÿ≠ŸÑŸÇÿ© ÿßŸÑŸÄ loop
        response = ollama.chat(
            model=model_name,
            messages=[
                {
                    'role': 'system',
                    'content': 'ÿ£ŸÜÿ™ ŸÖÿ™ÿ±ÿ¨ŸÖ ÿ™ŸÇŸÜŸä ŸÖÿ≠ÿ™ÿ±ŸÅ. ŸÇŸÖ ÿ®ÿ™ÿ±ÿ¨ŸÖÿ© ÿßŸÑŸÜÿµ ÿ•ŸÑŸâ ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ÿ®ÿ£ÿ≥ŸÑŸàÿ® ÿπŸÑŸÖŸä ÿ±ÿµŸäŸÜ. ÿßŸÜÿ™ÿ®Ÿá ŸÑŸÑŸÖÿµÿ∑ŸÑÿ≠ÿßÿ™ ÿßŸÑÿ™ŸÇŸÜŸäÿ© (ŸÖÿ´ŸÑÿßŸã: Agents = ŸàŸÉŸÑÿßÿ° ÿ∞ŸÉŸäÿ©ÿå Learning = ÿßŸÑÿ™ÿπŸÑŸÖÿå Problem-solving = ÿ≠ŸÑ ÿßŸÑŸÖÿ¥ŸÉŸÑÿßÿ™). ÿ™ÿ¨ŸÜÿ® ÿßŸÑÿ™ÿ±ÿ¨ŸÖÿ© ÿßŸÑÿ≠ÿ±ŸÅŸäÿ© ŸàÿßŸÉÿ™ÿ® ÿ®ŸÑÿ∫ÿ© ÿπÿ±ÿ®Ÿäÿ© ÿ≥ŸÑŸäŸÖÿ© ŸÜÿ≠ŸàŸäÿßŸã.'
                },
                {
                    'role': 'user',
                    'content': chunk
                },
            ],
            # ÿ•ÿ∂ÿßŸÅÿ© ÿÆŸäÿßÿ±ÿßÿ™ ŸÑÿ™ŸÇŸÑŸäŸÑ ÿßŸÑÿπÿ¥Ÿàÿßÿ¶Ÿäÿ© ŸàŸÖŸÜÿπ ÿßŸÑŸáŸÑŸàÿ≥ÿ©
            options={
                'temperature': 0.1,  # ÿ™ŸÇŸÑŸäŸÑ ÿßŸÑÿ≠ÿ±ÿßÿ±ÿ© ŸÑÿ≤ŸäÿßÿØÿ© ÿßŸÑÿØŸÇÿ©
                'top_p': 0.9
            }
        )

import ollama
import os
import time
from tqdm.notebook import tqdm  # ÿ¥ÿ±Ÿäÿ∑ ÿ™ŸÇÿØŸÖ ŸÑÿ®Ÿäÿ¶ÿ© ŸÉŸàŸÑÿßÿ®

# ------------------- ÿ•ÿπÿØÿßÿØÿßÿ™ ÿßŸÑŸÖÿ≥ÿ™ÿÆÿØŸÖ -------------------
INPUT_FILE = "text.txt"           # ÿßÿ≥ŸÖ ÿßŸÑŸÖŸÑŸÅ ÿßŸÑÿ∞Ÿä ÿ™ÿ±ŸäÿØ ÿ™ÿ±ÿ¨ŸÖÿ™Ÿá
OUTPUT_FILE = "translated_text.txt" # ÿßÿ≥ŸÖ ÿßŸÑŸÖŸÑŸÅ ÿßŸÑŸÜÿßÿ™ÿ¨
MODEL_NAME = "llama3.1:8b"        # ÿßŸÑŸÖŸàÿØŸäŸÑ ÿßŸÑŸÖÿ≥ÿ™ÿÆÿØŸÖ
CHUNK_SIZE = 1500                 # ÿ≠ÿ¨ŸÖ ÿßŸÑÿ¨ÿ≤ÿ° (ÿ™ŸÖ ÿ™ŸÇŸÑŸäŸÑŸá ŸÇŸÑŸäŸÑÿßŸã ŸÑÿ∂ŸÖÿßŸÜ ÿ¨ŸàÿØÿ© ÿ£ÿπŸÑŸâ)
# --------------------------------------------------------

def split_text_smart(text, max_length):
    """
    ÿ™ŸÇÿ≥ŸäŸÖ ÿßŸÑŸÜÿµ ÿ®ÿ∞ŸÉÿßÿ° ÿπŸÜÿØ ÿßŸÑŸÖÿ≥ÿßŸÅÿßÿ™ ÿ£Ÿà ÿπŸÑÿßŸÖÿßÿ™ ÿßŸÑÿ™ÿ±ŸÇŸäŸÖ ŸÑÿπÿØŸÖ ŸÇÿ∑ÿπ ÿßŸÑÿ¨ŸÖŸÑ.
    """
    chunks = []
    while len(text) > max_length:
        # ÿßŸÑÿ®ÿ≠ÿ´ ÿπŸÜ ÿ£ŸÇÿ±ÿ® ŸÜŸÇÿ∑ÿ© ÿ£Ÿà ŸÖÿ≥ÿßŸÅÿ© ŸÇÿ®ŸÑ ÿßŸÑÿ≠ÿØ ÿßŸÑÿ£ŸÇÿµŸâ
        # ŸÜÿ≠ÿßŸàŸÑ ÿßŸÑÿ®ÿ≠ÿ´ ÿπŸÜ ŸÜŸáÿßŸäÿ© ÿ¨ŸÖŸÑÿ© (.) ÿ£ŸàŸÑÿßŸã ŸÑŸäŸÉŸàŸÜ ÿßŸÑÿ™ŸÇÿ≥ŸäŸÖ ŸÖŸÜÿ∑ŸÇŸäÿßŸã
        split_index = text.rfind('.', 0, max_length)

        if split_index == -1: # ÿ•ÿ∞ÿß ŸÑŸÖ ŸÜÿ¨ÿØ ŸÜŸÇÿ∑ÿ©ÿå ŸÜÿ®ÿ≠ÿ´ ÿπŸÜ ŸÖÿ≥ÿßŸÅÿ©
            split_index = text.rfind(' ', 0, max_length)

        if split_index == -1: # ÿ•ÿ∞ÿß ŸÉÿßŸÜÿ™ ÿ¨ŸÖŸÑÿ© ÿπŸÖŸÑÿßŸÇÿ© ÿ¨ÿØÿßŸãÿå ŸÜŸÇÿ∑ÿπ ÿπŸÜÿØ ÿßŸÑÿ≠ÿØ ÿßŸÑÿ£ŸÇÿµŸâ
            split_index = max_length

        # ÿ•ÿ∂ÿßŸÅÿ© ÿßŸÑÿ¨ÿ≤ÿ° ŸÑŸÑŸÇÿßÿ¶ŸÖÿ© (ŸÖÿπ ÿßÿ≠ÿ™ÿ≥ÿßÿ® ÿßŸÑŸÜŸÇÿ∑ÿ© ŸÅŸä ÿßŸÑÿ¨ÿ≤ÿ° ÿßŸÑÿ£ŸàŸÑ)
        chunks.append(text[:split_index + 1])
        text = text[split_index + 1:].strip()

    if text:
        chunks.append(text)
    return chunks

def translate_segment(text_chunk, retries=3):
    """
    ÿØÿßŸÑÿ© ŸÑÿ™ÿ±ÿ¨ŸÖÿ© ÿ¨ÿ≤ÿ° Ÿàÿßÿ≠ÿØ ŸÖÿπ ŸÖÿ≠ÿßŸàŸÑÿ© ÿ•ÿπÿßÿØÿ© ÿßŸÑŸÖÿ≠ÿßŸàŸÑÿ© ÿπŸÜÿØ ÿßŸÑŸÅÿ¥ŸÑ
    """
    system_prompt = (
        "ÿ£ŸÜÿ™ ŸÖÿ™ÿ±ÿ¨ŸÖ ÿ™ŸÇŸÜŸä Ÿàÿ£ŸÉÿßÿØŸäŸÖŸä ŸÖÿ≠ÿ™ÿ±ŸÅ. "
        "ŸÖŸáŸÖÿ™ŸÉ: ÿ™ÿ±ÿ¨ŸÖÿ© ÿßŸÑŸÜÿµ ÿßŸÑÿ™ÿßŸÑŸä ŸÖŸÜ ÿßŸÑÿ•ŸÜÿ¨ŸÑŸäÿ≤Ÿäÿ© ÿ•ŸÑŸâ ÿßŸÑÿπÿ±ÿ®Ÿäÿ© ÿßŸÑŸÅÿµÿ≠Ÿâ ÿßŸÑÿ≥ŸÑŸäŸÖÿ©. "
        "ÿßŸÑŸÇŸàÿßÿπÿØ: "
        "1. ÿßÿ≥ÿ™ÿÆÿØŸÖ ŸÖÿµÿ∑ŸÑÿ≠ÿßÿ™ ÿπŸÑŸÖŸäÿ© ÿØŸÇŸäŸÇÿ© (ŸÖÿ´ŸÑÿßŸã: Agents = ŸàŸÉŸÑÿßÿ°ÿå AI = ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸä). "
        "2. ÿ≠ÿßŸÅÿ∏ ÿπŸÑŸâ ŸÇŸàÿßÿπÿØ ÿßŸÑŸÜÿ≠Ÿà (ÿ™ÿ∞ŸÉŸäÿ± Ÿàÿ™ÿ£ŸÜŸäÿ´ ÿßŸÑÿ£ŸÅÿπÿßŸÑ ÿ®ÿØŸÇÿ©). "
        "3. ŸÑÿß ÿ™ÿ∂ŸÅ ÿ£Ÿä ÿ≠Ÿàÿßÿ± ÿ¨ÿßŸÜÿ®Ÿäÿå ÿ£ÿπÿ∑ŸÜŸä ÿßŸÑÿ™ÿ±ÿ¨ŸÖÿ© ŸÅŸÇÿ∑."
    )

    for attempt in range(retries):
        try:
            response = ollama.chat(
                model=MODEL_NAME,
                messages=[
                    {'role': 'system', 'content': system_prompt},
                    {'role': 'user', 'content': text_chunk},
                ],
                options={
                    'temperature': 0.1,  # ÿØŸÇÿ© ÿπÿßŸÑŸäÿ© ÿ¨ÿØÿßŸã ŸàŸÖŸÜÿπ ÿßŸÑŸáŸÑŸàÿ≥ÿ©
                    'top_p': 0.9,
                }
            )
            return response['message']['content']
        except Exception as e:
            print(f"\n‚ö†Ô∏è ÿ™ÿ≠ÿ∞Ÿäÿ±: ŸÅÿ¥ŸÑÿ™ ÿßŸÑŸÖÿ≠ÿßŸàŸÑÿ© {attempt+1} ŸÑÿ™ÿ±ÿ¨ŸÖÿ© ÿßŸÑÿ¨ÿ≤ÿ°. ÿßŸÑÿÆÿ∑ÿ£: {e}")
            time.sleep(2) # ÿßŸÜÿ™ÿ∏ÿßÿ± ÿ´ÿßŸÜŸäÿ™ŸäŸÜ ŸÇÿ®ŸÑ ÿßŸÑŸÖÿ≠ÿßŸàŸÑÿ© ŸÖÿ±ÿ© ÿ£ÿÆÿ±Ÿâ

    return "[ŸÅÿ¥ŸÑÿ™ ÿ™ÿ±ÿ¨ŸÖÿ© Ÿáÿ∞ÿß ÿßŸÑÿ¨ÿ≤ÿ° ÿ®ÿπÿØ ÿπÿØÿ© ŸÖÿ≠ÿßŸàŸÑÿßÿ™]"

# ------------------- ÿßŸÑÿ™ŸÜŸÅŸäÿ∞ ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿä -------------------

# 1. ÿßŸÑÿ™ÿ£ŸÉÿØ ŸÖŸÜ Ÿàÿ¨ŸàÿØ ÿßŸÑŸÖŸÑŸÅ
if not os.path.exists(INPUT_FILE):
    print(f"‚ùå ÿÆÿ∑ÿ£: ÿßŸÑŸÖŸÑŸÅ {INPUT_FILE} ÿ∫Ÿäÿ± ŸÖŸàÿ¨ŸàÿØ! ÿ™ÿ£ŸÉÿØ ŸÖŸÜ ÿ±ŸÅÿπ ÿßŸÑŸÖŸÑŸÅ Ÿàÿ™ÿ≥ŸÖŸäÿ™Ÿá ÿ®ÿ¥ŸÉŸÑ ÿµÿ≠Ÿäÿ≠.")
else:
    print("üöÄ ÿ¨ÿßÿ±Ÿä ŸÇÿ±ÿßÿ°ÿ© ÿßŸÑŸÖŸÑŸÅ Ÿàÿ™ŸÇÿ≥ŸäŸÖŸá...")
    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        full_text = f.read()

    # ÿ™ŸÇÿ≥ŸäŸÖ ÿßŸÑŸÜÿµ
    chunks = split_text_smart(full_text, CHUNK_SIZE)
    print(f"üìÑ ÿ™ŸÖ ÿ™ŸÇÿ≥ŸäŸÖ ÿßŸÑŸÜÿµ ÿ•ŸÑŸâ {len(chunks)} ÿ¨ÿ≤ÿ°. ÿßŸÑÿ®ÿØÿ° ŸÅŸä ÿßŸÑÿ™ÿ±ÿ¨ŸÖÿ©...\n")

    # ŸÖÿ≥ÿ≠ ŸÖÿ≠ÿ™ŸàŸâ ŸÖŸÑŸÅ ÿßŸÑŸÖÿÆÿ±ÿ¨ÿßÿ™ ÿßŸÑŸÇÿØŸäŸÖ ŸÑÿ®ÿØÿ° ŸÉÿ™ÿßÿ®ÿ© ÿ¨ÿØŸäÿØÿ©
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write("")

    # 2. ÿ≠ŸÑŸÇÿ© ÿßŸÑÿ™ÿ±ÿ¨ŸÖÿ© ŸÖÿπ ÿ¥ÿ±Ÿäÿ∑ ÿßŸÑÿ™ŸÇÿØŸÖ
    # ŸÜÿ≥ÿ™ÿÆÿØŸÖ tqdm ŸÑÿπÿ±ÿ∂ ÿ¥ÿ±Ÿäÿ∑ ÿßŸÑÿ™ÿ≠ŸÖŸäŸÑ
    for i, chunk in enumerate(tqdm(chunks, desc="ÿ¨ÿßÿ±Ÿä ÿßŸÑÿ™ÿ±ÿ¨ŸÖÿ©")):

        translated_text = translate_segment(chunk)

        # ŸÉÿ™ÿßÿ®ÿ© ÿßŸÑŸÜÿ™Ÿäÿ¨ÿ© ŸÅŸàÿ±ÿßŸã ŸÑŸÑŸÖŸÑŸÅ (ÿ≠ÿ™Ÿâ ŸÑÿß Ÿäÿ∂Ÿäÿπ ÿ¥Ÿäÿ° ŸÑŸà ÿßŸÜŸÇÿ∑ÿπ ÿßŸÑÿßÿ™ÿµÿßŸÑ)
        with open(OUTPUT_FILE, "a", encoding="utf-8") as f:
            f.write(translated_text + "\n\n")

    print(f"\n‚úÖ ÿ™ŸÖÿ™ ÿßŸÑÿπŸÖŸÑŸäÿ© ÿ®ŸÜÿ¨ÿßÿ≠! ÿ™ŸÖ ÿ≠ŸÅÿ∏ ÿßŸÑÿ™ÿ±ÿ¨ŸÖÿ© ŸÅŸä: {OUTPUT_FILE}")

    # ÿπÿ±ÿ∂ ÿπŸäŸÜÿ©
    print("-" * 30)
    print("üëÄ ÿπŸäŸÜÿ© ŸÖŸÜ ÿ®ÿØÿßŸäÿ© ÿßŸÑÿ™ÿ±ÿ¨ŸÖÿ©:")
    with open(OUTPUT_FILE, "r", encoding="utf-8") as f:
        print(f.read()[:500] + "...")
    print("-" * 30)

import ollama
import os
import time
from tqdm.notebook import tqdm # Colab Progress Bar

# ------------------- User Settings -------------------
INPUT_FILE = "text.txt" # Name of the file you want to translate
OUTPUT_FILE = "translated_text.txt" # Name of the output file
MODEL_NAME = "llama3.1:8b" # Model used
CHUNK_SIZE = 1500 # Chunk size (slightly reduced for better quality)
# --------------------------------------------------------

def split_text_smart(text, max_length):

""
Smartly splits text at spaces or punctuation marks to avoid breaking sentences.

""
chunks = []

while len(text) > max_length:

# Finds the nearest point or space before the maximum

# Attempts to find the end of a sentence (.) first for logical splitting
split_index = text.rfind('.', 0, max_length)

if split_index == -1: # If we don't find a point, we look for a distance

split_index = text.rfind(' ', 0, max_length)

if split_index == -1: # If the sentence is too long, we cut at the maximum length

split_index = max_length

# Add the segment to the list (counting the point in the first segment)

chunks.append(text[:split_index + 1])

text = text[split_index + 1:].strip()

if text:

chunks.append(text)

return chunks

def translate_segment(text_chunk, retries=3):

""

A function to translate one segment and attempt to retry if it fails

""
system_prompt = (
"You are a professional technical and academic translator."

"Your task: Translate the following text from English into Modern Standard Arabic."
Rules:

1. Use precise scientific terminology (e.g., Agents = Agents, AI = Artificial Intelligence).

2. Maintain grammatical rules (precise gender agreement of verbs).

3. Do not add any side dialogue; just provide the translation.

)

for attempt in range(retries):

try:

response = ollama.chat(
model=MODEL_NAME,

messages=[
{'role': 'system', 'content': system_prompt},

{'role': 'user', 'content': text_chunk},

],

options={
'temperature': 0.1, # Very high accuracy and hallucination prevention

'top_p': 0.9,

}
)

return response['message']['content']

except Exception as e:

print(f"\n‚ö†Ô∏è Warning: The attempt to translate the part failed. Error: {e}")

time.sleep(2) # Wait two seconds before trying again

return "[This part failed to translate after several attempts]"

# ------------------- Main Implementation -------------------

# 1. Check if the file exists
if not os.path.exists(INPUT_FILE):

print(f"‚ùå Error: The file {INPUT_FILE} does not exist! Make sure you uploaded and named the file correctly.")
else:

print("üöÄ Reading and splitting the file...")

with open(INPUT_FILE, "r", encoding="utf-8") as f:

full_text = f.read()

# Split the text

chunks = split_text_smart(full_text, CHUNK_SIZE)

print(f"üìÑ The text has been split into {len(chunks)} chunks. Starting to translate...")

# Clear the contents of the old output file to start writing a new one
with open(OUTPUT_FILE, "w", encoding="utf-8") as f:

f.write("")

# 2. Translation Loop with Progress Bar

# We use tqdm to display the progress bar

for i, chunk in enumerate(tqdm(chunks, desc="Translating in progress")):

translated_text = translate_segment(chunk)

# Write the result immediately to the file (so nothing is lost if the connection is interrupted)

with open(OUTPUT_FILE, "a", encoding="utf-8") as f:

f.write(translated_text + "\n\n")

print(f"\n‚úÖ Operation completed successfully! Translation saved in: {OUTPUT_FILE}")

# Display Sample

print("-" * 30)

print("üëÄ Sample from the beginning of the translation:")

with open(OUTPUT_FILE, "r", encoding="utf-8") as f:

print(f.read()[:500] + "...")

print("-" * 30)

import ollama
import os
import time
from tqdm.notebook import tqdm  # Progress bar for Colab

# ------------------- Configuration -------------------
INPUT_FILE = "/content/gpt-summarizer/gpt_summarizer/text.txt"           # Source file name
OUTPUT_FILE = "translated_text.txt" # Output file name
MODEL_NAME = "llama3.1:8b"        # Model to use
CHUNK_SIZE = 1500                 # Character limit per chunk
# -----------------------------------------------------

def split_text_smart(text, max_length):
    """
    Splits text intelligently at periods (.) or spaces to avoid cutting sentences in half.
    """
    chunks = []
    while len(text) > max_length:
        # Find the last period before the max length to keep sentences intact
        split_index = text.rfind('.', 0, max_length)

        # If no period is found, look for the last space
        if split_index == -1:
            split_index = text.rfind(' ', 0, max_length)

        # If the chunk is one giant string (no spaces), cut at max_length
        if split_index == -1:
            split_index = max_length

        # Append the chunk including the punctuation
        chunks.append(text[:split_index + 1])
        text = text[split_index + 1:].strip()

    if text:
        chunks.append(text)
    return chunks

def translate_segment(text_chunk, retries=3):
    """
    Translates a single text chunk with retry logic in case of failure.
    """
    # The system prompt is in English, but instructs the AI to output Arabic.
    system_prompt = (
        "You are an expert technical and academic translator. "
        "Your task is to translate the following English text into professional, high-quality Arabic. "
        "Guidelines: "
        "1. Use accurate technical terms (e.g., translate 'Agents' correctly in an AI context). "
        "2. Ensure perfect Arabic grammar. "
        "3. Output ONLY the translation, without any introductory or concluding remarks."
    )

    for attempt in range(retries):
        try:
            response = ollama.chat(
                model=MODEL_NAME,
                messages=[
                    {'role': 'system', 'content': system_prompt},
                    {'role': 'user', 'content': text_chunk},
                ],
                options={
                    'temperature': 0.1,  # Low temperature for high accuracy/low hallucination
                    'top_p': 0.9,
                }
            )
            return response['message']['content']
        except Exception as e:
            print(f"\n[Warning] Attempt {attempt+1} failed. Error: {e}")
            time.sleep(2) # Wait 2 seconds before retrying

    return "[Failed to translate this segment after multiple attempts]"

# ------------------- Main Execution -------------------

def main():
    # 1. Check if input file exists
    if not os.path.exists(INPUT_FILE):
        print(f"[Error] File '{INPUT_FILE}' not found! Please upload it first.")
        return

    print("--- Starting Process ---")
    print(f"Reading file: {INPUT_FILE}...")

    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        full_text = f.read()

    # 2. Split text into chunks
    chunks = split_text_smart(full_text, CHUNK_SIZE)
    print(f"Document split into {len(chunks)} parts. Starting translation...\n")

    # Clear previous output file content
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write("")

    # 3. Process chunks with a progress bar
    for chunk in tqdm(chunks, desc="Translating"):

        translated_text = translate_segment(chunk)

        # Append result immediately to file (safeguard against crashes)
        with open(OUTPUT_FILE, "a", encoding="utf-8") as f:
            f.write(translated_text + "\n\n")

    print(f"\n[Success] Translation complete!")
    print(f"Output saved to: {OUTPUT_FILE}")

    # 4. Show a sample
    print("-" * 30)
    print("Sample of the output:")
    if os.path.exists(OUTPUT_FILE):
        with open(OUTPUT_FILE, "r", encoding="utf-8") as f:
            print(f.read()[:500] + "...")
    print("-" * 30)

if __name__ == "__main__":
    main()

"""--- Starting Process ---
Reading file: /content/gpt-summarizer/gpt_summarizer/text.txt...
Document split into 1 parts. Starting translation...

Translating:‚Äá100%
‚Äá1/1‚Äá[04:48<00:00,‚Äá288.13s/it]


[Success] Translation complete!
Output saved to: translated_text.txt
## ------------------------------
Sample of the output:
ÿßŸÑÿ™ÿπŸÑŸÖ ÿßŸÑÿ•ŸÑŸÉÿ™ÿ±ŸàŸÜŸä (AI) ŸáŸà ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿ∞Ÿä Ÿäÿ∏Ÿáÿ±Ÿá ÿßŸÑÿ¢ŸÑÿßÿ™ÿå ÿπŸÑŸâ ÿπŸÉÿ≥ ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿ∑ÿ®ŸäÿπŸä ÿßŸÑÿ∞Ÿä Ÿäÿ™ŸÖ.displayed ŸÖŸÜ ŸÇÿ®ŸÑ ÿßŸÑÿ≠ŸäŸàÿßŸÜÿßÿ™ ÿ®ŸÖÿß ŸÅŸä ÿ∞ŸÑŸÉ ÿßŸÑÿ®ÿ¥ÿ±.

ÿ™ŸÖ ÿ™ÿπÿ±ŸäŸÅ ÿßŸÑÿ®ÿ≠ÿ´ ÿπŸÜ ÿßŸÑÿ™ÿπŸÑŸÖ ÿßŸÑÿ•ŸÑŸÉÿ™ÿ±ŸàŸÜŸä ÿπŸÑŸâ ÿ£ŸÜŸá ŸÖÿ¨ÿßŸÑ ÿØÿ±ÿßÿ≥ÿ© ‡§è‡§úŸÜÿ≥Ÿäÿßÿ™ ÿ∞ŸÉŸäÿ©ÿå ŸàÿßŸÑÿ™Ÿä ÿ™ÿ¥Ÿäÿ± ÿ•ŸÑŸâ ÿ£Ÿä ŸÜÿ∏ÿßŸÖ ŸäŸÖŸÉŸÜŸá ÿ£ŸÜ ŸäÿØÿ±ŸÉ ÿ®Ÿäÿ¶ÿ™Ÿá ŸàŸäÿ™ÿÆÿ∞ ÿ•ÿ¨ÿ±ÿßÿ°ÿßÿ™ ÿ™ÿ≤ŸäÿØ ŸÅÿ±ÿµ ÿ™ÿ≠ŸÇŸäŸÇ ÿ£ŸáÿØÿßŸÅŸá.
ÿßÿ≥ÿ™ÿÆÿØŸÖ ÿßŸÑŸÖÿµÿ∑ŸÑÿ≠ "ÿßŸÑÿ™ÿπŸÑŸÖ ÿßŸÑÿ•ŸÑŸÉÿ™ÿ±ŸàŸÜŸä" ÿ≥ÿßÿ®ŸÇŸãÿß ŸÑŸàÿµŸÅ ÿßŸÑÿ¢ŸÑÿßÿ™ ÿßŸÑÿ™Ÿä ÿ™ŸÖÿßÿ´ŸÑ Ÿàÿ™ÿ∏Ÿáÿ± ŸÖŸáÿßÿ±ÿßÿ™ ÿßŸÑŸÖÿπÿ±ŸÅŸäÿ© ÿßŸÑÿ®ÿ¥ÿ±Ÿäÿ©ÿå ŸÖÿ´ŸÑ "ÿ™ÿπŸÑŸÖ" Ÿà " ÿ≠ŸÑ ÿßŸÑŸÖÿ¥ŸÉŸÑÿßÿ™".
ÿ™ŸÖ ÿ±ŸÅÿ∂ Ÿáÿ∞ÿß ÿßŸÑÿ™ÿπÿ±ŸäŸÅ ŸÖŸÜÿ∞ ÿ∞ŸÑŸÉ ÿßŸÑÿ≠ŸäŸÜ ŸÖŸÜ ŸÇÿ®ŸÑ ÿ®ÿßÿ≠ÿ´Ÿä AI ÿßŸÑŸÉÿ®ÿßÿ± ÿßŸÑÿ∞ŸäŸÜ ŸäÿµŸÅŸàŸÜ ÿßŸÑÿ¢ŸÜ ÿßŸÑÿ™ÿπŸÑŸÖ ÿßŸÑÿ•ŸÑŸÉÿ™ÿ±ŸàŸÜŸä ÿ®ÿßŸÑ ŸÖ...
# ------------------
"""

import ollama
import os
import time
from tqdm.notebook import tqdm  # Progress bar

# ------------------- Configuration -------------------
INPUT_FILE = "text.txt"
OUTPUT_FILE = "translated_text.txt"
MODEL_NAME = "llama3.1:8b"
CHUNK_SIZE = 1000                 # Reduced chunk size for better focus
# -----------------------------------------------------

def split_text_smart(text, max_length):
    """
    Splits text intelligently at periods or newlines.
    """
    chunks = []
    while len(text) > max_length:
        split_index = text.rfind('.', 0, max_length)
        if split_index == -1:
            split_index = text.rfind('\n', 0, max_length)
        if split_index == -1:
            split_index = text.rfind(' ', 0, max_length)
        if split_index == -1:
            split_index = max_length

        chunks.append(text[:split_index + 1])
        text = text[split_index + 1:].strip()

    if text:
        chunks.append(text)
    return chunks

def translate_segment(text_chunk, retries=3):
    """
    Translates text with strict enforcement of Arabic terminology.
    """

    # STRICT SYSTEM PROMPT
    # We explicitly map the terms that were failing (AI -> ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸä)
    # We explicitly forbid non-Arabic characters.
    system_prompt = (
        "You are a professional translator. Translate the text from English to Arabic. \n"
        "STRICT RULES:\n"
        "1. Terminology: Translate 'Artificial Intelligence' strictly as 'ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸä'. NEVER translate it as 'E-learning'.\n"
        "2. Terminology: Translate 'Agents' as 'ŸàŸÉŸÑÿßÿ°' or 'ÿ£ŸÜÿ∏ŸÖÿ©'.\n"
        "3. Output Language: The output must be 100% Arabic. Do NOT use English words (like 'displayed').\n"
        "4. Character Set: Do NOT use Hindi or Urdu characters (like '‡§è'). Use only standard Arabic letters.\n"
        "5. Grammar: Use formal, correct Arabic grammar."
    )

    for attempt in range(retries):
        try:
            response = ollama.chat(
                model=MODEL_NAME,
                messages=[
                    {'role': 'system', 'content': system_prompt},
                    {'role': 'user', 'content': text_chunk},
                ],
                options={
                    'temperature': 0.1,  # Keep it deterministic
                    'top_p': 0.85,       # Slightly stricter sampling
                    'num_predict': 512,  # Ensure enough space for output
                }
            )
            return response['message']['content']
        except Exception as e:
            print(f"\n[Warning] Attempt {attempt+1} failed. Error: {e}")
            time.sleep(2)

    return "[Translation Failed]"

# ------------------- Main Execution -------------------

def main():
    if not os.path.exists(INPUT_FILE):
        print(f"[Error] File '{INPUT_FILE}' not found.")
        return

    print("--- Starting Process (Strict Mode) ---")

    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        full_text = f.read()

    chunks = split_text_smart(full_text, CHUNK_SIZE)
    print(f"Document split into {len(chunks)} parts.")

    # Reset output file
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write("")

    for chunk in tqdm(chunks, desc="Translating"):
        translated_text = translate_segment(chunk)

        with open(OUTPUT_FILE, "a", encoding="utf-8") as f:
            f.write(translated_text + "\n\n")

    print(f"\n[Success] Translation complete!")

    # Show sample
    print("-" * 30)
    if os.path.exists(OUTPUT_FILE):
        with open(OUTPUT_FILE, "r", encoding="utf-8") as f:
            print(f.read()[:500] + "...")
    print("-" * 30)

if __name__ == "__main__":
    main()

"""ÿπŸàÿØÿ© f'''SYSTEM: ÿ£ŸÜÿ™ ŸÖÿ≠ŸÑŸÑ ŸÜÿµŸàÿµ ŸÖŸÅŸäÿØ Ÿäÿπÿ±ŸÅ ŸÉŸäŸÅŸäÿ© ÿ™ŸÑÿÆŸäÿµ ÿßŸÑŸÜÿµ ÿ•ŸÑŸâ ÿµÿ≠Ÿäÿ≠ {get_output_format_name()}.
ÿßŸÑŸÖÿ≥ÿ™ÿÆÿØŸÖ: ÿ™ŸÑÿÆŸäÿµ Ÿáÿ∞ÿß ÿßŸÑŸÜÿµ ÿßŸÑŸÖÿ¥ÿßÿ± ÿ•ŸÑŸäŸá ÿ®ÿßŸÑÿÆŸÑŸÅŸäÿßÿ™:
"""



"""### https://github.com/mrseanryan/gpt-summarizer/blob/9dd9ae6bf70de990294a14d28d027821e5e530f1/gpt_summarizer/prompts.py#L5

# yaml ÿ£ÿ±ÿÆÿµ ŸÑÿ™ŸàŸÑŸäÿØ
OUTPUT_FORMAT_YAML = '''
Ÿäÿ¨ÿ® ÿ£ŸÜ ŸäŸÉŸàŸÜ ÿ™ŸÜÿ≥ŸäŸÇ ÿßŸÑÿ•ÿÆÿ±ÿßÿ¨ ÿµÿßŸÑÿ≠Ÿãÿß YAML ÿå ŸÖÿπ ÿßŸÑÿ≠ŸÇŸàŸÑ: title_in_quotes ÿå short_summary ÿå long_summary ÿå ÿßŸÑŸÅŸÇÿ±ÿßÿ™.
- ŸÑÿß ÿ™ŸÇŸÖ ÿ®ÿ™ÿ∂ŸÖŸäŸÜ ÿ£ÿ≠ÿ±ŸÅ YAML ÿßŸÑÿÆÿßÿµÿ© ŸÅŸä ÿßŸÑŸÜÿµ (ÿπŸÑŸâ ÿ≥ÿ®ŸäŸÑ ÿßŸÑŸÖÿ´ÿßŸÑ: ÿßŸÇÿ™ÿ®ÿßÿ≥ÿßÿ™ ŸÖŸÅÿ±ÿØÿ© ÿ£Ÿà ŸÇŸàŸÑŸàŸÜ)
- ŸÑÿß ÿ™ÿ≥ÿ™ÿÆÿØŸÖ ŸÖÿ¥ÿ∫ŸÑ ÿßÿ≥ÿ™ŸÖÿ±ÿßÿ± ÿßŸÑÿÆÿ∑ '|'
- ŸÑŸÑÿ±ÿµÿßÿµ ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ÿßŸÑŸàÿßÿµŸÑÿ© '-'ÿå ŸÑÿß ÿ™ÿ≥ÿ™ÿÆÿØŸÖ '*'
- ÿØŸÑÿßŸÑÿ© ÿπŸÑŸâ ÿßŸÑŸÜÿßÿ™ÿ¨ ÿßŸÑÿ•ÿ¨ŸÖÿßŸÑŸä ŸÖÿπ "" ŸÑÿß "--"
'''

OUTPUT_FORMAT_YAML_SHORTER = '''
The output format must be valid YAML, with the fields: title_in_quotes, short_summary, long_summary.
- do NOT include YAML special characters in the text (for example: single quotes or colons)
- do NOT use the line-continuation operator '|'
- for bullets use hyphen '-', do NOT use '*'
- denote the overall output with ``` NOT '---'
'''

OUTPUT_TEXT_STYLE = "The output text preserve the original style and tone. The summary MUST summarize the contents of the text, this is NOT a commentary."

SYSTEM_PROMPT__OPENAI = f"You are a summary assistant, skilled in summarizing texts whilst preserving the main points and original style. The target language is {config.TARGET_LANGUAGE}."

OUTPUT_TEXT_STYLE = "ŸÜÿµ ÿßŸÑÿ•ÿÆÿ±ÿßÿ¨ Ÿäÿ≠ÿßŸÅÿ∏ ÿπŸÑŸâ ÿßŸÑŸÜŸÖÿ∑ ÿßŸÑÿ£ÿµŸÑŸä ŸàÿßŸÑŸÜÿ∫ŸÖÿ©. Ÿäÿ¨ÿ® ÿ£ŸÜ ŸäŸÑÿÆÿµ ÿßŸÑŸÖŸÑÿÆÿµ ŸÖÿ≠ÿ™ŸàŸäÿßÿ™ ÿßŸÑŸÜÿµÿå Ÿáÿ∞ÿß ŸÑŸäÿ≥ ÿ™ÿπŸÑŸäŸÇÿß".

SYSTEM_PROMPT__OPENAI = f"You ŸÖÿ≥ÿßÿπÿØ ŸÖŸÑÿÆÿµÿå ŸÖÿßŸáÿ± ŸÅŸä ÿ™ŸÑÿÆŸäÿµ ÿßŸÑŸÜÿµŸàÿµ ŸÖÿπ ÿßŸÑÿ≠ŸÅÿßÿ∏ ÿπŸÑŸâ ÿßŸÑŸÜŸÇÿßÿ∑ ÿßŸÑÿ±ÿ¶Ÿäÿ≥Ÿäÿ© ŸàÿßŸÑÿ£ÿ≥ŸÑŸàÿ® ÿßŸÑÿ£ÿµŸÑŸä. ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑŸÖÿ≥ÿ™ŸáÿØŸÅÿ© ŸáŸä {config.TARGET_LANGUAGE}.

PARAGRAPHS = "\n    - paragraphs should be an array of one sentence summaries: one sentence for each paragraph.\n"

    return f'''

return f'''
1. Analyze the given input text.
    - The input text is delimited by triple backticks.
2. {_get_output_format(include_paragraphs=include_paragraphs)}
3. Create a title and a short and long summary in the target language {target_language}.
    - {OUTPUT_TEXT_STYLE}
    - short_summary should be {config.SHORT_SUMMARY_WORD_COUNT} words long.
    - long_summary should be {config.LONG_SUMMARY_WORD_COUNT} words long.{PARAGRAPHS}
4. After generating the summaries, stop and check that the output is valid {get_output_format_name()}.

ÿßŸÑÿπŸàÿØÿ© f '''
1. ÿ™ÿ≠ŸÑŸäŸÑ ŸÜÿµ ÿßŸÑÿ•ÿØÿÆÿßŸÑ ÿßŸÑŸÖÿπÿ∑Ÿâ.
    - Ÿäÿ™ŸÖ ÿ™ÿπŸäŸäŸÜ ŸÜÿµ ÿßŸÑÿ•ÿØÿÆÿßŸÑ ÿ®Ÿàÿßÿ≥ÿ∑ÿ© ÿÆŸÑŸÅŸäÿßÿ™ ÿ´ŸÑÿßÿ´Ÿäÿ©.
2. {_get_output_format(include_paragraphs=include_paragraphs)}
3. ÿ•ŸÜÿ¥ÿßÿ° ÿπŸÜŸàÿßŸÜ ŸàŸÖŸÑÿÆÿµ ŸÇÿµŸäÿ± Ÿàÿ∑ŸàŸäŸÑ ŸÅŸä ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑŸáÿØŸÅ {target_language}.
    - {OUTPUT_TEXT_STYLE}
    - Ÿäÿ¨ÿ® ÿ£ŸÜ ŸäŸÉŸàŸÜ {config.SHORT_SUMMARY_WORD_COUNT} ŸÉŸÑŸÖÿßÿ™ ÿ∑ŸàŸäŸÑÿ©.
    - Ÿäÿ¨ÿ® ÿ£ŸÜ ŸäŸÉŸàŸÜ {config.LONG_SUMMARY_WORD_COUNT} ŸÉŸÑŸÖÿßÿ™ ÿ∑ŸàŸäŸÑÿ©.{ ÿßŸÑŸÅŸÇÿ±ÿßÿ™}
4. ÿ®ÿπÿØ ÿ•ŸÜÿ¥ÿßÿ° ÿßŸÑŸÖŸÑÿÆÿµÿßÿ™ ÿå ÿ™ŸàŸÇŸÅ Ÿàÿ™ÿ≠ŸÇŸÇ ŸÖŸÜ ÿ£ŸÜ ÿßŸÑÿ•ÿÆÿ±ÿßÿ¨ ÿµÿßŸÑÿ≠ {get_output_format_name()}.
"""



"""        PARAGRAPHS = "\n        - paragraphs should be an array of one sentence shortened-versions: one sentence for each paragraph.\n"

    print("[complex prompt]")
    return f'''Examine the provided user prompt, the text input, noting its style of writing and tone.

RULES:
R1. Do NOT mention the text, study, document or paper.
R2. Create shorted versions of the original text, in the same style.
R3. Preserve the 'person' or 'narrator' of the text: for example, if the text is written in first-person, then also output in first-person.

PROCESS TO FOLLOW:
    1. Analyze the given input text, noting its style and tone.
        - The input text is delimited by triple backticks.
    2. {_get_output_format(include_paragraphs=include_paragraphs)}
    3. Create a title and a short and long version in the target language {target_language}.
        - {OUTPUT_TEXT_STYLE}
        - short_summary should be {config.SHORT_SUMMARY_WORD_COUNT} words long, in the original style.
        - long_summary should be {config.LONG_SUMMARY_WORD_COUNT} words long, in the original style.{PARAGRAPHS}
        - shortened texts should be as if excerpts of the original text. example: 'While recent language models have the ability to take long contexts as input, relatively little is known about how well the language models
use longer context.' -> 'Whilst recent language models can accept long contexts, little is know about the quality of output'.
    4. Do NOT output a commentary - do NOT use phrases such as 'The paper examines...', 'It is noted...' or 'This text'.
    5. After generating the shortened texts, stop and check that the output is valid {get_output_format_name()}.

<thinking>
For each generated shortened text:
- check does the generated text follow the RULES.
- check is the writing style and tone same as original.
- check is the shortened text as if part of the original text NOT a commentary.
- check the shortened text is direct, as a primary source, and not a commentary.
- do NOT mention the text, study, document or paper.
- do NOT output a commentary - do NOT use phrases such as 'The paper examines...', 'It is noted...' or 'The text discusses...' or 'This text lists...' or 'The text...' or 'The study...'.
- avoid duplication
[Continue for all items]
</thinking>

text: ```{input_text}```

"""

import ollama
import os
import time
from tqdm.notebook import tqdm  # Progress bar

# ------------------- Configuration -------------------
INPUT_FILE = "/content/gpt-summarizer/gpt_summarizer/text.txt"
OUTPUT_FILE = "translated_text.txt"
MODEL_NAME = "llama3.1:8b"
CHUNK_SIZE = 1000                 # Reduced chunk size for better focus
# -----------------------------------------------------

def split_text_smart(text, max_length):
    """
    Splits text intelligently at periods or newlines.
    """
    chunks = []
    while len(text) > max_length:
        split_index = text.rfind('.', 0, max_length)
        if split_index == -1:
            split_index = text.rfind('\n', 0, max_length)
        if split_index == -1:
            split_index = text.rfind(' ', 0, max_length)
        if split_index == -1:
            split_index = max_length

        chunks.append(text[:split_index + 1])
        text = text[split_index + 1:].strip()

    if text:
        chunks.append(text)
    return chunks

def translate_segment(text_chunk, retries=3):
    """
    Translates text with strict enforcement of Arabic terminology.
    """

    # STRICT SYSTEM PROMPT
    # We explicitly map the terms that were failing (AI -> ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸä)
    # We explicitly forbid non-Arabic characters.
    system_prompt = (
        "You are a professional translator. Translate the text from English to Arabic. \n"
        "STRICT RULES:\n"
        "1. Terminology: Translate 'Artificial Intelligence' strictly as 'ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸä'. NEVER translate it as 'E-learning'.\n"
        "2. Terminology: Translate 'Agents' as 'ŸàŸÉŸÑÿßÿ°' or 'ÿ£ŸÜÿ∏ŸÖÿ©'.\n"
        "3. Output Language: The output must be 100% Arabic. Do NOT use English words (like 'displayed').\n"
        "4. Character Set: Do NOT use Hindi or Urdu characters (like '‡§è'). Use only standard Arabic letters.\n"
        "5. Grammar: Use formal, correct Arabic grammar."
    )

    for attempt in range(retries):
        try:
            response = ollama.chat(
                model=MODEL_NAME,
                messages=[
                    {'role': 'system', 'content': system_prompt},
                    {'role': 'user', 'content': text_chunk},
                ],
                options={
                    'temperature': 0.1,  # Keep it deterministic
                    'top_p': 0.85,       # Slightly stricter sampling
                    'num_predict': 512,  # Ensure enough space for output
                }
            )
            return response['message']['content']
        except Exception as e:
            print(f"\n[Warning] Attempt {attempt+1} failed. Error: {e}")
            time.sleep(2)

    return "[Translation Failed]"

# ------------------- Main Execution -------------------

def main():
    if not os.path.exists(INPUT_FILE):
        print(f"[Error] File '{INPUT_FILE}' not found.")
        return

    print("--- Starting Process (Strict Mode) ---")

    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        full_text = f.read()

    chunks = split_text_smart(full_text, CHUNK_SIZE)
    print(f"Document split into {len(chunks)} parts.")

    # Reset output file
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write("")

    for chunk in tqdm(chunks, desc="Translating"):
        translated_text = translate_segment(chunk)

        with open(OUTPUT_FILE, "a", encoding="utf-8") as f:
            f.write(translated_text + "\n\n")

    print(f"\n[Success] Translation complete!")

    # Show sample
    print("-" * 30)
    if os.path.exists(OUTPUT_FILE):
        with open(OUTPUT_FILE, "r", encoding="utf-8") as f:
            print(f.read()[:500] + "...")
    print("-" * 30)

if __name__ == "__main__":
    main()

"""--- Starting Process (Strict Mode) ---
Document split into 1 parts.

Translating:‚Äá100%
‚Äá1/1‚Äá[05:26<00:00,‚Äá326.45s/it]


[Success] Translation complete!
------------------------------
ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸä ŸáŸà ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿ∞Ÿä Ÿäÿ∏Ÿáÿ±Ÿá ÿßŸÑÿ¢ŸÑÿßÿ™ÿå ÿπŸÑŸâ ÿπŸÉÿ≥ ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿ∑ÿ®ŸäÿπŸä ÿßŸÑÿ∞Ÿä Ÿäÿ™ŸÖ.displaying ÿ®Ÿàÿßÿ≥ÿ∑ÿ© ÿßŸÑÿ≠ŸäŸàÿßŸÜÿßÿ™ ÿ®ŸÖÿß ŸÅŸä ÿ∞ŸÑŸÉ ÿßŸÑÿ®ÿ¥ÿ±.

ŸàŸÇÿØ ÿ™ŸÖ ÿ™ÿπÿ±ŸäŸÅ ÿßŸÑÿ®ÿ≠ÿ´ ÿπŸÜ ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸä ŸÉŸÖÿ¨ÿßŸÑ ÿØÿ±ÿßÿ≥ÿ© ÿßŸÑŸàŸÉŸÑÿßÿ° ÿßŸÑÿ∞ŸÉŸäŸäŸÜÿå ŸàÿßŸÑÿ∞Ÿä Ÿäÿ¥Ÿäÿ± ÿ•ŸÑŸâ ÿ£Ÿä ŸÜÿ∏ÿßŸÖ ŸäŸÖŸÉŸÜŸá ÿ£ŸÜ ŸäÿØÿ±ŸÉ ÿ®Ÿäÿ¶ÿ™Ÿá ŸàŸäÿ™ÿÆÿ∞ ÿ•ÿ¨ÿ±ÿßÿ°ÿßÿ™ ÿ™ÿ≤ŸäÿØ ŸÖŸÜ ŸÅÿ±ÿµ ÿ™ÿ≠ŸÇŸäŸÇ ÿ£ŸáÿØÿßŸÅŸá.
ÿ™ŸÖ ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ŸÖÿµÿ∑ŸÑÿ≠ "ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸä" ÿ≥ÿßÿ®ŸÇŸãÿß ŸÑŸàÿµŸÅ ÿßŸÑÿ¢ŸÑÿßÿ™ ÿßŸÑÿ™Ÿä ÿ™ŸÖÿ´ŸÑ Ÿàÿ™ÿ∏Ÿáÿ± ŸÖŸáÿßÿ±ÿßÿ™ ÿßŸÑŸÖÿπÿ±ŸÅŸäÿ© ÿßŸÑÿ®ÿ¥ÿ±Ÿäÿ©ÿå ŸÖÿ´ŸÑ "ÿßŸÑÿ™ÿπŸÑŸÖ" ŸàÿßŸÑ " ÿ≠ŸÑ ÿßŸÑŸÖÿ¥ŸÉŸÑÿßÿ™".
ŸàŸÇÿØ ÿ™ŸÖ ÿ±ŸÅÿ∂ Ÿáÿ∞ÿß ÿßŸÑÿ™ÿπÿ±ŸäŸÅ ŸÖŸÜÿ∞ ÿ∞ŸÑŸÉ ÿßŸÑÿ≠ŸäŸÜ ŸÖŸÜ ŸÇÿ®ŸÑ ÿ®ÿßÿ≠ÿ´Ÿä ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸä ÿßŸÑÿ£ÿ®ÿ±ÿ≤ ÿßŸÑÿ∞ŸäŸÜ ŸäÿµŸÅŸàŸÜ ÿßŸÑÿ¢ŸÜ ÿßŸÑÿ∞ŸÉ...
------------------------------
### New Section
"""



import ollama
import os
import time
from tqdm.notebook import tqdm

# ------------------- Configuration -------------------
INPUT_FILE = "/content/gpt-summarizer/gpt_summarizer/text.txt"
OUTPUT_FILE = "output_arabic.txt"
MODEL_NAME = "llama3.1:8b"
CHUNK_SIZE = 1200  # Good balance for Llama 3 context

# ------------------- PROMPT ENGINEERING -------------------
# Inspired by: mrseanryan/gpt-summarizer/prompts.py
# We define "Personas" to guide the model strictly.

PROMPT_TEMPLATES = {
    "tech_translation": (
        "ROLE: You are an expert Technical Writer and Linguist.\n"
        "TASK: Translate the following English text into professional Arabic.\n"
        "GUIDELINES:\n"
        "1. Accuracy: Preserve technical meaning. Do not dumb it down.\n"
        "2. Terminology: Use standard Arabic technical terms (e.g., 'Artificial Intelligence' -> 'ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸä').\n"
        "3. Tone: Formal, objective, and concise. No marketing fluff.\n"
        "4. STRICT OUTPUT: Output ONLY the Arabic translation. No English words. No Hindi characters.\n"
        "5. Formatting: Maintain the original paragraph structure."
    ),
    "tech_summary": (
        "ROLE: You are an expert Technical Writer.\n"
        "TASK: Summarize the key points of the following text in Arabic.\n"
        "GUIDELINES:\n"
        "1. Format: Use bullet points (-).\n"
        "2. Focus: Extract the most important technical facts and arguments.\n"
        "3. Brevity: Be concise but comprehensive.\n"
        "4. Language: The output must be 100% in Arabic."
    )
}

# Select which prompt to use here:
SELECTED_PROMPT = PROMPT_TEMPLATES["tech_translation"]
# Change to PROMPT_TEMPLATES["tech_summary"] if you want a summary instead.

# --------------------------------------------------------

def split_text_smart(text, max_length):
    """Splits text without breaking sentences."""
    chunks = []
    while len(text) > max_length:
        # Try splitting at paragraph end
        split_index = text.rfind('\n', 0, max_length)
        # If no paragraph, try sentence end
        if split_index == -1:
            split_index = text.rfind('.', 0, max_length)
        # If no sentence, try space
        if split_index == -1:
            split_index = text.rfind(' ', 0, max_length)
        # Last resort: hard cut
        if split_index == -1:
            split_index = max_length

        chunks.append(text[:split_index + 1])
        text = text[split_index + 1:].strip()

    if text:
        chunks.append(text)
    return chunks

def process_segment(text_chunk, retries=3):
    """Sends the chunk to Ollama with the selected engineering prompt."""

    for attempt in range(retries):
        try:
            response = ollama.chat(
                model=MODEL_NAME,
                messages=[
                    {'role': 'system', 'content': SELECTED_PROMPT},
                    {'role': 'user', 'content': text_chunk},
                ],
                options={
                    'temperature': 0.1, # Low temp = High precision (Technical Writer style)
                    'top_p': 0.9,
                }
            )
            return response['message']['content']
        except Exception as e:
            print(f"\n[Warning] Attempt {attempt+1} failed. Error: {e}")
            time.sleep(2)

    return "[Error: Segment failed to process]"

def main():
    if not os.path.exists(INPUT_FILE):
        print(f"[Error] File '{INPUT_FILE}' not found.")
        return

    print("--- Starting Process with 'Expert Technical Writer' Persona ---")

    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        full_text = f.read()

    chunks = split_text_smart(full_text, CHUNK_SIZE)
    print(f"Document split into {len(chunks)} parts.")

    # Clear output file
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write("")

    # Process chunks
    for chunk in tqdm(chunks, desc="Processing"):
        result = process_segment(chunk)

        with open(OUTPUT_FILE, "a", encoding="utf-8") as f:
            f.write(result + "\n\n")

    print(f"\n[Success] Task complete! Saved to: {OUTPUT_FILE}")

    # Preview
    print("-" * 30)
    print("Output Preview:")
    if os.path.exists(OUTPUT_FILE):
        with open(OUTPUT_FILE, "r", encoding="utf-8") as f:
            print(f.read()[:500] + "...")
    print("-" * 30)

if __name__ == "__main__":
    main()

"""--- Starting Process with 'Expert Technical Writer' Persona ---
Document split into 1 parts.

Processing:‚Äá100%
‚Äá1/1‚Äá[05:35<00:00,‚Äá335.66s/it]


[Success] Task complete! Saved to: output_arabic.txt
------------------------------
Output Preview:
ŸäŸèÿπÿ±ŸéŸÅ ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸä (AI) ÿ®ÿ£ŸÜŸá ÿßŸÑÿ≠ŸÉŸÖÿ© ÿßŸÑÿ™Ÿä ÿ™ÿ∏Ÿáÿ±Ÿáÿß ÿßŸÑÿ¢ŸÑÿßÿ™ÿå ÿπŸÑŸâ ÿπŸÉÿ≥ ÿßŸÑÿ≠ŸÉŸÖÿ© ÿßŸÑÿ∑ÿ®ŸäÿπŸäÿ© ÿßŸÑÿ™Ÿä ÿ™ÿ™ŸÖŸäÿ≤ ÿ®Ÿáÿß ÿßŸÑÿ≠ŸäŸàÿßŸÜÿßÿ™ ÿ®ŸÖÿß ŸÅŸä ÿ∞ŸÑŸÉ ÿßŸÑÿ®ÿ¥ÿ±.

ŸàŸÇÿØ ÿ™ŸÖ ÿ™ÿπÿ±ŸäŸÅ ÿßŸÑÿ®ÿ≠ÿ´ ÿπŸÜ ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸä ÿ®ÿ£ŸÜŸáÿß ŸÖÿ¨ÿßŸÑ ÿØÿ±ÿßÿ≥ÿ© ‡§è‡§úŸÜÿ≥Ÿäÿßÿ™ ÿ∞ŸÉŸäÿ©ÿå ŸàÿßŸÑÿ™Ÿä ÿ™ÿ¥Ÿäÿ± ÿ•ŸÑŸâ ÿ£Ÿä ŸÜÿ∏ÿßŸÖ ŸäÿØÿ±ŸÉ ÿ®Ÿäÿ¶ÿ™Ÿá ŸàŸäÿ™ÿÆÿ∞ ÿ•ÿ¨ÿ±ÿßÿ°ÿßÿ™ ÿ™ÿ≤ŸäÿØ ŸÖŸÜ ŸÅÿ±ÿµ ÿ™ÿ≠ŸÇŸäŸÇ ÿ£ŸáÿØÿßŸÅŸá.

ŸàŸÖÿπ ÿ∞ŸÑŸÉÿå ÿßÿ≥ÿ™ŸèÿÆÿØŸÖ ÿßŸÑŸÖÿµÿ∑ŸÑÿ≠ "ÿ∞ŸÉÿßÿ° ÿßÿµÿ∑ŸÜÿßÿπŸä" ÿ≥ÿßÿ®ŸÇŸãÿß ŸÑŸàÿµŸÅ ÿßŸÑÿ¢ŸÑÿßÿ™ ÿßŸÑÿ™Ÿä ÿ™ŸÇŸÑÿØ Ÿàÿ™ÿ∏Ÿáÿ± ŸÖŸáÿßÿ±ÿßÿ™ËÆ§ŸÖŸäÿ© ÿ®ÿ¥ÿ±Ÿäÿ© ŸÖÿ±ÿ™ÿ®ÿ∑ÿ© ÿ®ÿßŸÑÿØŸÖÿßÿ∫ ÿßŸÑÿ®ÿ¥ÿ±Ÿäÿå ŸÖÿ´ŸÑ "ÿßŸÑÿ™ÿπŸÑŸÖ" Ÿà " ÿ≠ŸÑ ÿßŸÑŸÖÿ¥ŸÉŸÑÿßÿ™".

ŸàŸÇÿØ ÿ™ŸÖ ÿ±ŸÅÿ∂ Ÿáÿ∞ÿß ÿßŸÑÿ™ÿπÿ±ŸäŸÅ ŸÖŸÜÿ∞ ÿ∞ŸÑŸÉ ÿßŸÑÿ≠ŸäŸÜ ŸÖŸÜ ŸÇÿ®ŸÑ ÿ®ÿßÿ≠ÿ´Ÿä ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸä ÿßŸÑÿ®ÿßÿ±ÿ≤ŸäŸÜ ÿßŸÑÿ∞...
------------------------------
### New Section

https://github.com/mrseanryan/gpt-summarizer/blob/9dd9ae6bf70de990294a14d28d027821e5e530f1/gpt_summarizer/prompts.py#L5
"""

import ollama
import os
import time
from tqdm.notebook import tqdm

# ------------------- Configuration -------------------
INPUT_FILE = "text.txt"
OUTPUT_FILE = "output_arabic_v2.txt"
MODEL_NAME = "llama3.1:8b"
CHUNK_SIZE = 1000  # Smaller chunks reduce hallucination risks

# ------------------- ENGINEERED PROMPT (Anti-Glitch) -------------------
# Updated based on your request + constraints to fix Hindi/Chinese glitches

PROMPT_TEMPLATES = {
    "strict_tech_translation": (
        "ROLE: You are an expert Computer Science Translator specialized in AI.\n"
        "TASK: Translate the text to Modern Standard Arabic (Fusha).\n\n"
        "CRITICAL RULES (MUST FOLLOW):\n"
        "1. NO GLITCHES: Do NOT use Hindi characters (e.g., '‡§è'), Chinese (e.g., 'ËÆ§'), or mixed scripts. Use ONLY Arabic letters.\n"
        "2. TERMINOLOGY MAPPING:\n"
        "   - 'Artificial Intelligence' -> 'ÿßŸÑÿ∞ŸÉÿßÿ° ÿßŸÑÿßÿµÿ∑ŸÜÿßÿπŸä' (NOT 'ÿ≠ŸÉŸÖÿ©' or 'ÿ™ÿπŸÑŸÖ ÿ•ŸÑŸÉÿ™ÿ±ŸàŸÜŸä').\n"
        "   - 'Intelligence' -> 'ÿ∞ŸÉÿßÿ°'.\n"
        "   - 'Agents' -> 'ŸàŸÉŸÑÿßÿ°' or 'ŸÉŸäÿßŸÜÿßÿ™' (NOT '‡§è‡§úŸÜÿ≥Ÿäÿßÿ™').\n"
        "   - 'Cognitive' -> 'ÿ•ÿØÿ±ÿßŸÉŸäÿ©' or 'ŸÖÿπÿ±ŸÅŸäÿ©'.\n"
        "3. STYLE: Academic, concise, and professional.\n"
        "4. OUTPUT: Provide the Arabic translation ONLY. No intro, no notes."
    )
}

SELECTED_PROMPT = PROMPT_TEMPLATES["strict_tech_translation"]

# --------------------------------------------------------

def split_text_smart(text, max_length):
    chunks = []
    while len(text) > max_length:
        split_index = text.rfind('\n', 0, max_length)
        if split_index == -1: split_index = text.rfind('.', 0, max_length)
        if split_index == -1: split_index = text.rfind(' ', 0, max_length)
        if split_index == -1: split_index = max_length

        chunks.append(text[:split_index + 1])
        text = text[split_index + 1:].strip()

    if text:
        chunks.append(text)
    return chunks

def process_segment(text_chunk, retries=3):
    for attempt in range(retries):
        try:
            response = ollama.chat(
                model=MODEL_NAME,
                messages=[
                    {'role': 'system', 'content': SELECTED_PROMPT},
                    {'role': 'user', 'content': text_chunk},
                ],
                options={
                    'temperature': 0.3,  # Increased slightly to avoid "Hindi token trap"
                    'top_p': 0.9,
                    'repeat_penalty': 1.1, # Penalize repetition/glitches
                    'num_predict': 2048,   # Allow enough space for output
                }
            )

            content = response['message']['content']

            # Simple check: If output contains Hindi/Chinese chars, retry
            # (Very basic filter logic)
            if any("\u0900" <= c <= "\u097F" for c in content): # Hindi block
                raise ValueError("Detected Hindi characters in output!")

            return content

        except Exception as e:
            print(f"\n[Warning] Attempt {attempt+1} failed/rejected. Reason: {e}")
            time.sleep(1)

    return "[Error: Translation Failed]"

def main():
    if not os.path.exists(INPUT_FILE):
        print(f"[Error] File '{INPUT_FILE}' not found.")
        return

    print("--- Starting Process (Strict Anti-Glitch Mode) ---")

    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        full_text = f.read()

    chunks = split_text_smart(full_text, CHUNK_SIZE)
    print(f"Document split into {len(chunks)} parts.")

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write("")

    for chunk in tqdm(chunks, desc="Translating"):
        result = process_segment(chunk)
        with open(OUTPUT_FILE, "a", encoding="utf-8") as f:
            f.write(result + "\n\n")

    print(f"\n[Success] Task complete! Saved to: {OUTPUT_FILE}")

    print("-" * 30)
    if os.path.exists(OUTPUT_FILE):
        with open(OUTPUT_FILE, "r", encoding="utf-8") as f:
            print(f.read()[:600] + "...")
    print("-" * 30)

if __name__ == "__main__":
    main()

!pip install pdfplumber

import pdfplumber

def extract_text_from_pdf(pdf_path, output_txt_path):
    """
    ÿ™ÿ≥ÿ™ÿÆÿ±ÿ¨ ÿßŸÑŸÜÿµ ŸÖŸÜ ŸÖŸÑŸÅ PDF Ÿàÿ™ÿ≠ŸÅÿ∏Ÿá ŸÅŸä ŸÖŸÑŸÅ ŸÜÿµŸä ÿ®ŸÜŸÅÿ≥ ÿßŸÑÿ™ŸÜÿ≥ŸäŸÇ ÿßŸÑÿ™ŸÇÿ±Ÿäÿ®Ÿä (ŸÅŸÇÿ±ÿßÿ™ Ÿàÿ≥ÿ∑Ÿàÿ±).

    :param pdf_path: ŸÖÿ≥ÿßÿ± ŸÖŸÑŸÅ PDF ÿßŸÑÿ£ÿµŸÑŸä
    :param output_txt_path: ŸÖÿ≥ÿßÿ± ÿßŸÑŸÖŸÑŸÅ ÿßŸÑŸÜÿµŸä ÿßŸÑŸÖÿ∑ŸÑŸàÿ® ÿ≠ŸÅÿ∏Ÿá
    """
    extracted_text = []

    try:
        with pdfplumber.open(pdf_path) as pdf:
            for page_num, page in enumerate(pdf.pages, start=1):
                # ÿßÿ≥ÿ™ÿÆÿ±ÿßÿ¨ ÿßŸÑŸÜÿµ ŸÖÿπ ÿßŸÑÿ≠ŸÅÿßÿ∏ ÿπŸÑŸâ ÿßŸÑÿ≥ÿ∑Ÿàÿ±
                text = page.extract_text()
                if text:
                    extracted_text.append(text)
                    # ÿ•ÿ∂ÿßŸÅÿ© ŸÅÿßÿµŸÑ ÿ®ŸäŸÜ ÿßŸÑÿµŸÅÿ≠ÿßÿ™ (ÿßÿÆÿ™Ÿäÿßÿ±Ÿä)
                    extracted_text.append("\n" + "="*50 + f" ÿµŸÅÿ≠ÿ© {page_num} " + "="*50 + "\n")
                else:
                    print(f"ŸÑŸÖ Ÿäÿ™ŸÖ ÿßŸÑÿπÿ´Ÿàÿ± ÿπŸÑŸâ ŸÜÿµ ŸÅŸä ÿßŸÑÿµŸÅÿ≠ÿ© {page_num}")

        # ŸÉÿ™ÿßÿ®ÿ© ÿßŸÑŸÜÿµ ÿßŸÑŸÖÿ≥ÿ™ÿÆÿ±ÿ¨ ŸÅŸä ŸÖŸÑŸÅ ŸÜÿµŸä
        with open(output_txt_path, 'w', encoding='utf-8') as f:
            f.write("\n".join(extracted_text))

        print(f"‚úÖ ÿ™ŸÖ ÿ≠ŸÅÿ∏ ÿßŸÑŸÜÿµ ÿßŸÑŸÖÿ≥ÿ™ÿÆÿ±ÿ¨ ŸÅŸä: {output_txt_path}")

    except Exception as e:
        print(f"‚ùå ÿ≠ÿØÿ´ ÿÆÿ∑ÿ£ ÿ£ÿ´ŸÜÿßÿ° ŸÖÿπÿßŸÑÿ¨ÿ© ÿßŸÑŸÖŸÑŸÅ: {e}")

# === ÿßŸÑÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ===
if __name__ == "__main__":
    # ÿ∫ŸäŸëÿ± Ÿáÿ∞Ÿá ÿßŸÑŸÖÿ≥ÿßÿ±ÿßÿ™ ÿ≠ÿ≥ÿ® ŸÖŸÑŸÅŸÉ
    pdf_file = "/content/Dracula (Novel)_1-5.pdf"          # ÿßÿ≥ŸÖ ŸÖŸÑŸÅ PDF (Ÿäÿ¨ÿ® ÿ£ŸÜ ŸäŸÉŸàŸÜ ŸÅŸä ŸÜŸÅÿ≥ ÿßŸÑŸÖÿ¨ŸÑÿØ ÿ£Ÿà ÿ£ÿπÿ∑ ŸÖÿ≥ÿßÿ±Ÿãÿß ŸÉÿßŸÖŸÑÿßŸã)
    output_file = "ÿßŸÑŸÜÿµ_ÿßŸÑŸÖÿ≥ÿ™ÿÆÿ±ÿ¨.txt"  # ÿßÿ≥ŸÖ ÿßŸÑŸÖŸÑŸÅ ÿßŸÑŸÜÿµŸä ÿßŸÑŸÜÿßÿ™ÿ¨

    extract_text_from_pdf(pdf_file, output_file)

import ollama
import os
import time
from tqdm.notebook import tqdm

# ------------------- Configuration -------------------
INPUT_FILE = "/content/1.txt"
OUTPUT_FILE = "translated_general.txt"
MODEL_NAME = "llama3.1:8b"
# Chunk size: 1000-1200 is safe for maintaining context without memory issues
CHUNK_SIZE = 1200

# ------------------- GENERAL SYSTEM PROMPT -------------------
# This prompt is designed to work with ANY text type (technical, story, news, etc.)
# It focuses on language purity and style, not specific words.

SYSTEM_PROMPT = (
    "ROLE: You are an expert linguist and professional translator.\n"
    "TASK: Translate the provided text into fluent, high-quality Modern Standard Arabic (Fusha).\n"
    "GUIDELINES:\n"
    "1. Context Awareness: Analyze the text to understand the context. If it is technical, use technical Arabic terms. If it is narrative, use descriptive Arabic.\n"
    "2. Language Purity: The output must be strictly in Arabic script. Do NOT use Hindi, Urdu, or Chinese characters under any circumstances.\n"
    "3. Grammar: Ensure correct grammar and sentence structure.\n"
    "4. formatting: Maintain the original paragraph structure.\n"
    "5. OUTPUT: Provide ONLY the Arabic translation. Do not add notes like 'Here is the translation'."
)

# -------------------------------------------------------------

def split_text_smart(text, max_length):
    """
    Splits text efficiently without breaking sentences.
    """
    chunks = []
    while len(text) > max_length:
        # Priority 1: Split at a newline (paragraph end)
        split_index = text.rfind('\n', 0, max_length)

        # Priority 2: Split at a period (sentence end)
        if split_index == -1:
            split_index = text.rfind('.', 0, max_length)

        # Priority 3: Split at a space (word end)
        if split_index == -1:
            split_index = text.rfind(' ', 0, max_length)

        # Fallback: Hard split if no separators found
        if split_index == -1:
            split_index = max_length

        chunks.append(text[:split_index + 1])
        text = text[split_index + 1:].strip()

    if text:
        chunks.append(text)
    return chunks

def translate_segment(text_chunk, retries=3):
    """
    Translates a chunk with retry logic to handle potential model hiccups.
    """
    for attempt in range(retries):
        try:
            response = ollama.chat(
                model=MODEL_NAME,
                messages=[
                    {'role': 'system', 'content': SYSTEM_PROMPT},
                    {'role': 'user', 'content': text_chunk},
                ],
                options={
                    # Temperature 0.3 allows the model enough flexibility to pick
                    # the right Arabic words without getting stuck in a loop (which causes glitches).
                    'temperature': 0.3,
                    'top_p': 0.9,
                    'num_predict': 2048,
                }
            )

            content = response['message']['content']
            return content

        except Exception as e:
            print(f"\n[Warning] Attempt {attempt+1} failed. Error: {e}")
            time.sleep(1)

    return "[Error: Segment translation failed]"

def main():
    # 1. Validation
    if not os.path.exists(INPUT_FILE):
        print(f"[Error] File '{INPUT_FILE}' not found. Please upload it.")
        return

    print("--- Starting General Translation ---")

    # 2. Read File
    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        full_text = f.read()

    # 3. Split
    chunks = split_text_smart(full_text, CHUNK_SIZE)
    print(f"Document split into {len(chunks)} parts.")

    # 4. Prepare Output File
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write("")

    # 5. Process
    for chunk in tqdm(chunks, desc="Translating"):
        result = translate_segment(chunk)

        # Append immediately to file
        with open(OUTPUT_FILE, "a", encoding="utf-8") as f:
            f.write(result + "\n\n")

    print(f"\n[Success] Process complete! File saved as: {OUTPUT_FILE}")

    # 6. Preview
    print("-" * 30)
    print("Preview of the result:")
    if os.path.exists(OUTPUT_FILE):
        with open(OUTPUT_FILE, "r", encoding="utf-8") as f:
            print(f.read()[:600] + "...")
    print("-" * 30)

if __name__ == "__main__":
    main()



"""https://huggingface.co/CohereLabs/aya-expanse-8b

https://huggingface.co/CohereLabs/c4ai-command-r7b-arabic-02-2025

https://huggingface.co/bartowski/aya-expanse-32b-GGUF/tree/main

https://huggingface.co/bartowski/aya-expanse-8b-GGUF/tree/main

https://huggingface.co/spaces/CohereLabs/command-a-translate

https://huggingface.co/bartowski/CohereLabs_command-a-reasoning-08-2025-GGUF/tree/main/CohereLabs_command-a-reasoning-08-2025-Q3_K_M

https://huggingface.co/bartowski/c4ai-command-r7b-12-2024-GGUF/tree/main
"""

https://huggingface.co/CohereLabs/c4ai-command-r7b-12-2024


Languages covered: The model has been trained on 23 languages: English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Arabic, Chinese, Russian, Polish, Turkish, Vietnamese, Dutch, Czech, Indonesian, Ukrainian, Romanian, Greek, Hindi, Hebrew, and Persian.

Context length: Command R7B supports a context length of 128K.



/content/translated_general.txt
ÿØÿ±ÿßŸÉŸàŸÑÿß
ŸÖŸÜ ÿ™ÿ£ŸÑŸäŸÅ ÿ®ÿ±ÿßŸÖ ÿ≥ÿ™ŸàŸÉÿ±
ÿßŸÑŸÖÿ∑ÿ®Ÿàÿπÿ© ÿπÿßŸÖ 1897
ÿßŸÑŸÅÿµŸÑ ÿßŸÑÿ£ŸàŸÑ
ÿ≥ÿ¨ŸÑ ÿ¨ŸàŸÜÿßÿ´ÿßŸÜ Ÿáÿßÿ±ŸÉÿ±
3 ŸÖÿßŸäŸà. ÿ®ÿ≥ÿ™ÿ±Ÿäÿ™ÿ≥Ÿäÿ≤. - ÿ∫ÿßÿØÿ±ŸÜÿß ŸÖŸäŸàŸÜÿÆ ŸÅŸä ÿßŸÑÿ≥ÿßÿπÿ© ÿßŸÑÿ´ÿßŸÖŸÜÿ© ŸàÿßŸÑÿ´ŸÑÿßÿ´ŸäŸÜ ŸÖÿ≥ÿßÿ°Ÿã ŸÅŸä ÿßŸÑŸäŸàŸÖ ÿßŸÑÿ£ŸàŸÑ ŸÖŸÜ ŸÖÿßŸäŸàÿå ŸàŸàÿµŸÑŸÜÿß ÿ•ŸÑŸâ ŸÅŸäŸäŸÜÿß ŸÅŸä ÿßŸÑÿµÿ®ÿßÿ≠ ÿßŸÑÿ®ÿßŸÉÿ± ÿßŸÑÿ™ÿßŸÑŸäÿå ŸÉÿßŸÜ Ÿäÿ¨ÿ® ÿ£ŸÜ ŸÜÿµŸÑ ŸÅŸä ÿßŸÑÿ≥ÿßÿπÿ© ÿßŸÑÿ≥ÿßÿØÿ≥ÿ© Ÿàÿ£ÿ±ÿ®ÿπŸäŸÜ ÿØŸÇŸäŸÇÿ©ÿå ŸÑŸÉŸÜ ÿßŸÑŸÇÿ∑ÿßÿ± ÿ™ÿ£ÿÆÿ± ÿ≥ÿßÿπÿ© Ÿàÿßÿ≠ÿØÿ©.
Ÿäÿ®ÿØŸà ÿ£ŸÜ ÿ®ŸàÿØÿßÿ®ÿ≥ÿ™ ŸÖŸÉÿßŸÜ ÿ±ÿßÿ¶ÿπÿå ŸÖŸÜ ŸÜÿ∏ÿ±ÿ© ŸÖÿß ÿßÿ≥ÿ™ÿ∑ÿπÿ™Ÿáÿß ŸÖŸÜ ÿÆŸÑÿßŸÑ ÿßŸÑŸÇÿ∑ÿßÿ± Ÿàÿ®ÿßŸÑŸÇÿ±ÿ® ÿßŸÑÿ∞Ÿä ÿßÿ≥ÿ™ÿ∑ÿπÿ™ ÿßŸÑŸÖÿ¥Ÿä ŸÅŸäŸá ŸÖŸÜ ÿßŸÑÿ¥Ÿàÿßÿ±ÿπ. ÿÆŸÅÿ™ ÿ£ŸÜ ÿ¢ÿ™Ÿä ÿ®ÿπŸäÿØŸãÿß ÿπŸÜ ÿßŸÑŸÖÿ≠ÿ∑ÿ©ÿå ŸÑÿ£ŸÜŸÜÿß ŸàÿµŸÑŸÜÿß ŸÖÿ™ÿ£ÿÆÿ±ŸàŸÜ Ÿàÿ≥ŸàŸÅ ŸÜÿ∫ÿßÿØÿ± ŸÅŸä ÿßŸÑŸàŸÇÿ™ ÿßŸÑÿµÿ≠Ÿäÿ≠ ŸÇÿØÿ± ÿßŸÑÿ•ŸÖŸÉÿßŸÜ.
ÿßŸÑ impression ÿßŸÑÿ™Ÿä ÿ≠ÿµŸÑÿ™ ÿπŸÑŸäŸáÿß ŸáŸä ÿ£ŸÜŸÜÿß ŸÜÿ™ÿ±ŸÉ ÿßŸÑÿ∫ÿ±ÿ® ŸàŸÜÿØÿÆŸÑ ÿßŸÑÿ¥ÿ±ŸÇÿõ ÿ£ŸÉÿ´ÿ± ÿßŸÑÿ¨ÿ≥Ÿàÿ± ÿßŸÑŸÖÿ∞ŸáŸÑÿ© ŸÅŸä ÿßŸÑÿπÿßŸÑŸÖ Ÿäÿ±ÿ®ÿ∑ ÿ®ŸäŸÜ ÿßŸÑÿ∫ÿ±ÿ® ŸàÿßŸÑÿ¥ÿ±ŸÇÿå ŸàŸáŸä ÿ¨ÿ≥ÿ± ÿπŸÑŸâ ÿßŸÑŸÜŸáÿ± ÿßŸÑÿØÿßŸÜŸàÿ® ÿßŸÑÿ∞Ÿä ŸáŸÜÿß ŸÖŸÜ ÿßŸÑÿπÿ±ÿ∂ ÿßŸÑŸÉÿ®Ÿäÿ± ŸàÿßŸÑŸÖÿπŸÖŸàŸÑÿ© ÿßŸÑÿπŸÖŸäŸÇÿ©ÿå ŸàŸàÿ∂ÿπŸÜÿß ÿ®ŸäŸÜ ÿßŸÑÿ™ŸÇÿßŸÑŸäÿØ ÿßŸÑÿ≠ÿßŸÉŸÖÿ© ŸÑŸÑÿπÿ´ŸÖÿßŸÜŸäŸäŸÜ.

================================================== ÿµŸÅÿ≠ÿ© 1 ==================================================

ÿ∫ÿßÿØÿ±ŸÜÿß ŸÅŸä ŸàŸÇÿ™ ÿ¨ŸäÿØ ŸÜÿ≥ÿ®ŸäŸãÿßÿå ŸàŸàÿµŸÑŸÜÿß ÿ®ÿπÿØ ÿ∫ÿ±Ÿàÿ® ÿßŸÑÿ¥ŸÖÿ≥ ÿ•ŸÑŸâ ŸÉŸÑŸàÿ≥ŸÜÿ®ÿ±ÿ¨. ÿ™ŸàŸÇŸÅÿ™ ŸáŸÜÿß ŸÑŸäŸÑÿ™ŸÜŸä ŸÅŸä ŸÅŸÜÿØŸÇ ÿ±ŸàŸäÿßŸÑ. ÿ£ŸÉŸÑÿ™ ŸÅŸä ÿßŸÑÿ∫ÿØÿßÿ° ÿ£Ÿà ÿ®ÿßŸÑÿ£ÿ≠ÿ±Ÿâ ÿßŸÑÿπÿ¥ÿßÿ°ÿå ÿØÿ¨ÿßÿ¨ÿ© ŸÖÿ∑ŸáŸàÿ±ÿ© ÿ®ÿ∑ÿ±ŸäŸÇÿ© ŸÖÿπŸäŸÜÿ© ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ÿßŸÑŸÅŸÑŸÅŸÑ ÿßŸÑÿ£ÿ≠ŸÖÿ±ÿå ŸÉÿßŸÜÿ™ ÿ∑ÿπŸÖÿ© ÿ¨ŸäÿØÿ© ŸàŸÑŸÉŸÜ ÿ¨ŸÅŸäŸÅÿ©. (ŸÖŸÑÿßÿ≠ÿ∏ÿ©: ÿßÿ≠ÿµŸÑ ÿπŸÑŸâ ŸàÿµŸÅÿ© ŸÑŸáÿß ŸÑ ŸÖŸäŸÜÿß). ÿ≥ÿ£ŸÑÿ™ ÿßŸÑÿÆÿßÿØŸÖÿå ŸàŸÇÿßŸÑ ŸÑŸä ÿ•ŸÜŸáÿß ÿ™ÿ≥ŸÖŸâ "ÿ®ÿßŸæÿ±ŸäŸÉÿß ŸáŸäŸÜÿØŸäŸÑ"ÿå Ÿàÿ£ŸÜŸáÿß ÿ∑ÿπÿßŸÖ Ÿàÿ∑ŸÜŸäÿå n√™n Ÿäÿ¨ÿ® ÿ£ŸÜ ÿ£ÿ≥ÿ™ÿ∑Ÿäÿπ ÿßŸÑÿ≠ÿµŸàŸÑ ÿπŸÑŸäŸáÿß ŸÅŸä ÿ£Ÿä ŸÖŸÉÿßŸÜ.

ŸÅŸä ÿ£Ÿä ŸÖŸÉÿßŸÜ ÿπŸÑŸâ ÿ¨ÿ®ÿßŸÑ ÿßŸÑŸÉÿßÿ±ÿ®ÿßÿ™.
Ÿàÿ¨ÿØÿ™ ÿ£ŸÜ ŸÖÿπÿ±ŸÅÿ™Ÿä ÿßŸÑŸÇŸÑŸäŸÑŸá ÿ®ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿ£ŸÑŸÖÿßŸÜŸäÿ© ŸÉÿßŸÜÿ™ ŸÖŸÅŸäÿØÿ© ŸáŸÜÿßÿå Ÿà ŸÑÿß ÿ£ÿπÿ±ŸÅ ŸÉŸäŸÅ ÿ≥ÿ£ÿ≥ÿ™ÿ∑Ÿäÿπ ÿßŸÑÿπŸäÿ¥ ÿ®ÿØŸàŸÜŸáÿß.
ÿ®ÿπÿØŸÖÿß ÿßÿ≥ÿ™ŸÅÿØÿ™ ŸÖŸÜ ÿ®ÿπÿ∂ ÿßŸÑŸàŸÇÿ™ ÿßŸÑÿ∞Ÿä ŸÉÿßŸÜ ŸÖÿ™ŸàŸÅÿ±ÿß ŸÑŸä ŸÅŸä ŸÑŸÜÿØŸÜÿå ÿ≤ÿ±ÿ™ ÿßŸÑŸÖÿ™ÿ≠ŸÅ ÿßŸÑÿ®ÿ±Ÿäÿ∑ÿßŸÜŸä ŸàŸÇŸÖÿ™ ÿ®ÿ®ÿ≠ÿ´ ÿ®ŸäŸÜ ÿßŸÑŸÉÿ™ÿ® ŸàÿßŸÑÿÆÿ±ÿßÿ¶ÿ∑ ŸÅŸä ÿßŸÑŸÖŸÉÿ™ÿ®ÿ© ÿ≠ŸàŸÑ ÿ™ÿ±ÿßÿ≥Ÿäÿßÿå Ÿà ÿ£ÿØÿ±ŸÉÿ™ ÿ£ŸÜ ÿßŸÑŸÖÿπÿ±ŸÅÿ© ÿßŸÑŸÇŸÑŸäŸÑŸá ÿ®ÿßŸÑÿ®ŸÑÿßÿØ ÿ≥ŸàŸÅ ÿ™ŸÉŸàŸÜ ŸÖŸáŸÖÿ© ŸÅŸä ÿßŸÑÿ™ÿπÿßŸÖŸÑ ŸÖÿπ ŸÜÿ®ŸäŸÑ ŸÖŸÜ ÿ™ŸÑŸÉ ÿßŸÑÿ®ŸÑÿßÿØ.
ÿ£ŸÜÿß Ÿàÿ¨ÿØÿ™ ÿ£ŸÜ ÿßŸÑŸÖŸÜÿ∑ŸÇÿ© ÿßŸÑÿ™Ÿä ÿ∞ŸÉÿ±Ÿáÿß ŸáŸà ŸÅŸä ÿ£ŸÇÿµŸâ ÿ¥ÿ±ŸÇ ÿßŸÑÿ®ŸÑÿßÿØÿå ÿπŸÑŸâ ÿ≠ÿØŸàÿØ ÿ´ŸÑÿßÿ´ ÿØŸàŸÑ: ÿ™ÿ±ÿßÿ≥Ÿäÿß ŸàŸÖŸàŸÑÿØÿßŸÅŸäÿß Ÿàÿ®ŸàŸÉŸàŸÅŸäŸÜÿßÿå Ÿàÿ≥ÿ∑ ÿ¨ÿ®ÿßŸÑ ÿßŸÑŸÉÿßÿ±ÿ®ÿßÿ™ÿõ Ÿàÿßÿ≠ÿØÿ© ŸÖŸÜ ÿ£ÿ¨ÿ≤ÿßÿ° ÿ£Ÿàÿ±Ÿàÿ®ÿß ÿßŸÑÿ£ÿ¥ÿØ ÿ∫ŸÖŸàÿ∂ÿßŸã ŸàÿπÿØŸÖ ÿßŸÑŸÖÿπÿ±ŸÅÿ©.
ŸÑŸÖ ÿßÿ≥ÿ™ÿ∑ÿπ ÿßŸÑÿπÿ´Ÿàÿ± ÿπŸÑŸâ ÿ£Ÿä ÿÆÿ±Ÿäÿ∑ÿ© ÿ£Ÿà ÿπŸÖŸÑ Ÿäÿ®ŸäŸÜ ŸÖŸàŸÇÿπ ŸÇÿµÿ± ÿØÿ±ÿßŸÉŸàŸÑÿß ÿ®ÿØŸÇÿ©ÿå ŸÑÿ£ŸÜ ŸÑÿß ÿ™Ÿàÿ¨ÿØ ÿÆÿ±ÿßÿ¶ÿ∑ ŸÑŸáÿ∞Ÿá ÿßŸÑÿ®ŸÑÿßÿØ ÿ≠ÿ™Ÿâ ÿßŸÑÿ¢ŸÜ ŸÖÿ´ŸÑ ÿÆÿ±ÿßÿ¶ÿ∑ ÿßŸÑŸÖÿ≥ÿ≠ ÿßŸÑÿ¨ÿ∫ÿ±ÿßŸÅŸä ÿßŸÑÿ®ÿ±Ÿäÿ∑ÿßŸÜŸä ÿßŸÑÿ™Ÿä ŸÜÿ≥ÿ™ÿÆÿØŸÖŸáÿßÿõ ŸÑŸÉŸÜŸÜŸä Ÿàÿ¨ÿØÿ™ ÿ£ŸÜ ÿ®ÿ≥ÿ™ÿ±Ÿäÿ™ÿ≤ÿå ÿßŸÑŸÖÿØŸäŸÜÿ© ÿßŸÑŸÖÿ≠ŸÑŸäÿ© ÿßŸÑŸÖÿ∞ŸÉŸàÿ±ÿ© ŸÖŸÜ ŸÇÿ®ŸÑ ŸÉŸàŸÜÿ™ ÿØÿ±ÿßŸÉŸàŸÑÿßÿå ŸáŸä ÿ®ŸÑÿØÿ© ŸÖÿπÿ±ŸàŸÅÿ©.
ÿ≥ÿ£ÿ∂ŸäŸÅ ŸáŸÜÿß ÿ®ÿπÿ∂ ŸÖŸÑÿßÿ≠ÿ∏ÿßÿ™Ÿäÿå ÿ≠Ÿäÿ´ ŸÇÿØ ÿ™ÿ≥ÿßÿπÿØŸÜŸä ŸÅŸä ÿßÿ≥ÿ™ÿπÿßÿØÿ© ÿßŸÑÿ∞ÿßŸÉÿ±ÿ© ÿπŸÜÿØŸÖÿß ÿ£ÿ™ÿ≠ÿØÿ´ ŸÖÿπ ŸÖŸäŸÜÿß ÿπŸÜ ÿ±ÿ≠ŸÑÿßÿ™Ÿä.
ŸÅŸä ÿ≥ŸÉÿßŸÜ ÿ™ÿ±ÿßÿ≥Ÿäÿß ŸáŸÜÿßŸÉ ÿ£ÿ±ÿ®ÿπ ÿ∑Ÿàÿßÿ¶ŸÅ ŸÖÿÆÿ™ŸÑŸÅÿ©:
(Translation continues...)

ŸÅŸä ÿßŸÑÿ¨ŸÜŸàÿ® Saxonsÿå ŸàŸáŸÖ ŸÖÿÆŸÑŸàÿ∑ŸàŸÜ ÿ®Wallachsÿå ÿßŸÑÿ∞ŸäŸÜ ŸáŸÖ ÿ£ÿ≠ŸÅÿßÿØ Daciansÿõ ŸÅŸä ÿßŸÑÿ∫ÿ±ÿ® Magyarsÿå ŸàŸÅŸä ÿßŸÑÿ¥ÿ±ŸÇ ŸàÿßŸÑÿ¥ŸÖÿßŸÑ Szekelys. ÿ£ŸÜÿß ÿ£ÿ∞Ÿáÿ® ÿ•ŸÑŸâ ÿßŸÑÿ£ÿÆŸäÿ±ÿå ÿßŸÑÿ∞ŸäŸÜ Ÿäÿ≤ÿπŸÖŸàŸÜ ÿ£ŸÜŸáŸÖ ŸÖŸÜ ÿ∞ÿ±ŸäŸëÿ© Attila Ÿàÿ≠ŸÖŸëÿßŸÑ ÿßŸÑŸáŸàÿßÿ°. ŸÇÿØ ŸäŸÉŸàŸÜ Ÿáÿ∞ÿß ÿµÿ≠Ÿäÿ≠Ÿãÿßÿå ŸÅÿ•ÿ∞ÿß ŸÉÿßŸÜ Magyars ŸÇÿØ ÿ∫ÿ≤Ÿàÿß ÿßŸÑÿ®ŸÑÿßÿØ ŸÅŸä ÿßŸÑŸÇÿ±ŸÜ ÿßŸÑÿ≠ÿßÿØŸä ÿπÿ¥ÿ±ÿå Ÿàÿ¨ÿØŸàÿß ÿ≠ŸÖŸëÿßŸÑ ÿßŸÑŸáŸàÿßÿ° ŸÖŸÇŸäŸÖŸäŸÜ ŸÅŸäŸáÿß.

ŸÇÿ±ÿ£ÿ™ ÿ£ŸÜ ŸÉŸÑ ŸÖÿß Ÿäÿπÿ±ŸÅ ŸÖŸÜ ÿÆÿ±ÿßŸÅÿßÿ™ ÿßŸÑÿπÿßŸÑŸÖ ÿ¨ŸÖÿπ ŸÅŸä ÿ≠ŸÑŸÇ Carpathiansÿå ŸÉÿ£ŸÜŸáÿß ŸÖÿ±ŸÉÿ≤ ŸÑÿ®ÿπÿ∂ ŸÜŸàÿπ ŸÖŸÜ ÿßŸÑÿØŸàÿ±ÿßŸÜ ÿßŸÑÿÆŸäÿßŸÑŸäÿõ ŸÅÿ±ÿ®ŸÖÿß ÿ™ŸÉŸàŸÜ ÿ•ŸÇÿßŸÖÿ™ÿßŸä ŸÖÿ´Ÿäÿ±ÿ©. (ŸÖŸÑÿßÿ≠ÿ∏ÿ©ÿå Ÿäÿ¨ÿ® ÿ£ŸÜ ÿ£ÿ≥ÿ£ŸÑ ÿßŸÑŸÉŸàŸÜÿ™ ÿπŸÜŸáŸÖ.)

ŸÑŸÖ ÿ£ÿ≥ÿ™ÿ±Ÿäÿ≠ ÿ¨ŸäÿØŸãÿßÿå ÿ±ÿ∫ŸÖ ÿ£ŸÜ ÿ≥ÿ±Ÿäÿ±Ÿä ŸÉÿßŸÜ ŸÖÿ±Ÿäÿ≠Ÿãÿßÿå ŸÑÿ£ŸÜŸÜŸä ŸÉŸÜÿ™ ÿ£ŸÅŸÉÿ± ŸÅŸä ŸÉŸÑ sorts of ÿ£ÿ≠ŸÑÿßŸÖ ÿ∫ÿ±Ÿäÿ®ÿ©. ŸÉÿßŸÜÿ™ ŸáŸÜÿßŸÉ ŸÉŸÑÿ®ÿ© ÿ™ÿµÿ±ÿÆ ÿ∑ŸàÿßŸÑ ÿßŸÑŸÑŸäŸÑ ÿ™ÿ≠ÿ™ ŸÜÿßŸÅÿ∞ÿ™Ÿäÿå ŸàŸÇÿØ ŸäŸÉŸàŸÜ ŸÑŸáÿß ÿπŸÑÿßŸÇÿ© ÿ®Ÿáÿ∞ÿßÿõ ÿ£Ÿà ŸÇÿØ ŸäŸÉŸàŸÜ ŸÖŸÜ paprikaÿå ŸÑÿ£ŸÜŸÜŸä Ÿäÿ¨ÿ® ÿ£ŸÜ ÿ£ÿ≥ŸÇŸä ŸÉÿ£ÿ≥ ÿßŸÑŸÖÿßÿ° ŸÅŸä ŸÉÿßÿ±ŸäŸÅŸäÿå Ÿàÿßÿ≤ÿØÿßÿØÿ™ ÿ¨ŸÅÿßŸÅŸä. ŸÜÿ≠Ÿà ÿßŸÑŸÅÿ¨ÿ± ÿßÿ≥ÿ™ŸäŸÇÿ∏ÿ™ Ÿàÿ£ÿµÿ®ÿ≠ÿ™ ŸÖÿ≥ÿ™ŸäŸÇÿ∏Ÿãÿß ÿ®ÿ≥ÿ®ÿ® ÿßŸÑÿ∂ÿ±ÿ® ÿßŸÑŸÖÿ≥ÿ™ŸÖÿ± ÿπŸÑŸâ ÿ®ÿßÿ®Ÿäÿå ŸÅÿ®ÿØŸà ŸÑŸä ÿ£ŸÜŸá ŸÉŸÜÿ™ ÿ£ÿ≥ÿ™ÿ±Ÿäÿ≠ ÿ¨ŸäÿØŸãÿß ÿ¢ŸÜÿ∞ÿßŸÉ.

ŸÉŸÜÿ™ ŸÇÿØ ÿ£ŸÉŸÑÿ™ ŸÅŸä ÿßŸÑÿµÿ®ÿßÿ≠ ÿ£ŸÉÿ´ÿ± ŸÖŸÜ ÿßŸÑÿ®ÿßÿ®ÿ±ŸäŸÉÿßÿå Ÿà ŸÜŸàÿπŸãÿß ŸÖŸÜ ÿßŸÑÿ≠ÿ≥ÿßÿ° ŸÖŸÜ ÿØŸÇŸäŸÇ ÿßŸÑÿ∞ÿ±ÿ© ÿßŸÑÿ∞Ÿä ŸÇÿßŸÑŸàÿß ÿ•ŸÜŸá "ŸÖÿßŸÖÿßŸÑŸäŸÉÿß"ÿå Ÿà ÿÆÿ∂ÿ±Ÿàÿ© ŸÅŸÑŸÅŸÑ ŸÖŸÖÿ™ŸÑÿ¶ÿ© ÿ®ÿßŸÑÿØÿ¨ÿßÿ¨ ÿßŸÑŸÖŸÇÿ∑Ÿëÿπÿå Ÿàÿ¨ÿØÿ™ Ÿáÿ∞Ÿá ÿßŸÑÿ£ŸÉŸÑ ÿ¨ŸäÿØŸãÿß ÿ¨ÿØŸãÿßÿå ŸàŸäÿ≥ŸÖŸàŸÜŸáÿß "ÿ•ŸÖÿ®ŸÑÿßÿ™ÿß". (ŸÖÿ∞ŸÉŸàÿ±ÿ©: ÿ≠ÿµŸÑ ÿπŸÑŸâ ÿßŸÑŸàÿµŸÅÿ© ŸÑŸáÿ∞ÿß ÿßŸÑÿ£ŸÉŸÑ ÿ£Ÿäÿ∂Ÿãÿß.)

ŸÉÿßŸÜ ÿπŸÑŸä ÿ£ŸÜ ÿ£ÿ≥ÿ±ÿπ ŸÅŸä ÿßŸÑÿ£ŸÉŸÑ ŸÑÿ£ŸÜ ÿßŸÑŸÇÿ∑ÿßÿ± ÿ®ÿØÿ£ ŸÇÿ®ŸÑ ÿßŸÑÿ≥ÿßÿπÿ© ÿßŸÑÿ´ÿßŸÖŸÜÿ© ŸÇŸÑŸäŸÑŸãÿßÿå ÿ£Ÿà ÿ±ÿ®ŸÖÿß ŸÉÿßŸÜ Ÿäÿ¨ÿ® ÿ£ŸÜ Ÿäÿ®ÿØÿ£ ÿ®ÿ∞ŸÑŸÉÿå ŸÑÿ£ŸÜŸá ÿ®ÿπÿØ ÿ≥Ÿäÿ± ÿ•ŸÑŸâ ÿßŸÑŸÖÿ≠ÿ∑ÿ© ŸÅŸä ÿßŸÑÿ≥ÿßÿπÿ© 7:30 ŸÉÿßŸÜ ÿπŸÑŸä ÿ£ŸÜ ÿ£ÿ¨ŸÑÿ≥ ŸÅŸä ÿßŸÑŸÇÿ∑ÿßÿ± ŸÑŸÖÿØÿ© ÿ£ŸÉÿ´ÿ± ŸÖŸÜ ÿ≥ÿßÿπÿ© ŸÇÿ®ŸÑ ÿ£ŸÜ ŸÜÿ®ÿØÿ° ÿßŸÑÿ≠ÿ±ŸÉÿ©.

Ÿäÿ®ÿØŸà ŸÑŸä ÿ£ŸÜ ŸÉŸÑŸÖÿß ÿßÿ™ÿ¨ŸáŸÜÿß ÿ¥ÿ±ŸÇŸãÿß ÿ≤ÿßÿØÿ™ ÿπÿØŸÖ ÿØŸÇÿ™ŸáŸÖ. ŸÖÿßÿ∞ÿß Ÿäÿ¨ÿ® ÿ£ŸÜ ŸäŸÉŸàŸÜŸàÿß ŸÅŸä ÿßŸÑÿµŸäŸÜÿü

ŸÉÿßŸÜÿ™ ÿßŸÑŸäŸàŸÖ ÿ™ÿ®ÿØŸà ŸàŸÉÿ£ŸÜŸÜÿß ŸÜÿ≥ÿ™ÿ∫ÿ±ŸÇ ŸÅŸä ÿ®ŸÑÿßÿØ ŸÖŸÑŸäÿ¶ÿ© ÿ®ÿßŸÑÿ¨ŸÖÿßŸÑ ŸÖŸÜ ŸÉŸÑ ŸÜŸàÿπ. ÿ£ÿ≠ŸäÿßŸÜŸãÿß ÿ±ÿ£ŸäŸÜÿß ŸÖÿØŸÜ ÿµÿ∫Ÿäÿ±ÿ© ÿ£Ÿà ŸÇÿµŸàÿ± ÿπŸÑŸâ ŸÇŸÖŸÖ ÿ¨ÿ®ÿßŸÑ ŸÖÿ±ÿ™ŸÅÿπÿ© ŸÖÿ´ŸÑ ÿ™ŸÑŸÉ ÿßŸÑÿ™Ÿä ŸÜÿ±ÿßŸáÿß ŸÅŸä ÿßŸÑŸÇÿµÿµ ÿßŸÑŸÖŸÇÿØÿ≥ÿ©; Ÿàÿ£ÿ≠ŸäÿßŸÜŸãÿß ÿ≥ÿ±ÿπŸÜÿß ÿ®ÿ≥ŸáŸàŸÑÿ© ÿπÿ®ÿ± ÿ£ŸÜŸáÿßÿ± Ÿàÿ¢ÿ®ÿßÿ± Ÿäÿ®ÿØŸà ŸÖŸÜ ÿßŸÑÿ≠ÿßŸÅÿ© ÿßŸÑÿ≠ÿ¨ÿ±ŸäŸëÿ© ÿßŸÑŸàÿßÿ≥ÿπÿ© ŸÖŸÜ ŸÉŸÑ ÿ¨ÿßŸÜÿ® ŸÖŸÜŸáÿß ÿ£ŸÜŸáÿß ÿπÿ±ÿ∂ÿ© ŸÑŸÑŸÅŸäÿ∂ÿßŸÜÿßÿ™ ÿßŸÑŸÉÿ®Ÿäÿ±ÿ©. ÿ™ÿ≠ÿ™ÿßÿ¨ ŸÉŸÖŸäÿ© ŸÉÿ®Ÿäÿ±ÿ© ŸÖŸÜ ÿßŸÑŸÖÿßÿ°ÿå Ÿàÿ≥ÿ±ÿπÿ© ŸÇŸàŸäÿ©ÿå ŸÑÿ™ŸÜÿ∏ŸäŸÅ ÿ≠ÿßŸÅÿ© ÿÆÿßÿ±ÿ¨ŸäŸëÿ© ŸÑŸÜŸáÿ±.

ŸÅŸä ŸÉŸÑ ŸÖÿ≠ÿ∑ÿ© ŸÉÿßŸÜ ŸáŸÜÿßŸÉ ŸÖÿ¨ŸÖŸàÿπÿßÿ™ ŸÖŸÜ ÿßŸÑŸÜÿßÿ≥ÿå ÿ£ÿ≠ŸäÿßŸÜŸãÿß ÿ™ÿ¨ŸÖÿπÿßÿ™ÿå Ÿàÿ¨ŸÖŸäÿπŸáŸÖ Ÿäÿ±ÿ™ÿØŸàŸÜ ŸÖŸÑÿßÿ®ÿ≥ ŸÖÿÆÿ™ŸÑŸÅÿ©. ÿ®ÿπÿ∂ŸáŸÖ ŸÉÿßŸÜŸàÿß ŸÖÿ´ŸÑ ÿßŸÑŸÅŸÑÿßÿ≠ŸäŸÜ ŸÅŸä ÿßŸÑŸàÿ∑ŸÜ ÿ£Ÿà ÿßŸÑÿ∞ŸäŸÜ ÿ±ÿ£Ÿäÿ™ ŸÅŸä...

ŸÉÿßŸÜŸàÿß Ÿäÿ£ÿ™ŸàŸÜ ŸÖŸÜ ŸÅÿ±ŸÜÿ≥ÿß Ÿàÿ£ŸÑŸÖÿßŸÜŸäÿßÿå Ÿäÿ±ÿ™ÿØŸàŸÜ ŸÇŸÖŸäÿµ ŸÇÿµŸäÿ± Ÿàÿ∑ÿ±ÿ®Ÿàÿ¥ ŸÖÿ≥ÿ™ÿØŸäÿ± ŸàŸÇŸÖÿµÿßŸÜ ÿ£ÿµŸÑŸäÿ©ÿå ŸàŸÑŸÉŸÜ ŸÉÿßŸÜ ÿ®ÿπÿ∂ŸáŸÖ Ÿäÿ®ÿØŸà ŸÖÿ™ŸÖŸäÿ≤ÿßŸã.

ŸÜÿ∏ÿ±Ÿàÿß ÿ•ŸÑŸâ ÿßŸÑŸÜÿ≥ÿßÿ° ÿ¨ŸÖŸäŸÑÿßÿ™ ŸÅŸä ÿßŸÑÿ®ÿØÿßŸäÿ©ÿå ŸÑŸÉŸÜ ÿπŸÜÿØŸÖÿß ÿßŸÇÿ™ÿ±ÿ®ŸÜÿß ŸÖŸÜŸáŸÜÿå ÿßŸÉÿ™ÿ¥ŸÅŸÜÿß ÿ£ŸÜŸáŸÜ Ÿäÿ®ÿØŸàŸÜ ÿ∫Ÿäÿ± ŸÖÿ™ŸÉÿßŸÖŸÑÿßÿ™ ŸÅŸä ÿßŸÑÿÆÿµÿ±. ŸÉÿßŸÜÿ™ ÿ¨ŸÖŸäÿπŸáŸÜ ÿ™ŸÑÿ®ÿ≥ŸÜ ŸÉŸèŸÖŸëŸäŸÜ ÿ®Ÿäÿ∂ ÿ¢ŸÖŸÑŸäŸÜ ŸÖŸÜ ŸÜŸàÿπ ŸÖÿßÿå Ÿàÿ®ÿßŸÑÿ•ÿ∂ÿßŸÅÿ© ÿ•ŸÑŸâ ÿ∞ŸÑŸÉÿå ŸÉÿßŸÜ ŸÖÿπÿ∏ŸÖŸáŸÜ Ÿäÿ±ÿ™ÿØŸäŸÜ ÿ≠ÿ≤ÿßŸÖÿßŸã ŸÉÿ®Ÿäÿ±ÿßŸã ŸÖÿ≤ÿÆÿ±ŸÅÿßŸã ÿ®ÿßŸÑÿπÿØŸäÿØ ŸÖŸÜ ÿßŸÑÿ£ÿ¥ÿ±ÿ∑ÿ© ÿßŸÑÿ™Ÿä ÿ™ÿ™ŸÖÿßŸäŸÑ ŸÖÿ´ŸÑ ÿßŸÑŸÇÿµÿ®ÿßÿ™ ŸÅŸä ÿßŸÑÿ®ÿßŸÑŸäŸáÿå ŸàŸÑŸÉŸÜ ÿ∑ÿ®ÿπÿßŸãÿå ŸÉÿßŸÜÿ™ ŸáŸÜÿßŸÉ ŸÇŸÖŸäÿµÿßÿ™ ÿ™ÿ≠ÿ™Ÿáÿß.

ÿ£ŸÉÿ®ÿ± ŸÖÿß ÿ¥ÿßŸáÿØŸÜÿßŸá ŸÖŸÜ ÿ≠Ÿäÿ´ ÿßŸÑÿ∏ÿ±ÿßŸÅÿ© ŸÉÿßŸÜ ÿßŸÑÿ≥ŸÑŸà⁄§ÿßŸÉŸäŸàŸÜÿå ÿßŸÑÿ∞ŸäŸÜ ŸÉÿßŸÜŸàÿß ÿ£ŸÉÿ´ÿ± ÿ®ÿ±ÿ®ÿ±Ÿäÿ© ŸÖŸÜ ÿ®ÿßŸÇŸä ÿßŸÑŸÜÿßÿ≥ÿå ŸÖÿπ ÿ£ÿ∑ŸàÿßÿØ ÿ±ŸÅŸäÿπÿ© ŸÉÿ®Ÿäÿ±ÿ© ŸàŸÇŸÖÿµÿßŸÜ ÿ®Ÿäÿ∂ÿßÿ° ŸÖÿ™ÿ≥ÿÆÿ© ŸÉÿ®Ÿäÿ±ÿ© ŸàŸÇŸÖÿµÿßŸÜ ŸÖŸÜ ÿßŸÑŸÇÿ∑ŸÜ ÿßŸÑÿ®Ÿäÿ∂ ÿßŸÑÿ£ÿ®Ÿäÿ∂ÿå Ÿàÿ≠ÿ≤ÿßŸÖŸäŸÜ ŸÉÿ®Ÿäÿ±ŸäŸÜ ŸÖŸÜ ÿßŸÑÿ¨ŸÑÿØ ÿßŸÑÿ´ŸÇŸäŸÑÿå Ÿäÿ®ŸÑÿ∫ ÿπÿ±ÿ∂ŸáŸÖ ÿ™ŸÇÿ±Ÿäÿ®ÿßŸã ŸÇÿØŸÖŸãÿß Ÿàÿßÿ≠ÿØŸãÿßÿå ŸÖÿ≤ÿÆÿ±ŸÅŸäŸÜ ÿ®ÿßŸÑÿπÿØŸäÿØ ŸÖŸÜ ÿßŸÑŸÜÿ¨ŸàŸÖ ÿßŸÑÿ®ÿ±ŸàŸÜÿ≤Ÿäÿ©. ŸÉÿßŸÜŸàÿß Ÿäÿ±ÿ™ÿØŸàŸÜ ÿ≠ÿ∞ÿßÿ° ÿπÿßŸÑŸäÿå ŸÖÿπ ÿ™ÿ´ÿ®Ÿäÿ™ ŸÇŸÖŸäÿµŸáŸÖ ŸÅŸä ÿØÿßÿÆŸÑŸáÿå ŸàŸÉÿßŸÜ ŸÑÿØŸäŸáŸÖ ÿ¥ÿπÿ± ÿ£ÿ≥ŸàÿØ ÿ∑ŸàŸäŸÑ ŸàŸÖÿ≥ÿ™ÿ£ÿµŸÑÿ© ÿ£ÿ≥ŸàÿØ ÿ´ŸÇŸäŸÑÿ©. ŸáŸÖ ÿ£ÿµÿ≠ÿßÿ® ŸÖÿ∏Ÿáÿ± ÿ¨ŸÖŸäŸÑÿå ŸÑŸÉŸÜ ŸÑÿß Ÿäÿ®ÿØŸà ÿ£ŸÜŸáŸÜ ŸäŸÖÿ™ŸÑŸÉŸÜ ÿßŸÑŸÖÿ∏Ÿáÿ± ÿßŸÑÿ¨ÿ∞ÿ®. ŸÑŸà ŸÉÿßŸÜŸàÿß ÿπŸÑŸâ ÿßŸÑŸÖÿ≥ÿ±ÿ≠ÿå ŸÑŸÉÿßŸÜŸàÿß ÿ™ŸÖÿ´ŸäŸÑŸãÿß ŸÇÿØŸäŸÖŸãÿß ŸÖŸÜ ŸÅÿ±ŸÇÿ© ÿπÿ±ÿßÿ¶ÿ≥ ÿ®ÿ±ÿ®ÿ±Ÿäÿ© ÿ¥ÿ±ŸÇŸäÿ©. ŸàŸÖÿπ ÿ∞ŸÑŸÉÿå ŸàŸÅŸÇÿßŸã ŸÑŸÑŸÖÿπŸÑŸàŸÖÿßÿ™ ÿßŸÑÿ™Ÿä ÿ™ŸÑŸÇŸäÿ™Ÿáÿßÿå ŸáŸÖ ÿ£ŸÅÿ±ÿßÿØ ŸáŸäŸÜŸàŸÜ ŸàŸÖÿ™ÿÆÿßÿ∞ŸÑŸàŸÜ ŸÅŸä ÿßŸÑÿ™ÿπÿ®Ÿäÿ± ÿπŸÜ ÿßÿ≠ÿ™Ÿäÿßÿ¨ÿßÿ™ŸáŸÖ ÿßŸÑÿ∑ÿ®ŸäÿπŸäÿ©.

ŸÉÿßŸÜÿ™ ÿßŸÑÿ≥ÿßÿπÿ© ŸÖÿ™ÿ£ÿÆÿ±ÿ© ŸÖŸÜ ÿßŸÑŸÑŸäŸÑ ÿπŸÜÿØŸÖÿß ŸàÿµŸÑŸÜÿß ÿ•ŸÑŸâ ÿ®ÿ≥ÿ™ÿ±Ÿäÿ™ÿ≤ÿå ŸàŸáŸä ŸÖÿØŸäŸÜÿ© ŸÇÿØŸäŸÖÿ© ŸàŸÖÿ´Ÿäÿ±ÿ©. ŸäŸÇÿπ ÿπŸÑŸâ ÿ≠ÿßŸÅÿ© ÿßŸÑÿ®ŸÑÿßÿØ -ŸÅÿ•ŸÜ ŸÖŸÖÿ± ÿ®Ÿàÿ±ÿ∫Ÿà Ÿäÿ§ÿØŸä ŸÖŸÜŸáÿß ÿ•ŸÑŸâ ÿ®ŸàŸÉŸàŸÅŸäŸÜÿß- ŸàŸÑÿ∞ŸÑŸÉ ŸÅŸÇÿØ ÿπÿßŸÜŸâ ŸÖŸÜ Ÿàÿ¨ŸàÿØŸáÿß ÿ≠Ÿäÿßÿ© ŸÖÿ∂ÿ∑ÿ±ÿ®ÿ© ŸàŸäÿ™ÿ∂ÿ≠ ÿ∞ŸÑŸÉ ŸÅŸä ŸÉŸÑ ŸÖŸÉÿßŸÜ. ŸÇÿ®ŸÑ ÿÆŸÖÿ≥ŸäŸÜ ÿπÿßŸÖÿßŸã ÿ≠ÿØÿ´ÿ™ ÿ≥ŸÑÿ≥ŸÑÿ© ŸÖŸÜ ÿßŸÑÿ≠ŸàÿßÿØÿ´ ÿßŸÑŸÉÿ®ÿ±Ÿâÿå ŸÖŸÖÿß ÿ£ÿ≥ŸÅÿ± ÿπŸÜ ÿØŸÖÿßÿ± ŸÉÿ®Ÿäÿ± ÿπŸÑŸâ ÿÆŸÖÿ≥ ŸÖŸÜÿßÿ≥ÿ®ÿßÿ™ ŸÖÿÆÿ™ŸÑŸÅÿ©. ŸàŸÅŸä ÿ®ÿØÿßŸäÿ© ÿßŸÑŸÇÿ±ŸÜ ÿßŸÑÿ≥ÿßÿ®ÿπ ÿπÿ¥ÿ± ÿ™ÿπÿ±ÿ∂ÿ™ ŸÑŸÑÿßÿ≥ÿ™ŸäŸÑÿßÿ° ÿπŸÑŸâ ŸÖÿØÿ© ÿ´ŸÑÿßÿ´ÿ© ÿ£ÿ≥ÿßÿ®Ÿäÿπ ŸàŸàŸÇÿπÿ™ 13 ÿ£ŸÑŸÅ ŸÇÿ™ŸäŸÑÿå ŸàŸÉÿßŸÜÿ™ ÿßŸÑÿÆÿ≥ÿßÿ¶ÿ± ŸÅŸä ÿßŸÑÿ≠ÿ±ÿ® ÿ™ÿ≤ÿØÿßÿØ ÿ≥Ÿàÿ°ÿßŸã ÿ®ÿ≥ÿ®ÿ® ÿßŸÑÿ¨Ÿàÿπ ŸàÿßŸÑŸÖÿ±ÿ∂.

ÿ£ŸÖÿ±ŸÜŸä ŸÉŸàŸÜÿ™ ÿØÿ±ÿßŸÉŸàŸÑÿß ÿ®ÿßŸÑÿ∞Ÿáÿßÿ® ÿ•ŸÑŸâ ŸÅŸÜÿØŸÇ ÿßŸÑŸÉÿ±ŸàŸÜ ÿßŸÑÿ∞Ÿáÿ®Ÿäÿå ÿßŸÑÿ∞Ÿä Ÿàÿ¨ÿØÿ™Ÿá ÿ®ŸÖÿ∏Ÿáÿ± ŸÇÿØŸäŸÖ ÿ¨ÿØÿßŸãÿå ŸÑÿ£ŸÜŸÜŸä ŸÉŸÜÿ™ ÿ£ÿ±ŸäÿØ ÿ£ŸÜ ÿ£ÿ±Ÿâ ŸÉŸÑ ŸÖÿß ŸäŸÖŸÉŸÜŸÜŸä ŸÖŸÜ ÿ¥ÿ¶ŸàŸÜ ÿßŸÑÿ®ŸÑÿØ.

ŸÉÿßŸÜÿ™ ŸáŸÜÿßŸÉ ÿ™ŸàŸÇÿπÿßÿ™ ÿ®ÿ£ŸÜŸÜŸä ÿ≥ÿ£ŸÉŸàŸÜ ŸáŸÜÿßŸÉÿå ŸÅÿπŸÜÿØŸÖÿß ÿßŸÇÿ™ÿ±ÿ®ÿ™ ŸÖŸÜ ÿßŸÑÿ®ÿßÿ® Ÿàÿßÿ¨Ÿáÿ™ ÿßŸÖÿ±ÿ£ÿ© ŸÖÿ≥ŸÜÿ© ÿ∑Ÿäÿ®ÿ© ÿßŸÑŸÖÿ∏Ÿáÿ± ÿ™ÿ±ÿ™ÿØŸä ŸÖŸÑÿßÿ®ÿ≥ ÿßŸÑŸÅŸÑÿßÿ≠ŸäŸÜ ÿßŸÑÿπÿßÿØŸäÿ© -ŸÑÿ®ÿßÿ≥ ÿ£ÿ≥ŸÅŸÑ ÿ£ÿ®Ÿäÿ∂ ŸÖÿπ ŸÅÿ≥ÿ™ÿßŸÜ ÿ∂ÿÆŸÖ ŸÖŸÑŸàŸÜ ŸäÿµŸÑ ÿ•ŸÑŸâ ÿßŸÑŸÉÿ™ŸÅ- ÿπŸÜÿØŸÖÿß ÿßŸÇÿ™ÿ±ÿ®ÿ™ ŸÖŸÜŸáÿß ŸÇÿßŸÑÿ™: "ŸáŸà ÿßŸÑÿ≥ŸäÿØ ÿßŸÑÿ•ŸÜÿ¨ŸÑŸäÿ≤Ÿäÿü" ŸÇŸÑÿ™ ŸÑŸáÿß: "ŸÜÿπŸÖÿå ÿ¨ŸàŸÜÿßÿ´ÿßŸÜ Ÿáÿßÿ±ŸÉÿ±." ÿßÿ®ÿ™ÿ≥ŸÖÿ™ Ÿàÿßÿ±ÿ≥ŸÑÿ™ ÿ±ÿ≥ÿßŸÑÿ© ÿ•ŸÑŸâ ÿ±ÿ¨ŸÑ ŸÖÿ≥ŸÜ Ÿäÿ±ÿ™ÿØŸä ŸÇŸÖŸäÿµÿßŸã ÿ£ÿ®Ÿäÿ∂ÿßŸãÿå ŸÉÿßŸÜ ŸÇÿØ ÿßÿ™ÿ®ÿπŸáÿß ÿ•ŸÑŸâ ÿßŸÑÿ®ÿßÿ®.

ÿ∞Ÿáÿ®ÿå ŸÑŸÉŸÜŸá ÿπÿßÿØ ŸÅŸàÿ±Ÿãÿß ŸÖÿπ ÿ®ÿ±ŸäÿØ:
"ÿ£ÿ≠Ÿêÿ®ŸëŸèŸÉŸé.--ÿ£Ÿèÿ±ÿ≠Ÿéÿ®Ÿè ŸÑŸÉŸé ÿ®ÿßŸÑŸÉÿßÿ±ÿ®ÿßÿ™. ÿ£ŸéŸÜÿ∏Ÿèÿ±Ÿè ÿ•ŸÑŸâ ŸàÿµŸàŸÑŸÉŸê ÿ®ŸêÿßŸÑÿ≠ÿ±ÿµ. ÿßÿ≥ÿ™Ÿéÿ±ŸÇŸê ÿßŸÑŸÑŸéŸäŸíŸÑŸé ÿ¨ŸäÿØŸãÿß. ŸÅŸä ÿßŸÑÿ´ŸéŸÑÿßÿ´Ÿêÿ© ÿ∫ÿØŸãÿß ÿ≥Ÿäÿ®ÿØÿ£ ÿßŸÑÿ≥ŸéŸÅŸéÿ±Ÿè ŸÑŸêÿ®ŸèŸÉŸà⁄§ŸäŸÜÿßÿå ŸàŸäŸèÿ≠ÿ™ŸÅÿ∏Ÿè ÿ®ŸÖŸÉÿßŸÜŸç ŸÑŸÉŸé ŸÅŸäŸáŸê. ŸÅŸä ŸÖÿ∂ŸäŸÇ ÿßŸÑÿ®Ÿàÿ±ÿ∫Ÿà ÿ≥ŸàŸÅ ŸäŸÜÿ™ÿ∏ÿ±ŸÉŸê ÿπÿ±ÿ®ÿ™Ÿä Ÿàÿ≥Ÿäÿ£ÿÆÿ∞ŸÉŸê ÿ•ŸÑŸéŸäŸëŸé. ÿ£Ÿéÿ™ŸÖŸÜŸâ ÿ£ŸÜ ŸäŸÉŸàŸÜŸé ÿ≥ŸÅÿ±ŸÉ ŸÖŸÜ ŸÑŸÜÿØŸÜ ÿ≥ÿπŸäÿØŸãÿßÿå Ÿàÿ£ŸÜ ÿ™Ÿèÿ≥Ÿéÿ±ŸëŸéÿπŸé ŸÅŸä ÿ•ŸÇÿßŸÖÿ™ŸÉ ŸÅŸä ÿ£ÿ±ÿ∂Ÿä ÿßŸÑÿ¨ŸÖŸäŸÑÿ©.--ÿ£Ÿéÿ≠Ÿêÿ®ŸëŸèŸÉŸé,
ÿØÿ±ŸàŸÉŸàŸÑÿß."
4 ŸÖÿßŸäŸà- Ÿàÿ¨ÿØÿ™Ÿè ÿ£ŸÜŸë ÿ£ŸáŸÑŸä ŸÇÿØ ÿ≠ÿµŸÑŸàÿß ÿπŸÑŸâ ÿ®ÿ±ŸäÿØŸç ŸÖŸÜ ÿßŸÑŸÇÿ≥Ÿäÿ≥ÿå ŸäŸéÿØŸêÿ±ŸèŸáŸèŸÖ ÿ®Ÿêÿ≠ÿ¨ÿ≤ ÿ£ŸÅÿ∂ŸÑ ŸÖŸÉÿßŸÜŸç ŸÅŸä ÿßŸÑÿ≥ŸéŸÅŸéÿ±Ÿê ŸÑŸäÿõ ŸÑŸÉŸÜ ÿπŸÜÿØ ÿ≥ÿ§ÿßŸÑŸáŸÖ ÿπŸÜ ÿ™ŸÅÿßÿµŸäŸÑŸêŸáŸê ÿ®ÿØŸàÿß ŸÖÿ™ÿ±ÿØÿØŸäŸÜÿå Ÿàÿßÿπÿ™ÿ±ŸÅŸàÿß ÿ®ÿ£ŸéŸÜŸëŸéŸáŸÖ ŸÑÿß Ÿäÿ≥ÿ™ÿ∑ŸäÿπŸàŸÜ ŸÅŸáŸÖŸä ÿßŸÑÿ£ŸÑŸÖÿßŸÜŸäÿ©.

================================================== ÿµŸÅÿ≠ÿ© 4 ==================================================

ŸÑÿß ŸäŸÖŸÉŸÜ ÿ£ŸÜ ŸäŸÉŸàŸÜŸé Ÿáÿ∞ÿß ÿßŸÑÿµÿßÿØŸÇŸãÿßÿå ŸÑÿ£ŸÜŸëŸáŸÖ ŸÉÿßŸÜŸàÿß ŸäŸÅŸáŸÖŸàŸÜŸáÿß ÿ¨ŸäÿØŸãÿß ÿ≠ÿ™Ÿâ ÿ∞ŸÑŸÉ ÿßŸÑÿ≠ŸäŸÜÿõ ÿπŸÑŸâ ÿßŸÑÿ£ŸÇŸÑÿå ŸÉÿßŸÜÿ™ ÿßŸÑÿ•ÿ¨ÿßÿ®ÿßÿ™Ÿè ŸÑÿ£ÿ≥ÿ¶ŸÑÿ™Ÿä ÿØŸÇŸäŸÇÿ©Ÿã ŸÉÿ£ŸéŸÜŸëŸéŸáŸÖ Ÿäÿ≥ÿ™ÿ∑ŸäÿπŸàŸÜ ŸÅŸáŸÖŸáÿß. ŸàŸÇŸÅ ÿßŸÑÿ≤ÿπŸäŸÖ Ÿàÿ≤Ÿàÿ¨ÿ™Ÿáÿå ÿßŸÑÿ≥ŸäÿØÿ© ÿßŸÑŸÖÿ≥ŸÜÿ© ÿßŸÑÿ™Ÿä ÿßÿ≥ÿ™ŸÇÿ®ŸÑÿ™ŸÜŸäÿå ŸäŸÜÿ∏ÿ±ÿßŸÜ ÿ®ÿπÿ∂ŸáŸÖÿß ÿßŸÑÿ®ÿπÿ∂ ÿ®ŸêŸÜÿ∏ÿ±ÿ© ÿÆŸàŸÅŸäÿ© ŸÜŸàÿπŸãÿß ŸÖÿß. ÿ®ÿØÿß ÿ£ŸÜŸë ÿßŸÑŸÖÿßŸÑ ŸÇÿØ ÿ£Ÿèÿ±ÿ≥ŸêŸÑŸé ŸÅŸä ÿ®ÿ±ŸäÿØŸçÿå Ÿàÿ£ŸéŸÜŸëŸé ÿ∞ŸÑŸÉ ŸÉŸÑ ŸÖÿß Ÿäÿπÿ±ŸÅŸàŸÜŸá. ÿπŸÜÿØŸÖÿß ÿ≥ÿ£ŸÑÿ™ŸáŸèŸÖ ÿ•ŸÜ ŸÉÿßŸÜŸàÿß Ÿäÿπÿ±ŸÅŸàŸÜ ÿßŸÑŸÇÿ≥Ÿäÿ≥ ÿØÿ±ŸàŸÉŸàŸÑÿßÿå

ŸàŸÉÿßŸÜÿß ŸäÿµŸÑŸäÿßŸÜ ÿßŸÑÿµŸÑÿßÿ©ÿå ŸàŸäÿ™ŸÇÿßÿØŸÖÿßŸÜÿå ŸàŸÇÿßŸÑŸàÿß ÿ•ŸÜŸáŸÖÿß ŸÑÿß Ÿäÿπÿ±ŸÅÿßŸÜ ÿ¥Ÿäÿ¶Ÿãÿß ŸÖŸÜ ÿ∞ŸÑŸÉÿå ŸÅŸÑŸÖ Ÿäÿ™ÿ≠ÿØÿ´ÿß ÿ£ŸäŸãÿß. ŸÉÿßŸÜ ÿßŸÑŸàŸÇÿ™ ŸÇÿ±Ÿäÿ®Ÿãÿß ŸÖŸÜ ÿßŸÜÿ∑ŸÑÿßŸÇ ÿßŸÑÿ≥ŸÅÿ±ÿå ŸàŸÑŸÖ ÿ£ŸÉŸÜ ŸÑÿØŸä ŸàŸÇÿ™ ŸÑÿ£ÿ≥ÿ£ŸÑ‡πÉ‡∏Ñ‡∏£Ÿãÿß ÿ¢ÿÆÿ±ÿå ŸÑÿ£ŸÜ ŸÉŸÑ ÿ¥Ÿäÿ° ŸÉÿßŸÜ ÿ∫ÿßŸÖÿ∂Ÿãÿß ŸàŸÖÿ´Ÿäÿ±Ÿãÿß ŸÑŸÑÿÆŸàŸÅ.

ŸàŸÉÿßŸÜÿ™ ÿßŸÑÿ≥ŸäÿØÿ© ÿßŸÑÿπÿ¨Ÿàÿ≤ ÿ™ÿ≠ÿ∂ÿ± ÿ•ŸÑŸâ ÿ∫ÿ±ŸÅÿ™Ÿä ŸÇÿ®ŸÑ ÿ£ŸÜ ÿ£ÿÆÿ±ÿ¨ÿå ŸàŸÇÿßŸÑÿ™ ÿ®ÿ≠ÿ±ŸÉÿ© Ÿáÿ≥ÿ™Ÿäÿ±Ÿäÿ©: "ŸÑÿß Ÿäÿ¨ÿ® ÿ£ŸÜ ÿ™ÿ∞Ÿáÿ®ÿü Ÿäÿß ÿ¥ÿßÿ® ÿßŸÑŸáŸäÿ±! ŸÑÿß Ÿäÿ¨ÿ® ÿ£ŸÜ ÿ™ÿ∞Ÿáÿ®ÿü" ŸÉÿßŸÜÿ™ ŸÅŸä ÿ≠ÿßŸÑÿ© ŸÖŸÜ ÿßŸÑÿ™ŸáŸäÿ¨ ÿßŸÑÿ¥ÿØŸäÿØ ÿ≠ÿ™Ÿâ ŸÅŸÇÿØÿ™ ÿ≥Ÿäÿ∑ÿ±ÿ™Ÿáÿß ÿπŸÑŸâ ÿßŸÑŸÑÿ∫ÿ© ÿßŸÑÿ£ŸÑŸÖÿßŸÜŸäÿ© ÿßŸÑÿ™Ÿä ÿ™ÿπŸÑŸÖÿ™Ÿáÿßÿå ŸàÿÆŸÑÿ∑ÿ™ ÿ®ŸäŸÜŸáÿß Ÿàÿ®ŸäŸÜ ŸÑÿ∫ÿ© ÿ£ÿÆÿ±Ÿâ ŸÑÿß ÿ£ÿπÿ±ŸÅŸáÿß.

ŸÉŸÜÿ™ ÿ£ÿ≥ÿ™ÿ∑Ÿäÿπ ÿßÿ™ÿ®ÿßÿπ ŸÉŸÑÿßŸÖŸáÿß ÿπŸÜ ÿ∑ÿ±ŸäŸÇ ÿ£ÿ≥ÿ¶ŸÑÿ© ŸÖÿ™ŸÉÿ±ÿ±ÿ©. ÿπŸÜÿØŸÖÿß ŸÇŸÑÿ™ ŸÑŸáÿß ÿ•ŸÜŸÜŸä Ÿäÿ¨ÿ® ÿ£ŸÜ ÿ£ÿ∞Ÿáÿ® ÿßŸÑÿ¢ŸÜÿå Ÿàÿ£ŸÜŸÜŸä ŸÖÿ¥ÿ∫ŸàŸÑ ŸÅŸä ÿ¥ÿ§ŸàŸÜ ŸÖŸáŸÖÿ©ÿå ÿ≥ÿ£ŸÑÿ™ ŸÖÿ±ÿ© ÿ£ÿÆÿ±Ÿâ: "ŸáŸÑ ÿ™ÿπÿ±ŸÅ ŸÖÿß ÿßŸÑŸäŸàŸÖÿü" ÿ£ÿ¨ÿ®ÿ™Ÿáÿß ÿ£ŸÜ ÿßŸÑŸäŸàŸÖ ÿßŸÑÿ±ÿßÿ®ÿπ ŸÖŸÜ ŸÖÿßŸäŸà.

ŸàŸÇŸÅÿ™ ÿ±ÿ£ÿ≥Ÿáÿß ŸàŸáŸä ÿ™ŸÇŸàŸÑ ŸÖÿ±ÿ© ÿ£ÿÆÿ±Ÿâ: "ŸÜÿπŸÖ! ÿ£ÿπÿ±ŸÅ ÿ∞ŸÑŸÉ! ÿ£ÿπÿ±ŸÅ ÿ∞ŸÑŸÉÿå ŸàŸÑŸÉŸÜ ŸáŸÑ ÿ™ÿπÿ±ŸÅ ŸÖÿß ÿßŸÑŸäŸàŸÖÿü" ÿπŸÜÿØŸÖÿß ŸÇŸÑÿ™ ŸÑŸáÿß ÿ•ŸÜŸÜŸä ŸÑÿß ÿ£ŸÅŸáŸÖÿåÁ∂öÿ™ ÿπŸÑŸâ ŸÇÿßÿ¶ŸÑŸä:

"ŸáŸà ÿßŸÑŸäŸàŸÖ ŸÇÿ®ŸÑ ÿπŸäÿØ ÿßŸÑŸÇÿØŸäÿ≥ ÿ¨Ÿàÿ±ÿ¨. ŸáŸÑ ŸÑÿß ÿ™ÿπÿ±ŸÅ ÿ£ŸÜ Ÿáÿ∞ÿß ÿßŸÑŸÑŸäŸÑÿå ÿπŸÜÿØ ÿµŸàÿ™ ÿßŸÑÿ≥ÿßÿπÿ© ŸÅŸä ŸÖŸÜÿ™ÿµŸÅ ÿßŸÑŸÑŸäŸÑÿå ÿ≥Ÿäÿ™ŸÖÿ™ÿπ ŸÉŸÑ ÿßŸÑÿ¥ÿ±Ÿàÿ± ŸÅŸä ÿßŸÑÿπÿßŸÑŸÖ ÿ®ÿßŸÑÿ≥Ÿäÿ∑ÿ±ÿ©ÿü ŸáŸÑ ŸÑÿß ÿ™ÿπÿ±ŸÅ ÿ∞ŸÑŸÉÿü

ÿπÿ±ŸÅÿ™ ŸÖŸÉÿßŸÜŸÉ Ÿà ŸÖÿß ÿ≥ÿ™ÿ∞Ÿáÿ® ÿ•ŸÑŸäŸáÿü ŸÉÿßŸÜÿ™ ŸÅŸä ÿ≠ÿßŸÑÿ© ŸÖŸÜ ÿßŸÑŸÇŸÑŸÇ ÿßŸÑÿ®ŸÑŸäÿ∫ÿå ŸÅÿ≠ÿßŸàŸÑÿ™ ÿ£ŸÜ ÿ£ÿ∑ŸÖÿ¶ŸÜŸáÿß ÿØŸàŸÜ ÿ¨ÿØŸàŸâ. ÿ´ŸÖ ÿßŸÜŸáÿ∂ÿ™ ÿπŸÑŸâ ÿ±ŸÉÿ®ÿ™Ÿä Ÿàÿ™Ÿàÿ≥ŸÑÿ™ ÿ•ŸÑŸäŸë ÿ£ŸÜ ŸÑÿß ÿ£ÿ∫ÿßÿØÿ±Ÿáÿßÿõ ÿπŸÑŸâ ÿßŸÑÿ£ŸÇŸÑ ÿ£ŸÜ ÿ¢ÿÆŸèÿ∞ ŸäŸàŸÖŸãÿß ÿ£Ÿà ÿßÿ´ŸÜŸäŸÜ ŸÇÿ®ŸÑ ÿßŸÑÿÆÿ±Ÿàÿ¨.

ŸÉÿßŸÜ ŸÉŸÑ ÿ∞ŸÑŸÉ ÿ£ŸÖÿ±Ÿãÿß ÿ≥ÿÆŸäŸÅŸãÿßÿå ŸÑŸÉŸÜŸÜŸä ŸÑŸÖ ÿ£ÿ¥ÿπÿ± ÿ®ÿßŸÑÿ±ÿßÿ≠ÿ©. ŸàŸÖÿπ ÿ∞ŸÑŸÉÿå ŸÉÿßŸÜ ŸáŸÜÿßŸÉ ÿ£ÿπŸÖÿßŸÑ Ÿäÿ¨ÿ® ÿ•ŸÜÿ¨ÿßÿ≤Ÿáÿßÿå ŸàŸÑÿß ŸäŸÖŸÉŸÜ ŸÑŸä ÿ£ŸÜ ÿ£ŸÑŸàŸä ÿ£Ÿä ÿ¥Ÿäÿ° ŸÖŸÜ ÿ£ÿ¨ŸÑŸáÿß.

ÿ≠ÿßŸàŸÑÿ™ ÿ£ŸÜ ÿ£ÿπŸäÿØŸáÿß ÿπŸÑŸâ ŸÇŸàÿ™Ÿáÿß ŸàŸÇŸàŸÑÿ™ ŸÑŸáÿß ÿ®ÿ¨ÿØŸäÿ© ŸÖÿß ÿ¥ŸÉÿ±ÿ™Ÿáÿßÿå ŸÑŸÉŸÜ Ÿàÿßÿ¨ÿ®Ÿä ŸÉÿßŸÜ ÿ£ŸÖÿ±Ÿãÿß ŸÑÿßÿ≤ŸÖŸãÿßÿå Ÿàÿ®ÿßŸÑÿ™ÿßŸÑŸä Ÿäÿ¨ÿ® ÿπŸÑŸäŸë ÿßŸÑÿÆÿ±Ÿàÿ¨.

ÿ´ŸÖÁ´ôÿ™ Ÿàÿ£ÿ≥ŸÇŸÖÿ™ ÿπŸäŸÜŸäŸáÿßÿå ÿ´ŸÖ ÿ£ÿÆÿ∞ÿ™ ÿµŸÑŸäÿ®Ÿãÿß ŸÖŸÜ ÿ≠ŸàŸÑ ÿπŸÜŸÇŸáÿß ŸàŸÇÿØŸÖÿ™Ÿá ŸÑŸä.

"""ÿ±ÿ£ŸäŸä ÿ®ÿµÿ±ÿßÿ≠ÿ©: ÿßŸÑÿ™ÿ±ÿ¨ŸÖÿ© ÿ¨ŸäÿØÿ© ŸÉÿ®ÿØÿßŸäÿ© ŸÑŸÅŸáŸÖ ÿßŸÑŸÖÿπŸÜŸâ ÿßŸÑÿπÿßŸÖÿå ŸàŸÑŸÉŸÜŸáÿß "ÿ∫Ÿäÿ± ÿµÿßŸÑÿ≠ÿ© ŸÑŸÑŸÜÿ¥ÿ±" ÿ£Ÿà ÿßŸÑŸÇÿ±ÿßÿ°ÿ© ÿßŸÑŸÖŸÖÿ™ÿπÿ© ÿ®Ÿàÿ∂ÿπŸáÿß ÿßŸÑÿ≠ÿßŸÑŸä.

ÿ•ŸÑŸäŸÉ ÿ™ÿ≠ŸÑŸäŸÑŸä ÿßŸÑÿ™ŸÅÿµŸäŸÑŸä ŸÑŸÑŸÖÿ¥ÿßŸÉŸÑ ÿßŸÑŸÖŸàÿ¨ŸàÿØÿ© ŸÅŸä ÿßŸÑŸÜÿµ (ŸàŸÑŸÖÿßÿ∞ÿß ÿ≠ÿØÿ´ÿ™)ÿå ŸàŸÉŸäŸÅŸäÿ© ÿ•ÿµŸÑÿßÿ≠Ÿáÿß ÿ®ÿ±ŸÖÿ¨ŸäÿßŸã:
1. ÿßŸÑŸÖÿ¥ÿßŸÉŸÑ ÿßŸÑŸÉÿ®ÿ±Ÿâ ŸÅŸä ÿßŸÑŸÜÿµ (The Issues):

    ÿ®ŸÇÿßŸäÿß ÿßŸÑÿ•ŸÜÿ¨ŸÑŸäÿ≤Ÿäÿ© (Code Switching):
    ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ "ÿ™ŸÉÿ≥ÿßŸÑ" ÿπŸÜ ÿ™ÿ±ÿ¨ŸÖÿ© ÿ®ÿπÿ∂ ÿßŸÑŸÉŸÑŸÖÿßÿ™ Ÿàÿ™ÿ±ŸÉŸáÿß ŸÉŸÖÿß ŸáŸä:

        ÿ£ŸÖÿ´ŸÑÿ©: impressionÿå Saxonsÿå Wallachsÿå Magyarsÿå paprikaÿå sorts of.

        ÿßŸÑÿ≥ÿ®ÿ®: ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ÿßÿπÿ™ÿ®ÿ± Ÿáÿ∞Ÿá ÿßŸÑŸÉŸÑŸÖÿßÿ™ ŸÖÿµÿ∑ŸÑÿ≠ÿßÿ™ ÿπŸÑŸÖŸäÿ© ÿ£Ÿà ÿ£ÿ≥ŸÖÿßÿ° ŸÑÿß ÿ™ÿ™ÿ±ÿ¨ŸÖÿå ÿ£Ÿà ÿ£ŸÜ "ÿßŸÑŸÖŸàÿ¨Ÿá" ŸÑŸÖ ŸäŸÉŸÜ ÿµÿßÿ±ŸÖÿßŸã ÿ®ŸÖÿß ŸäŸÉŸÅŸä ŸÑÿ™ÿπÿ±Ÿäÿ® ÿßŸÑÿ£ÿ≥ŸÖÿßÿ°.

    "ÿÆŸÑŸÑ" ÿßŸÑÿ≠ÿ±ŸàŸÅ ÿßŸÑÿµŸäŸÜŸäÿ© (Glitches):
    ÿ∏Ÿáÿ±ÿ™ ÿ≠ÿ±ŸàŸÅ ÿµŸäŸÜŸäÿ© Ÿàÿ≥ÿ∑ ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿßŸÑÿπÿ±ÿ®Ÿäÿ©!

        ÿ£ŸÖÿ´ŸÑÿ©: Áª≠ÿ™ (ÿ®ÿØŸÑÿßŸã ŸÖŸÜ "ÿßÿ≥ÿ™ŸÖÿ±ÿ™")ÿå Á´ôÿ™ (ÿ®ÿØŸÑÿßŸã ŸÖŸÜ "ŸàŸÇŸÅÿ™").

        ÿßŸÑÿ≥ÿ®ÿ®: Ÿáÿ∞ÿß Ÿäÿ≠ÿØÿ´ ÿ∫ÿßŸÑÿ®ÿßŸã ÿπŸÜÿØŸÖÿß ÿ™ŸÉŸàŸÜ ÿßŸÑŸÄ Temperature ŸÖŸÜÿÆŸÅÿ∂ÿ© ÿ¨ÿØÿßŸã (0.1)ÿå ŸÖŸÖÿß Ÿäÿ¨ÿπŸÑ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ŸäÿπŸÑŸÇ ŸÅŸä "ÿ™ŸàŸÉŸÜ" ÿÆÿßÿ∑ÿ¶ ŸàŸÑÿß Ÿäÿ≥ÿ™ÿ∑Ÿäÿπ ÿßŸÑÿÆÿ±Ÿàÿ¨ ŸÖŸÜŸá.

    ÿ£ÿÆÿ∑ÿßÿ° ŸÅŸä ÿßŸÑŸÖÿπŸÜŸâ (ŸÉÿßÿ±ÿ´Ÿäÿ© ŸàŸÖÿ∂ÿ≠ŸÉÿ©):

        "ÿ£ÿ≠ÿ®ŸÉ... ÿØÿ±ÿßŸÉŸàŸÑÿß": ÿÆÿ™ŸÖ ÿßŸÑÿ±ÿ≥ÿßŸÑÿ© ÿ®ŸÉŸÑŸÖÿ© "ÿ£ÿ≠ÿ®ŸÉ"! (ŸÅŸä ÿßŸÑŸÜÿµ ÿßŸÑÿ£ÿµŸÑŸä "Your friend" ÿ£Ÿà "Yours"). ÿØÿ±ÿßŸÉŸàŸÑÿß Ÿàÿ≠ÿ¥ ŸÖÿ±ÿπÿ® ŸàŸÑŸäÿ≥ ŸÖÿ≠ÿ®ÿßŸã ŸàŸÑŸáÿßŸÜÿßŸã! üòÇ

        "ÿßŸÑŸÇÿ≥Ÿäÿ≥": Ÿäÿ®ÿØŸà ÿ£ŸÜŸá ÿ™ÿ±ÿ¨ŸÖ ŸÉŸÑŸÖÿ© ŸÖÿß ÿÆÿ∑ÿ£Ÿãÿå ŸÅÿØÿ±ÿßŸÉŸàŸÑÿß ŸáŸà "ŸÉŸàŸÜÿ™" (Count) ŸàŸÑŸäÿ≥ ŸÇÿ≥Ÿäÿ≥ÿßŸã.

    ÿßŸÑÿ±ŸÉÿßŸÉÿ© ÿßŸÑŸÑÿ∫ŸàŸäÿ©:

        ÿ¨ŸÖŸÑ ŸÖÿ´ŸÑ "ÿßŸÑ impression ÿßŸÑÿ™Ÿä ÿ≠ÿµŸÑÿ™ ÿπŸÑŸäŸáÿß" ÿ∂ÿπŸäŸÅÿ© ÿ¨ÿØÿßŸã.
"""







import ollama
import os
import time
import re
from tqdm.notebook import tqdm

# ------------------- Configuration -------------------
INPUT_FILE = "text.txt"
OUTPUT_FILE = "dracula_arabic.txt"
MODEL_NAME = "llama3.1:8b"
CHUNK_SIZE = 1200

# ------------------- LITERARY SYSTEM PROMPT -------------------
# Ÿáÿ∞ÿß ÿßŸÑŸÖŸàÿ¨Ÿá ŸÖÿÆÿµÿµ ŸÑŸÑÿ£ÿØÿ® ŸàÿßŸÑÿ±ŸàÿßŸäÿßÿ™
# Ÿäÿ∑ŸÑÿ® ŸÖŸÜ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ÿ™ÿπÿ±Ÿäÿ® ÿßŸÑÿ£ÿ≥ŸÖÿßÿ° (Transliteration) ÿ®ÿØŸÑÿßŸã ŸÖŸÜ ÿ™ÿ±ŸÉŸáÿß ÿ®ÿßŸÑÿ•ŸÜÿ¨ŸÑŸäÿ≤Ÿäÿ©
SYSTEM_PROMPT = (
    "ROLE: You are a professional literary translator aimed at Arab readers.\n"
    "TASK: Translate the novel text into fluent, captivating Modern Standard Arabic.\n"
    "CRITICAL RULES:\n"
    "1. NO ENGLISH LEFT: You must transliterate proper names (e.g., 'Saxons' -> 'ÿßŸÑÿ≥ŸÉÿ≥ŸàŸÜŸäŸàŸÜ', 'Bistritz' -> 'ÿ®Ÿäÿ≥ÿ™ÿ±Ÿäÿ™ÿ≤'). Do not leave ANY Latin characters.\n"
    "2. NO GLITCHES: Do NOT use Chinese characters (like Á´ô, Áª≠) or Hindi characters. Use ONLY Arabic script.\n"
    "3. TONE: Maintain the atmospheric, gothic, and antique tone of the novel (19th century style).\n"
    "4. ACCURACY: Translate 'Count' as 'ÿßŸÑŸÉŸàŸÜÿ™' (not priest). Translate endings like 'Yours' as 'ÿßŸÑŸÖÿÆŸÑÿµ ŸÑŸÉ' (not I love you).\n"
    "5. OUTPUT: Provide ONLY the Arabic text."
)

def remove_foreign_chars(text):
    """
    ÿØÿßŸÑÿ© ÿ™ŸÜÿ∏ŸäŸÅ ÿ•ÿ∂ÿßŸÅŸäÿ© ŸÑÿ•ÿ≤ÿßŸÑÿ© ÿ£Ÿä ÿ≠ÿ±ŸàŸÅ ÿµŸäŸÜŸäÿ© ÿ£Ÿà ÿ•ŸÜÿ¨ŸÑŸäÿ≤Ÿäÿ© ŸÇÿØ ÿ™ÿ™ÿ≥ÿ±ÿ®
    """
    # ÿßŸÑÿ≥ŸÖÿßÿ≠ ŸÅŸÇÿ∑ ÿ®ÿßŸÑÿπÿ±ÿ®Ÿäÿ©ÿå ÿßŸÑÿ£ÿ±ŸÇÿßŸÖÿå ŸàÿπŸÑÿßŸÖÿßÿ™ ÿßŸÑÿ™ÿ±ŸÇŸäŸÖ ÿßŸÑÿ£ÿ≥ÿßÿ≥Ÿäÿ©
    # Ÿáÿ∞ÿß ÿßŸÑÿ™ÿπÿ®Ÿäÿ± ÿßŸÑŸÜŸÖÿ∑Ÿä (Regex) Ÿäÿ≠ÿ∞ŸÅ ÿßŸÑÿ≠ÿ±ŸàŸÅ ÿßŸÑŸÑÿßÿ™ŸäŸÜŸäÿ© ŸàÿßŸÑÿµŸäŸÜŸäÿ© ŸàÿßŸÑŸáŸÜÿØŸäÿ©
    clean_text = re.sub(r'[a-zA-Z]', '', text) # ÿ≠ÿ∞ŸÅ ÿßŸÑÿ•ŸÜÿ¨ŸÑŸäÿ≤Ÿäÿ©
    # (ŸäŸÖŸÉŸÜ ÿ•ÿ∂ÿßŸÅÿ© ÿßŸÑŸÖÿ≤ŸäÿØ ŸÖŸÜ ÿßŸÑŸÅŸÑÿßÿ™ÿ± ŸáŸÜÿß ÿ•ÿ∞ÿß ŸÑÿ≤ŸÖ ÿßŸÑÿ£ŸÖÿ±)
    return clean_text

def split_text_smart(text, max_length):
    chunks = []
    while len(text) > max_length:
        split_index = text.rfind('\n', 0, max_length)
        if split_index == -1: split_index = text.rfind('.', 0, max_length)
        if split_index == -1: split_index = text.rfind(' ', 0, max_length)
        if split_index == -1: split_index = max_length
        chunks.append(text[:split_index + 1])
        text = text[split_index + 1:].strip()
    if text: chunks.append(text)
    return chunks

def translate_segment(text_chunk, retries=3):
    for attempt in range(retries):
        try:
            response = ollama.chat(
                model=MODEL_NAME,
                messages=[
                    {'role': 'system', 'content': SYSTEM_PROMPT},
                    {'role': 'user', 'content': text_chunk},
                ],
                options={
                    'temperature': 0.4, # ÿ±ŸÅÿπŸÜÿß ÿßŸÑÿ≠ÿ±ÿßÿ±ÿ© ŸÇŸÑŸäŸÑÿßŸã ŸÑŸÑÿ≥ÿ±ÿØ ÿßŸÑŸÇÿµÿµŸä
                    'top_p': 0.9,
                    'num_predict': 2048,
                }
            )

            content = response['message']['content']

            # ÿßŸÑÿ™ÿ≠ŸÇŸÇ ŸÖŸÜ Ÿàÿ¨ŸàÿØ ÿ≠ÿ±ŸàŸÅ ÿµŸäŸÜŸäÿ© ÿ¥ÿßÿ¶ÿπÿ© ŸÅŸä ÿßŸÑÿ£ÿÆÿ∑ÿßÿ°
            if any(char in content for char in ['Á´ô', 'Áª≠', 'ÊòØ', 'ÁöÑ']):
                 raise ValueError("Detected Chinese glitch characters!")

            return content

        except Exception as e:
            print(f"\n[Warning] Attempt {attempt+1} failed. Reason: {e}")
            time.sleep(1)

    return "[Translation Failed for this part]"

def main():
    if not os.path.exists(INPUT_FILE):
        print("File not found.")
        return

    print("--- Starting Literary Translation (Dracula Mode) ---")

    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        full_text = f.read()

    chunks = split_text_smart(full_text, CHUNK_SIZE)
    print(f"Split into {len(chunks)} parts.")

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write("")

    for chunk in tqdm(chunks, desc="Translating"):
        result = translate_segment(chunk)

        # ÿ™ŸÜÿ∏ŸäŸÅ ÿ•ÿ∂ÿßŸÅŸä (ÿßÿÆÿ™Ÿäÿßÿ±Ÿä)
        # result = remove_foreign_chars(result)

        with open(OUTPUT_FILE, "a", encoding="utf-8") as f:
            f.write(result + "\n\n")

    print(f"\n[Done] Check {OUTPUT_FILE}")

    # ÿπÿ±ÿ∂ ÿπŸäŸÜÿ©
    if os.path.exists(OUTPUT_FILE):
        with open(OUTPUT_FILE, "r", encoding="utf-8") as f:
            print(f.read()[:500] + "...")

if __name__ == "__main__":
    main()







import ollama
import os
import time
import re
from tqdm.notebook import tqdm

# ------------------- Settings -------------------
INPUT_FILE = "/content/1.txt"           # The file you want to translate
OUTPUT_FILE = "translated_general.txt"
MODEL_NAME = "llama3.1:8b"
CHUNK_SIZE = 1000                 # Safe size to keep context

# ------------------- UNIVERSAL SYSTEM PROMPT -------------------
# This prompt guides the AI to behave like a human translator
# regardless of the text type (Fiction, Non-fiction, etc.)

SYSTEM_PROMPT = (
    "ROLE: You are an expert professional translator proficient in English and Modern Standard Arabic (Fusha).\n"
    "TASK: Translate the provided text into natural, fluent Arabic.\n"
    "GUIDELINES:\n"
    "1. TRANSFORMATION: Do not translate word-for-word. Translate the *meaning* and *context*.\n"
    "2. PROPER NOUNS: Transliterate names and places into Arabic phonetically (e.g., 'London' -> 'ŸÑŸÜÿØŸÜ', 'Harker' -> 'Ÿáÿßÿ±ŸÉÿ±'). Do NOT leave them in English.\n"
    "3. IDIOMS: Adapt idioms and letter closings to their Arabic equivalents (e.g., 'Yours' at end of letter -> 'ÿßŸÑŸÖÿÆŸÑÿµ ŸÑŸÉ').\n"
    "4. SCRIPT: The output must be 100% Arabic script. Do not use Hindi/Chinese characters.\n"
    "5. PUNCTUATION: Adjust punctuation to suit Arabic writing standards."
)

# ---------------------------------------------------------------

def split_text_smart(text, max_length):
    """
    Splits text attempting to keep paragraphs and sentences intact.
    """
    chunks = []
    while len(text) > max_length:
        # Priority 1: Newline (Paragraph)
        split_index = text.rfind('\n', 0, max_length)

        # Priority 2: Period (Sentence)
        if split_index == -1:
            split_index = text.rfind('.', 0, max_length)

        # Priority 3: Space (Word)
        if split_index == -1:
            split_index = text.rfind(' ', 0, max_length)

        # Fallback
        if split_index == -1:
            split_index = max_length

        chunks.append(text[:split_index + 1])
        text = text[split_index + 1:].strip()

    if text:
        chunks.append(text)
    return chunks

def translate_segment(text_chunk, retries=3):
    """
    Translates a chunk with retry logic to handle model instability.
    """
    for attempt in range(retries):
        try:
            response = ollama.chat(
                model=MODEL_NAME,
                messages=[
                    {'role': 'system', 'content': SYSTEM_PROMPT},
                    {'role': 'user', 'content': text_chunk},
                ],
                options={
                    # Temperature 0.35 is balanced:
                    # High enough to be creative with literary text.
                    # Low enough to avoid hallucinations/glitches.
                    'temperature': 0.35,
                    'top_p': 0.9,
                    'num_predict': 2048,
                }
            )

            content = response['message']['content']

            # Basic sanity check: If output is empty, retry
            if not content.strip():
                raise ValueError("Empty response received")

            return content

        except Exception as e:
            print(f"\n[Warning] Attempt {attempt+1} failed. Reason: {e}")
            time.sleep(1)

    return "[Error: Could not translate this segment]"

def main():
    if not os.path.exists(INPUT_FILE):
        print(f"[Error] File '{INPUT_FILE}' not found.")
        return

    print("--- Starting General Translation ---")

    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        full_text = f.read()

    chunks = split_text_smart(full_text, CHUNK_SIZE)
    print(f"Document split into {len(chunks)} parts.")

    # Clear output file
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write("")

    # Process
    for chunk in tqdm(chunks, desc="Translating"):
        result = translate_segment(chunk)

        with open(OUTPUT_FILE, "a", encoding="utf-8") as f:
            f.write(result + "\n\n")

    print(f"\n[Success] Translation saved to: {OUTPUT_FILE}")

    # Preview
    print("-" * 30)
    print("Preview:")
    if os.path.exists(OUTPUT_FILE):
        with open(OUTPUT_FILE, "r", encoding="utf-8") as f:
            print(f.read()[:600] + "...")
    print("-" * 30)

if __name__ == "__main__":
    main()

""":
1. ÿßŸÑÿ™ÿ≠ÿ≥ŸÜ ÿßŸÑŸÖŸÑÿ≠Ÿàÿ∏ (ÿßŸÑÿ¨ŸäÿØ):

    ÿßÿÆÿ™ŸÅÿßÿ° ÿßŸÑÿ£ÿ≠ÿ±ŸÅ ÿßŸÑÿµŸäŸÜŸäÿ© ŸàÿßŸÑŸáŸÜÿØŸäÿ©: ŸÑŸÖ ÿ™ÿπÿØ ÿ™ÿ∏Ÿáÿ± ÿ±ŸÖŸàÿ≤ ÿ∫ÿ±Ÿäÿ®ÿ© ŸÖÿ´ŸÑ Áª≠ ÿ£Ÿà ‡§è‡§úÿå ŸàŸáÿ∞ÿß ÿ•ŸÜÿ¨ÿßÿ≤ ŸÉÿ®Ÿäÿ± ÿ®ŸÅÿ∂ŸÑ ÿßŸÑŸÅŸÑÿßÿ™ÿ± Ÿàÿ∂ÿ®ÿ∑ ÿßŸÑÿ≠ÿ±ÿßÿ±ÿ©.

    ÿßŸÑŸÇÿ∂ÿßÿ° ÿπŸÑŸâ ÿßŸÑÿ•ŸÜÿ¨ŸÑŸäÿ≤Ÿäÿ©: ŸÖÿπÿ∏ŸÖ ÿßŸÑŸÉŸÑŸÖÿßÿ™ ÿ™Ÿèÿ±ÿ¨ŸÖÿ™ ÿ£Ÿà ÿπŸèÿ±Ÿëÿ®ÿ™ (ŸÖÿ´ŸÑ "ÿ®ÿ≥ÿ™ÿ±Ÿäÿ™ÿ≤"ÿå "ÿ®ÿ®ÿ±ŸäŸÉÿß ŸáÿßŸÜÿØŸäŸÑ").

    ÿßŸÑÿ≥ÿ±ÿØ ŸÖŸÅŸáŸàŸÖ: ŸäŸÖŸÉŸÜŸÉ ŸÇÿ±ÿßÿ°ÿ© ÿßŸÑŸÜÿµ ŸàŸÅŸáŸÖ ÿßŸÑŸÇÿµÿ© Ÿàÿ™ÿ™ÿßÿ®ÿπ ÿßŸÑÿ£ÿ≠ÿØÿßÿ´.

2. ÿßŸÑŸÖÿ¥ÿßŸÉŸÑ ÿßŸÑŸÖÿ≥ÿ™ŸÖÿ±ÿ© (ÿßŸÑÿ™Ÿä ÿ™ÿ≠ÿ™ÿßÿ¨ ŸÑÿ≠ŸÑ):

    ÿ®ÿ∑ÿ° ÿ¥ÿØŸäÿØ: ÿ∞ŸÉÿ±ÿ™ ÿ£ŸÜ ÿßŸÑÿ™ÿ±ÿ¨ŸÖÿ© "ÿßÿ≥ÿ™ŸÖÿ±ÿ™ ŸàŸÇÿ™ ŸÉÿ®Ÿäÿ±". Ÿáÿ∞ÿß ŸÖÿ™ŸàŸÇÿπ ŸÑÿ£ŸÜ ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ Llama 3.1 8B ÿ´ŸÇŸäŸÑ ŸÜÿ≥ÿ®ŸäÿßŸã ÿπŸÑŸâ ÿßŸÑŸÖÿπÿßŸÑÿ¨ (CPU).

    ÿ®ŸÇÿßŸäÿß ÿ™ÿπŸÑŸäŸÖÿßÿ™ (Leaking): ÿßŸÑŸÜŸÖŸàÿ∞ÿ¨ ŸÖÿß ÿ≤ÿßŸÑ Ÿäÿ∑ÿ®ÿπ ŸÖŸÑÿßÿ≠ÿ∏ÿßÿ™ ÿ®ÿßŸÑÿ•ŸÜÿ¨ŸÑŸäÿ≤Ÿäÿ© Ÿàÿ≥ÿ∑ ÿßŸÑŸÜÿµ ÿßŸÑÿπÿ±ÿ®Ÿä!

        ŸÖÿ´ÿßŸÑ: Note: I followed the guidelines...

        ŸÖÿ´ÿßŸÑ: Note: Transliteration of "St. George's Day"...

        ŸÖÿ´ÿßŸÑ: appearanceÿßŸã (ŸÉŸÑŸÖÿ© ÿ•ŸÜÿ¨ŸÑŸäÿ≤Ÿäÿ© ŸÖÿØŸÖÿ¨ÿ© ÿ®ÿ™ŸÜŸàŸäŸÜ ÿπÿ±ÿ®Ÿä! Ÿáÿ∞ÿß "Glitch" ÿ¨ÿØŸäÿØ ŸàŸÖÿ∂ÿ≠ŸÉ).

    ÿ£ÿÆÿ∑ÿßÿ° ÿ™ÿ±ÿ¨ŸÖÿ© ÿßŸÑÿ≥ŸäÿßŸÇ:

        "ÿ£ÿ≠ÿ®ŸÉ... ÿØÿ±ÿßŸÉŸàŸÑÿß": ŸÖÿß ÿ≤ÿßŸÑÿ™ ŸÖŸàÿ¨ŸàÿØÿ©! ŸÑŸÖ ŸäŸÅŸáŸÖ ÿ£ŸÜ Yours ÿ™ÿπŸÜŸä "ÿßŸÑŸÖÿÆŸÑÿµ ŸÑŸÉ".

        "ÿ£ÿ¨ŸÜÿ®Ÿä ÿ•ŸÜÿ¨ŸÑŸäÿ≤Ÿäÿü": (The English Herr?) ÿ™ÿ±ÿ¨ŸÖŸáÿß "ÿßŸÑÿ£ÿ¨ŸÜÿ®Ÿä" ÿ®ÿØŸÑÿßŸã ŸÖŸÜ "ÿßŸÑÿ≥ŸäÿØ".

        "ÿßŸÑÿØŸàŸÇÿ© ÿØÿ±ÿßŸÉŸàŸÑÿß": ÿ≠ŸàŸÑ ÿßŸÑŸÉŸàŸÜÿ™ ÿ•ŸÑŸâ ÿ£ŸÜÿ´Ÿâ (ÿØŸàŸÇÿ©)!

ÿßŸÑÿ≠ŸÑ ŸÑŸÖÿ¥ŸÉŸÑÿ© ÿßŸÑÿ®ÿ∑ÿ° + ÿßŸÑÿ¨ŸàÿØÿ© (The Final Optimization)

ŸÑÿ≠ŸÑ ŸÖÿ¥ŸÉŸÑÿ© ÿßŸÑÿ®ÿ∑ÿ°ÿå ÿ≥ŸÜŸÇŸàŸÖ ÿ®ÿÆÿ∑Ÿàÿ™ŸäŸÜ:

    ÿ™ŸÅÿπŸäŸÑ ÿ™ÿ≥ÿ±Ÿäÿπ GPU (ÿ•ÿ¨ÿ®ÿßÿ±Ÿä): ÿ∞ŸÉÿ±ÿ™ ŸÅŸä ÿßŸÑÿ®ÿØÿßŸäÿ© ÿ£ŸÜŸÉ ÿ™ÿ≥ÿ™ÿÆÿØŸÖ Colab. Ÿäÿ¨ÿ® ÿßŸÑÿ™ÿ£ŸÉÿØ ŸÖŸÜ ÿ£ŸÜŸÉ ÿ™ÿ≥ÿ™ÿÆÿØŸÖ T4 GPU ŸàŸÑŸäÿ≥ ÿßŸÑŸÖÿπÿßŸÑÿ¨ ŸÅŸÇÿ∑.

    ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ŸÜŸÖŸàÿ∞ÿ¨ ÿ£ÿµÿ∫ÿ± (Phi-3 Mini): ÿ•ÿ∞ÿß ŸÉŸÜÿ™ ŸÖÿµÿ±ÿßŸã ÿπŸÑŸâ ÿßŸÑÿ≥ÿ±ÿπÿ© ÿßŸÑŸÅÿßÿ¶ŸÇÿ©ÿå ŸÅŸÜŸÖŸàÿ∞ÿ¨ Phi-3 ŸÖŸÜ ŸÖÿßŸäŸÉÿ±Ÿàÿ≥ŸàŸÅÿ™ (3.8 ŸÖŸÑŸäÿßÿ± ÿ®ÿßÿ±ÿßŸÖÿ™ÿ±) ÿ£ÿ≥ÿ±ÿπ ÿ®ŸÄ 3 ÿ£ÿ∂ÿπÿßŸÅ ŸÖŸÜ Llama 3 (8 ŸÖŸÑŸäÿßÿ±)ÿå Ÿàÿ¨ŸäÿØ ÿ¨ÿØÿßŸã ŸÅŸä ÿßŸÑÿ™ÿ±ÿ¨ŸÖÿ©.

ÿ•ŸÑŸäŸÉ ÿßŸÑŸÉŸàÿØ ÿßŸÑŸÖÿ≠ÿØÿ´ ÿ®ÿßŸÑŸÉÿßŸÖŸÑ ÿßŸÑÿ∞Ÿä:

    Ÿäÿ≥ÿ™ÿÆÿØŸÖ ŸÜŸÖŸàÿ∞ÿ¨ Phi-3 (ÿßŸÑÿ£ÿ≥ÿ±ÿπ ŸàÿßŸÑÿ£ÿÆŸÅ).

    Ÿäÿ≠ÿ™ŸàŸä ÿπŸÑŸâ ŸÅŸÑÿ™ÿ± ÿµÿßÿ±ŸÖ (Strict Cleaner) Ÿäÿ≠ÿ∞ŸÅ ÿ£Ÿä ŸÖŸÑÿßÿ≠ÿ∏ÿßÿ™ ÿ•ŸÜÿ¨ŸÑŸäÿ≤Ÿäÿ© (Note: ...) ŸÖŸÜ ÿßŸÑŸÜÿßÿ™ÿ¨ ÿßŸÑŸÜŸáÿßÿ¶Ÿä ŸÇÿ®ŸÑ ÿ≠ŸÅÿ∏Ÿá.


"""

import ollama
import os
import re
import time
from tqdm.notebook import tqdm

# ------------------- Settings -------------------
INPUT_FILE = "text.txt"
OUTPUT_FILE = "translated_generic.txt"
MODEL_NAME = "llama3.1:8b"  # Best balance for quality
CHUNK_SIZE = 1000           # Optimal size for context

# ------------------- GENERIC SYSTEM PROMPT -------------------
# No hardcoded dictionaries. Relies on the model's intelligence.
SYSTEM_PROMPT = (
    "ROLE: You are a professional translator proficient in English and Modern Standard Arabic.\n"
    "TASK: Translate the provided text into fluent, natural Arabic based on the context.\n"
    "RULES:\n"
    "1. CONTEXT: Analyze the text style (narrative, technical, or formal) and translate accordingly.\n"
    "2. NO NOTES: Do NOT provide explanations, headers, or notes like 'Here is the translation'. Output ONLY the Arabic text.\n"
    "3. SCRIPT: Use only Arabic script. Transliterate proper names if necessary.\n"
    "4. PUNCTUATION: Adjust punctuation to match Arabic standards."
)

def remove_english_artifacts(text):
    """
    Filters out lines that contain mostly English characters.
    This solves the issue of 'Note: I followed the guidelines...' appearing in the output.
    """
    lines = text.split('\n')
    cleaned_lines = []

    for line in lines:
        # Check if the line has English letters
        if re.search(r'[a-zA-Z]', line):
            # Count English vs Arabic chars
            english_char_count = len(re.findall(r'[a-zA-Z]', line))
            total_len = len(line.strip())

            # If a line is more than 30% English, discard it (it's likely a note)
            if total_len > 0 and (english_char_count / total_len) > 0.3:
                continue

        cleaned_lines.append(line)

    return '\n'.join(cleaned_lines).strip()

def split_text_smart(text, max_length):
    chunks = []
    while len(text) > max_length:
        # Split by paragraph first
        split_index = text.rfind('\n', 0, max_length)
        if split_index == -1: split_index = text.rfind('.', 0, max_length)
        if split_index == -1: split_index = text.rfind(' ', 0, max_length)
        if split_index == -1: split_index = max_length

        chunks.append(text[:split_index + 1])
        text = text[split_index + 1:].strip()

    if text:
        chunks.append(text)
    return chunks

def translate_segment(text_chunk, retries=3):
    for attempt in range(retries):
        try:
            response = ollama.chat(
                model=MODEL_NAME,
                messages=[
                    {'role': 'system', 'content': SYSTEM_PROMPT},
                    {'role': 'user', 'content': text_chunk},
                ],
                options={
                    'temperature': 0.3, # Balanced for general use
                    'num_predict': 2048,
                }
            )

            content = response['message']['content']

            # Apply the filter to remove English notes immediately
            cleaned_content = remove_english_artifacts(content)

            return cleaned_content

        except Exception as e:
            print(f"\n[Warning] Attempt {attempt+1} failed. Error: {e}")
            time.sleep(1)

    return "" # Return empty string on failure rather than error message

def main():
    if not os.path.exists(INPUT_FILE):
        print(f"[Error] File '{INPUT_FILE}' not found.")
        return

    print("--- Starting General Translation ---")

    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        full_text = f.read()

    chunks = split_text_smart(full_text, CHUNK_SIZE)
    print(f"Document split into {len(chunks)} parts.")

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write("")

    for chunk in tqdm(chunks, desc="Translating"):
        result = translate_segment(chunk)

        if result:
            with open(OUTPUT_FILE, "a", encoding="utf-8") as f:
                f.write(result + "\n\n")

    print(f"\n[Success] Done! Saved to: {OUTPUT_FILE}")

    # Preview
    if os.path.exists(OUTPUT_FILE):
        print("-" * 30)
        with open(OUTPUT_FILE, "r", encoding="utf-8") as f:
            print(f.read()[:500] + "...")
        print("-" * 30)

if __name__ == "__main__":
    main()

"""

ŸÑŸÑÿ≠ÿµŸàŸÑ ÿπŸÑŸâ ÿ≥ÿ±ÿπÿ© ÿπÿßŸÑŸäÿ© ÿ¨ÿØÿßŸã ŸàŸÖÿ≥ÿßÿ≠ÿ© ÿµÿ∫Ÿäÿ±ÿ© (ÿ£ŸÇŸÑ ŸÖŸÜ 2 ÿ¨Ÿäÿ¨ÿß) ŸÖÿπ ÿØÿπŸÖ ÿ¨ŸäÿØ ŸÑŸÑÿπÿ±ÿ®Ÿäÿ©ÿå ÿ£ŸÜÿµÿ≠ŸÉ ÿ®ÿßÿ≥ÿ™ÿÆÿØÿßŸÖ ŸÖŸàÿØŸäŸÑ Gemma 2 (ŸÜÿ≥ÿÆÿ© 2B) ŸÖŸÜ ÿ¨Ÿàÿ¨ŸÑÿå ÿ£Ÿà Qwen 2.5 (ŸÜÿ≥ÿÆÿ© 1.5B).

ÿ•ŸÑŸäŸÉ ÿßŸÑÿÆŸäÿßÿ±ÿßÿ™ ÿßŸÑÿ£ÿÆŸÅ ÿπŸÑŸâ ÿßŸÑÿ•ÿ∑ŸÑÿßŸÇ:

    gemma2:2b (ÿßŸÑÿ≠ÿ¨ŸÖ: 1.6 ÿ¨Ÿäÿ¨ÿß ŸÅŸÇÿ∑) -> ŸÖŸÖÿ™ÿßÿ≤ Ÿàÿ≥ÿ±Ÿäÿπ ÿ¨ÿØÿßŸã.

    qwen2.5:1.5b (ÿßŸÑÿ≠ÿ¨ŸÖ: 1.1 ÿ¨Ÿäÿ¨ÿß ŸÅŸÇÿ∑) -> ÿ∑Ÿäÿßÿ±ÿ© (ÿ£ÿ≥ÿ±ÿπ ÿ¥Ÿäÿ° ŸÖŸÖŸÉŸÜ).

    phi3:mini (ÿßŸÑÿ≠ÿ¨ŸÖ: 2.3 ÿ¨Ÿäÿ¨ÿß) -> ÿßŸÑŸÜÿ≥ÿÆÿ© ÿßŸÑÿµÿ∫Ÿäÿ±ÿ© ŸÖŸÜ Phi3 (ÿ£ÿÆŸÅ ÿ®ŸÉÿ´Ÿäÿ± ŸÖŸÜ Medium)."""

!ollama pull gemma2:2b

!ollama list

import ollama
import os
import re
import time
from tqdm.notebook import tqdm

# ------------------- Settings -------------------
INPUT_FILE = "/content/1.txt"
OUTPUT_FILE = "translated_fast.txt"

# ÿßÿÆÿ™ÿ±ŸÜÿß Ÿáÿ∞ÿß ÿßŸÑŸÖŸàÿØŸäŸÑ ŸÑÿ£ŸÜŸá ÿÆŸÅŸäŸÅ ÿ¨ÿØÿßŸã (1.6 GB) ŸàŸäÿØÿπŸÖ ÿßŸÑÿπÿ±ÿ®Ÿäÿ©
MODEL_NAME = "gemma2:2b"
# ÿ®ÿØŸäŸÑ ÿ¢ÿÆÿ± ÿ≥ÿ±Ÿäÿπ ÿ¨ÿØÿßŸã: "qwen2.5:1.5b"

CHUNK_SIZE = 800  # ÿ≠ÿ¨ŸÖ ÿ£ÿµÿ∫ÿ± ŸÑÿ≥ÿ±ÿπÿ© ÿ£ŸÉÿ®ÿ±

# ------------------- SYSTEM PROMPT -------------------
SYSTEM_PROMPT = (
    "ROLE: Professional Translator.\n"
    "TASK: Translate text to Arabic.\n"
    "RULES:\n"
    "1. OUTPUT: Arabic only. No English characters. No Notes.\n"
    "2. NAMES: Transliterate names (e.g., Jonathan -> ÿ¨ŸàŸÜÿßÿ´ÿßŸÜ).\n"
    "3. MEANING: Translate context, not word-for-word."
)

def remove_english_artifacts(text):
    """Filter out English noise"""
    lines = text.split('\n')
    cleaned_lines = []
    for line in lines:
        if re.search(r'[a-zA-Z]', line):
            english_char_count = len(re.findall(r'[a-zA-Z]', line))
            if len(line) > 0 and (english_char_count / len(line)) > 0.4:
                continue
        cleaned_lines.append(line)
    return '\n'.join(cleaned_lines).strip()

def split_text_smart(text, max_length):
    chunks = []
    while len(text) > max_length:
        split_index = text.rfind('\n', 0, max_length)
        if split_index == -1: split_index = text.rfind('.', 0, max_length)
        if split_index == -1: split_index = max_length
        chunks.append(text[:split_index + 1])
        text = text[split_index + 1:].strip()
    if text: chunks.append(text)
    return chunks

def translate_segment(text_chunk, retries=3):
    for attempt in range(retries):
        try:
            response = ollama.chat(
                model=MODEL_NAME,
                messages=[
                    {'role': 'system', 'content': SYSTEM_PROMPT},
                    {'role': 'user', 'content': text_chunk},
                ],
                options={
                    'temperature': 0.2, # ÿ≠ÿ±ÿßÿ±ÿ© ŸÖŸÜÿÆŸÅÿ∂ÿ© ŸÑŸÑÿ≥ÿ±ÿπÿ© ŸàÿßŸÑÿØŸÇÿ©
                    'num_predict': 1024,
                }
            )
            content = response['message']['content']
            return remove_english_artifacts(content)
        except Exception as e:
            time.sleep(1)
    return ""

def main():
    if not os.path.exists(INPUT_FILE):
        print("File not found.")
        return

    # ÿ™ÿ≠ŸÖŸäŸÑ ÿßŸÑŸÖŸàÿØŸäŸÑ ÿ™ŸÑŸÇÿßÿ¶ŸäÿßŸã ÿ•ÿ∞ÿß ŸÑŸÖ ŸäŸÉŸÜ ŸÖŸàÿ¨ŸàÿØÿßŸã
    print(f"üöÄ Pulling model {MODEL_NAME} (Size: ~1.6 GB)...")
    try:
        ollama.pull(MODEL_NAME)
    except:
        pass

    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        full_text = f.read()

    chunks = split_text_smart(full_text, CHUNK_SIZE)
    print(f"‚ö° Processing {len(chunks)} chunks with {MODEL_NAME}...")

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write("")

    for chunk in tqdm(chunks, desc="Translating"):
        result = translate_segment(chunk)
        if result:
            with open(OUTPUT_FILE, "a", encoding="utf-8") as f:
                f.write(result + "\n\n")

    print(f"‚úÖ Done! Saved to: {OUTPUT_FILE}")

    # ÿπÿ±ÿ∂ ÿπŸäŸÜÿ©
    if os.path.exists(OUTPUT_FILE):
        print("-" * 30)
        with open(OUTPUT_FILE, "r", encoding="utf-8") as f:
            print(f.read()[:500] + "...")
        print("-" * 30)

if __name__ == "__main__":
    main()

"""gemmaa2 baaaaaaaaaaaaaaaaaaad"""









import ollama
import os
import re
import time
from tqdm.notebook import tqdm

# ------------------- Settings -------------------
INPUT_FILE = "text.txt"
OUTPUT_FILE = "dracula_final.txt"

# ÿ≥ŸÜÿπŸàÿØ ŸÑŸÄ Phi-3 Medium ŸÑÿ£ŸÜŸá ÿ£ŸÅÿ∂ŸÑ ÿ™Ÿàÿßÿ≤ŸÜ ÿ®ŸäŸÜ ÿßŸÑÿ∞ŸÉÿßÿ° ŸàÿßŸÑÿ≥ÿ±ÿπÿ© ŸÑŸÑŸÜÿµŸàÿµ ÿßŸÑÿ£ÿØÿ®Ÿäÿ©
# ÿßŸÑŸÜŸÖÿßÿ∞ÿ¨ ÿßŸÑÿµÿ∫Ÿäÿ±ÿ© (2B) ÿ™ŸÅÿ¥ŸÑ ŸÅŸä ÿßŸÑÿ±ŸàÿßŸäÿßÿ™ Ÿàÿ™ŸÉÿ±ÿ± ÿßŸÑŸÉŸÑÿßŸÖ
MODEL_NAME = "phi3:medium"

CHUNK_SIZE = 1000

# ------------------- SYSTEM PROMPT -------------------
SYSTEM_PROMPT = (
    "ROLE: Expert Arabic Translator for Novels.\n"
    "TASK: Translate the English text to Arabic.\n"
    "RULES:\n"
    "1. NO REPETITION: Never repeat sentences. If you get stuck, move to the next sentence.\n"
    "2. TRANSLITERATE: 'Munich' -> 'ŸÖŸäŸàŸÜÿÆ', 'Dracula' -> 'ÿØÿ±ÿßŸÉŸàŸÑÿß'.\n"
    "3. SCRIPT: Arabic only. No English/Japanese characters.\n"
    "4. STYLE: Narrative and descriptive."
)

def remove_garbage(text):
    """ÿ≠ÿ∞ŸÅ ÿßŸÑÿ™ŸÉÿ±ÿßÿ± ŸàÿßŸÑÿ£ÿ≥ÿ∑ÿ± ÿßŸÑÿ∫ÿ±Ÿäÿ®ÿ©"""
    lines = text.split('\n')
    cleaned = []
    seen = set()
    for line in lines:
        line = line.strip()
        # ÿ≠ÿ∞ŸÅ ÿßŸÑÿ≥ÿ∑ÿ± ÿ•ÿ∞ÿß ŸÉÿßŸÜ ÿ™ŸÉÿ±ÿßÿ±ÿßŸã ŸÑÿ≥ÿ∑ÿ± ÿ≥ÿßÿ®ŸÇ
        if line in seen:
            continue
        # ÿ≠ÿ∞ŸÅ ÿßŸÑÿ£ÿ≥ÿ∑ÿ± ÿßŸÑÿ™Ÿä ÿ™ÿ≠ÿ™ŸàŸä ÿ™ÿ¥ŸÉŸäŸÑ ŸÖÿ®ÿßŸÑÿ∫ ŸÅŸäŸá (ÿπŸÑÿßŸÖÿ© ÿßŸÑŸáŸÑŸàÿ≥ÿ©)
        if len(re.findall(r'[\u064B-\u065F]', line)) > len(line) * 0.4:
            continue

        if line:
            seen.add(line)
            cleaned.append(line)
    return '\n'.join(cleaned)

def split_text_smart(text, max_length):
    chunks = []
    while len(text) > max_length:
        split_index = text.rfind('\n', 0, max_length)
        if split_index == -1: split_index = text.rfind('.', 0, max_length)
        if split_index == -1: split_index = max_length
        chunks.append(text[:split_index + 1])
        text = text[split_index + 1:].strip()
    if text: chunks.append(text)
    return chunks

def translate_segment(text_chunk, retries=3):
    for attempt in range(retries):
        try:
            response = ollama.chat(
                model=MODEL_NAME,
                messages=[
                    {'role': 'system', 'content': SYSTEM_PROMPT},
                    {'role': 'user', 'content': text_chunk},
                ],
                options={
                    'temperature': 0.4,     # ÿ≠ÿ±ÿßÿ±ÿ© ŸÖÿ™Ÿàÿ≥ÿ∑ÿ© ŸÑŸÑÿ•ÿ®ÿØÿßÿπ ÿ®ÿØŸàŸÜ ÿ¨ŸÜŸàŸÜ
                    'repeat_penalty': 1.3,  # ŸáÿßŸÖ ÿ¨ÿØÿßŸã: ŸäŸÖŸÜÿπ ÿ™ŸÉÿ±ÿßÿ± ÿßŸÑÿ¨ŸÖŸÑ (ÿßŸÑÿ≠ŸÑ ŸÑŸÖÿ¥ŸÉŸÑÿ™ŸÉ)
                    'top_k': 50,            # ÿ™ŸÜŸàŸäÿπ ÿßŸÑŸÉŸÑŸÖÿßÿ™
                    'num_predict': 1500,
                }
            )
            content = response['message']['content']
            return remove_garbage(content)
        except Exception as e:
            print(f"Error: {e}")
            time.sleep(1)
    return "[Translation Failed]"

def main():
    if not os.path.exists(INPUT_FILE):
        print("File not found.")
        return

    print(f"üöÄ Pulling {MODEL_NAME}...")
    try:
        ollama.pull(MODEL_NAME)
    except:
        pass

    with open(INPUT_FILE, "r", encoding="utf-8") as f:
        full_text = f.read()

    chunks = split_text_smart(full_text, CHUNK_SIZE)
    print(f"‚ö° Translating {len(chunks)} parts...")

    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        f.write("")

    for chunk in tqdm(chunks, desc="Translating"):
        result = translate_segment(chunk)
        with open(OUTPUT_FILE, "a", encoding="utf-8") as f:
            f.write(result + "\n\n")

    print(f"‚úÖ Done! Check {OUTPUT_FILE}")

    if os.path.exists(OUTPUT_FILE):
        print("-" * 30)
        with open(OUTPUT_FILE, "r", encoding="utf-8") as f:
            print(f.read()[:500] + "...")
        print("-" * 30)

if __name__ == "__main__":
    main()