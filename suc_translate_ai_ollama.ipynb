{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bcab341dbb0b431586f96b10397ae8d1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f2c77961dcb44c59bfb22a87a7d2eea5",
              "IPY_MODEL_f48e8e4c64854ac9847af6dcc21aa3b7",
              "IPY_MODEL_5aa10f6f15b24eeab1d6f710c031657c"
            ],
            "layout": "IPY_MODEL_5b855eaed54e473ebd45017c1dce9147"
          }
        },
        "f2c77961dcb44c59bfb22a87a7d2eea5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_54fd3699332f4aa9b4ee81cb57302b91",
            "placeholder": "​",
            "style": "IPY_MODEL_61bd50953b5c4d2799823068a7762889",
            "value": "Translating: 100%"
          }
        },
        "f48e8e4c64854ac9847af6dcc21aa3b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c23f7bdcef4e4facaa4e55b2337c74f2",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_04b41bd28bce4be8abd2f6b1aa908c16",
            "value": 1
          }
        },
        "5aa10f6f15b24eeab1d6f710c031657c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be682891a717456baa18c5502a3f9c7f",
            "placeholder": "​",
            "style": "IPY_MODEL_2e1e9acb21bd4bb8bb3014a6c12544ca",
            "value": " 1/1 [04:48&lt;00:00, 288.13s/it]"
          }
        },
        "5b855eaed54e473ebd45017c1dce9147": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "54fd3699332f4aa9b4ee81cb57302b91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "61bd50953b5c4d2799823068a7762889": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c23f7bdcef4e4facaa4e55b2337c74f2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "04b41bd28bce4be8abd2f6b1aa908c16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "be682891a717456baa18c5502a3f9c7f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e1e9acb21bd4bb8bb3014a6c12544ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3826b35d8b8d4d1cb9c0386ba9c4adf1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_239428a71b3f452d9f5cc08984666019",
              "IPY_MODEL_21c5cd1aba90477495e41f3f323c3e22",
              "IPY_MODEL_6ab9f91ddbfe4b0792bfe8c1da280927"
            ],
            "layout": "IPY_MODEL_eb3699e2966d4f9ca704c68ee1131d6f"
          }
        },
        "239428a71b3f452d9f5cc08984666019": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0953cdc9c9b442529bcf3b8e33cc5f81",
            "placeholder": "​",
            "style": "IPY_MODEL_77122b9b36014b4e90dc5672caf8a865",
            "value": "Translating: 100%"
          }
        },
        "21c5cd1aba90477495e41f3f323c3e22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_06916acf24f94e5e9002bfa35b3dc475",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0d9812c513c94cab84941d18f625e0fc",
            "value": 1
          }
        },
        "6ab9f91ddbfe4b0792bfe8c1da280927": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_add38c9df6394be29d849f0b3750b92d",
            "placeholder": "​",
            "style": "IPY_MODEL_66b611a5804841a983a6cc75c989db04",
            "value": " 1/1 [05:26&lt;00:00, 326.45s/it]"
          }
        },
        "eb3699e2966d4f9ca704c68ee1131d6f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0953cdc9c9b442529bcf3b8e33cc5f81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77122b9b36014b4e90dc5672caf8a865": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "06916acf24f94e5e9002bfa35b3dc475": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d9812c513c94cab84941d18f625e0fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "add38c9df6394be29d849f0b3750b92d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66b611a5804841a983a6cc75c989db04": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "4e07d1f17afa453ca2edde8ac3afd437": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_76916399dc254ae1867b499137039ee7",
              "IPY_MODEL_3ffbd86ef40049e8887176ca4d1daae0",
              "IPY_MODEL_96049fa987f5441c84760e094d7a7698"
            ],
            "layout": "IPY_MODEL_2e10bd670c1340f491d2b4dceb4423a6"
          }
        },
        "76916399dc254ae1867b499137039ee7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a4de9a95f8a649f5b0ea5e92e4838acc",
            "placeholder": "​",
            "style": "IPY_MODEL_d84d52d247b64ea08cbfec8d59bd912d",
            "value": "Processing: 100%"
          }
        },
        "3ffbd86ef40049e8887176ca4d1daae0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b05afa04cdf542fd810a0e5716904429",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5d5cad7b6a2844b09486b819bc6c915d",
            "value": 1
          }
        },
        "96049fa987f5441c84760e094d7a7698": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64da4cd22e3647dc84df0ee3f2b5f5fe",
            "placeholder": "​",
            "style": "IPY_MODEL_aa113e316ee948abac24b65155cdc0fe",
            "value": " 1/1 [05:35&lt;00:00, 335.66s/it]"
          }
        },
        "2e10bd670c1340f491d2b4dceb4423a6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a4de9a95f8a649f5b0ea5e92e4838acc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d84d52d247b64ea08cbfec8d59bd912d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b05afa04cdf542fd810a0e5716904429": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5d5cad7b6a2844b09486b819bc6c915d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "64da4cd22e3647dc84df0ee3f2b5f5fe": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aa113e316ee948abac24b65155cdc0fe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f9431f3833d64504828fd6054d20c4a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_835b1f3bd2194361b4c2da7afc99859f",
              "IPY_MODEL_9928dd8889c04edc9b28afed7f14537d",
              "IPY_MODEL_20ae3146e3f14b2c8f657dfc1e21b613"
            ],
            "layout": "IPY_MODEL_6af04b13bcb64e6988526f534ce7fe20"
          }
        },
        "835b1f3bd2194361b4c2da7afc99859f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_85927480ea444ffbabdf2ec621ff616a",
            "placeholder": "​",
            "style": "IPY_MODEL_252f6ce6e0b84872bb90a478b9a71135",
            "value": "Translating: 100%"
          }
        },
        "9928dd8889c04edc9b28afed7f14537d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70e130e58cfb4964acce486ec2bfddd4",
            "max": 9,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_95236a32f0a5453b936d439fd3552c7c",
            "value": 9
          }
        },
        "20ae3146e3f14b2c8f657dfc1e21b613": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5f5124a726b24f73a9d4f94fa7cdded1",
            "placeholder": "​",
            "style": "IPY_MODEL_4cecd4be6499487a9a62cc576631700e",
            "value": " 9/9 [1:00:09&lt;00:00, 350.35s/it]"
          }
        },
        "6af04b13bcb64e6988526f534ce7fe20": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "85927480ea444ffbabdf2ec621ff616a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "252f6ce6e0b84872bb90a478b9a71135": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70e130e58cfb4964acce486ec2bfddd4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "95236a32f0a5453b936d439fd3552c7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "5f5124a726b24f73a9d4f94fa7cdded1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4cecd4be6499487a9a62cc576631700e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "afde0c3d92384a638f5eb4910faab878": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c8b51b1dd3f54ca2b1542e960f2c0b3f",
              "IPY_MODEL_9f306a87336d4a3b8a61eae6c98c71ed",
              "IPY_MODEL_667413fc7ae144dda1f9a3a5d20a2b58"
            ],
            "layout": "IPY_MODEL_2734297110764a27ae7508c76f54f197"
          }
        },
        "c8b51b1dd3f54ca2b1542e960f2c0b3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9990d62da1f47d691fcde94ec4fcc4a",
            "placeholder": "​",
            "style": "IPY_MODEL_63e1ffe83d59402db3571bc476d36415",
            "value": "Translating: 100%"
          }
        },
        "9f306a87336d4a3b8a61eae6c98c71ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_767fb82e377e42ebb8841eb7a5f09216",
            "max": 11,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8c2b61a8f9854ff48bf99d9db191108a",
            "value": 11
          }
        },
        "667413fc7ae144dda1f9a3a5d20a2b58": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bc7aed2928cc45c5a85a0f773c87032a",
            "placeholder": "​",
            "style": "IPY_MODEL_b8111ddaa9ee44d4a6c5cd0222f67980",
            "value": " 11/11 [1:02:51&lt;00:00, 286.92s/it]"
          }
        },
        "2734297110764a27ae7508c76f54f197": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9990d62da1f47d691fcde94ec4fcc4a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "63e1ffe83d59402db3571bc476d36415": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "767fb82e377e42ebb8841eb7a5f09216": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c2b61a8f9854ff48bf99d9db191108a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bc7aed2928cc45c5a85a0f773c87032a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8111ddaa9ee44d4a6c5cd0222f67980": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cb8e935b833a4ad9a98931674345d6ba": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1560891b509047e383bdae6966632662",
              "IPY_MODEL_a17b305ee4ed461bbeec10fc9e79cb30",
              "IPY_MODEL_75eb9b6c7cef41738b3dfbf322f1e7cf"
            ],
            "layout": "IPY_MODEL_a7ff0b1787f94fd793270ec93e091743"
          }
        },
        "1560891b509047e383bdae6966632662": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0c97a5bd08b041928cb726049f6d7231",
            "placeholder": "​",
            "style": "IPY_MODEL_99b512c11ca94bfebc75c0de335fe5cc",
            "value": "Translating: 100%"
          }
        },
        "a17b305ee4ed461bbeec10fc9e79cb30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5d08b33542be4ec99068b6c12fc968fd",
            "max": 4,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_0a58ad0273734128bc5da168e31dc4a6",
            "value": 4
          }
        },
        "75eb9b6c7cef41738b3dfbf322f1e7cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_162a77462be04d51a92d1ec432289673",
            "placeholder": "​",
            "style": "IPY_MODEL_ad27d7475bfd418bb7bd0f1e79014d43",
            "value": " 4/4 [16:01&lt;00:00, 225.51s/it]"
          }
        },
        "a7ff0b1787f94fd793270ec93e091743": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0c97a5bd08b041928cb726049f6d7231": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "99b512c11ca94bfebc75c0de335fe5cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d08b33542be4ec99068b6c12fc968fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a58ad0273734128bc5da168e31dc4a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "162a77462be04d51a92d1ec432289673": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad27d7475bfd418bb7bd0f1e79014d43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IqMj9cFdSIY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5KgqLGane5kB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "67zwfZJVe5nG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/mrseanryan/gpt-summarizer.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zdvDo5Cle5qL",
        "outputId": "9dc61068-b86f-43ec-ecd1-6d19f20b734c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'gpt-summarizer'...\n",
            "remote: Enumerating objects: 478, done.\u001b[K\n",
            "remote: Counting objects: 100% (133/133), done.\u001b[K\n",
            "remote: Compressing objects: 100% (81/81), done.\u001b[K\n",
            "remote: Total 478 (delta 75), reused 99 (delta 47), pack-reused 345 (from 1)\u001b[K\n",
            "Receiving objects: 100% (478/478), 748.90 KiB | 6.63 MiB/s, done.\n",
            "Resolving deltas: 100% (293/293), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!curl -fsSL https://ollama.com/install.sh | sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wiKwAF0Ue8Tw",
        "outputId": "3fb6ef60-735b-401f-da84-8007cd11fdcc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            ">>> Installing ollama to /usr/local\n",
            ">>> Downloading Linux amd64 bundle\n",
            "######################################################################## 100.0%\n",
            ">>> Creating ollama user...\n",
            ">>> Adding ollama user to video group...\n",
            ">>> Adding current user to ollama group...\n",
            ">>> Creating ollama systemd service...\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m systemd is not running\n",
            "\u001b[1m\u001b[31mWARNING:\u001b[m Unable to detect NVIDIA/AMD GPU. Install lspci or lshw to automatically detect and install GPU dependencies.\n",
            ">>> The Ollama API is now available at 127.0.0.1:11434.\n",
            ">>> Install complete. Run \"ollama\" from the command line.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama run llama3.1:8b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dmkScS9xfOoZ",
        "outputId": "2af8f6aa-3af2-49b1-f674-b1e1c2aff1ab"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?25l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[2K\u001b[1G\u001b[?25hError: 500 Internal Server Error: model requires more system memory (4.8 GiB) than is available (3.8 GiB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WMOJv2IAf1x9",
        "outputId": "d4fb9a7c-970c-4ff5-9707-9185ddc1be00"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME           ID              SIZE      MODIFIED       \n",
            "llama3.1:8b    46e0c10c039e    4.9 GB    17 seconds ago    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama run llama3.1:8b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfMtJ3Z7f3GW",
        "outputId": "1dc9372b-8817-4c9f-ac9d-4f26182e14bc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?25l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[2K\u001b[1G\u001b[?25hError: 500 Internal Server Error: model requires more system memory (4.8 GiB) than is available (3.9 GiB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0d54c7f",
        "outputId": "8556dec6-a875-4fe5-f880-8e85e5bc0618"
      },
      "source": [
        "print('Checking available Ollama models:')\n",
        "!ollama list"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking available Ollama models:\n",
            "NAME           ID              SIZE      MODIFIED      \n",
            "phi3:latest    4f2222927938    2.2 GB    6 seconds ago    \n",
            "llama3.1:8b    46e0c10c039e    4.9 GB    3 minutes ago    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull phi3"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dkJyAccuf6iI",
        "outputId": "7a1e628d-3aee-4c4a-8d97-6a90ed94f49f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama run phi3:latest"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o-YHjq57gdnI",
        "outputId": "eb3aa8b6-1f12-4437-eea8-5fa45dd0070a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?25l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[2K\u001b[1G\u001b[?25hError: 500 Internal Server Error: model requires more system memory (3.5 GiB) than is available (3.0 GiB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8556dec6-a875-4fe5-f880-8e85e5bc0618",
        "id": "dOQGBPRngtaE"
      },
      "source": [
        "print('Checking available Ollama models:')\n",
        "!ollama list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking available Ollama models:\n",
            "NAME           ID              SIZE      MODIFIED      \n",
            "phi3:latest    4f2222927938    2.2 GB    6 seconds ago    \n",
            "llama3.1:8b    46e0c10c039e    4.9 GB    3 minutes ago    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8556dec6-a875-4fe5-f880-8e85e5bc0618",
        "id": "X3bZCYowgsk3"
      },
      "source": [
        "print('Checking available Ollama models:')\n",
        "!ollama list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking available Ollama models:\n",
            "NAME           ID              SIZE      MODIFIED      \n",
            "phi3:latest    4f2222927938    2.2 GB    6 seconds ago    \n",
            "llama3.1:8b    46e0c10c039e    4.9 GB    3 minutes ago    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama run phi3:latest"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Ztu0Ulxg_zd",
        "outputId": "b46340dd-ff97-43fe-9ae7-2583aa1bd692"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?25l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[2K\u001b[1G\u001b[?25h\u001b[?2004h>>> \u001b[38;5;245mSend a message (/? for help)\u001b[28D\u001b[0m\u001b[Khi\n",
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?25l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[2K\u001b[1G\u001b[?25hHello\u001b[?25l\u001b[?25h there\u001b[?25l\u001b[?25h!\u001b[?25l\u001b[?25h How\u001b[?25l\u001b[?25h can\u001b[?25l\u001b[?25h I\u001b[?25l\u001b[?25h help\u001b[?25l\u001b[?25h you\u001b[?25l\u001b[?25h today\u001b[?25l\u001b[?25h?\u001b[?25l\u001b[?25h If\n",
            "\n",
            "\u001b[?25l\u001b[?25h>>> \u001b[38;5;245mSend a message (/? for help)\u001b[28D\u001b[0m^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cornsnake~=0.0.60 html2text==2024.2.26 json5==0.9.25 ollama==0.2.0 PyMuPDF==1.24.1 pyyaml==6.0.1 ruff==0.3.5"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40SJLJJAhGEN",
        "outputId": "09352931-6a22-48c4-b832-bc4296ceccc1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cornsnake~=0.0.60\n",
            "  Downloading cornsnake-0.0.104-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting html2text==2024.2.26\n",
            "  Downloading html2text-2024.2.26.tar.gz (56 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting json5==0.9.25\n",
            "  Downloading json5-0.9.25-py3-none-any.whl.metadata (30 kB)\n",
            "Collecting ollama==0.2.0\n",
            "  Downloading ollama-0.2.0-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting PyMuPDF==1.24.1\n",
            "  Downloading PyMuPDF-1.24.1-cp312-none-manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Collecting pyyaml==6.0.1\n",
            "  Downloading PyYAML-6.0.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting ruff==0.3.5\n",
            "  Downloading ruff-0.3.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (23 kB)\n",
            "Collecting httpx<0.28.0,>=0.27.0 (from ollama==0.2.0)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting PyMuPDFb==1.24.1 (from PyMuPDF==1.24.1)\n",
            "  Downloading PyMuPDFb-1.24.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.4 kB)\n",
            "INFO: pip is looking at multiple versions of cornsnake to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting cornsnake~=0.0.60\n",
            "  Downloading cornsnake-0.0.103-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<0.28.0,>=0.27.0->ollama==0.2.0) (4.12.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx<0.28.0,>=0.27.0->ollama==0.2.0) (2025.11.12)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<0.28.0,>=0.27.0->ollama==0.2.0) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx<0.28.0,>=0.27.0->ollama==0.2.0) (3.11)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from httpx<0.28.0,>=0.27.0->ollama==0.2.0) (1.3.1)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<0.28.0,>=0.27.0->ollama==0.2.0) (0.16.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<0.28.0,>=0.27.0->ollama==0.2.0) (4.15.0)\n",
            "Downloading json5-0.9.25-py3-none-any.whl (30 kB)\n",
            "Downloading ollama-0.2.0-py3-none-any.whl (9.5 kB)\n",
            "Downloading PyMuPDF-1.24.1-cp312-none-manylinux2014_x86_64.whl (3.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.9/3.9 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyYAML-6.0.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (724 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m725.0/725.0 kB\u001b[0m \u001b[31m42.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ruff-0.3.5-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyMuPDFb-1.24.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (30.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.8/30.8 MB\u001b[0m \u001b[31m20.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cornsnake-0.0.103-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: html2text\n",
            "  Building wheel for html2text (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for html2text: filename=html2text-2024.2.26-py3-none-any.whl size=33111 sha256=a9207c4e2569a5e42f1622dd64319c913eaa38fe8a7fdc8693d67db1d8c42285\n",
            "  Stored in directory: /root/.cache/pip/wheels/2b/01/23/578505d65e2a97d78bf1fe3fc8256ecf37572dc1df598b0eaf\n",
            "Successfully built html2text\n",
            "Installing collected packages: ruff, pyyaml, PyMuPDFb, json5, html2text, PyMuPDF, httpx, ollama, cornsnake\n",
            "  Attempting uninstall: ruff\n",
            "    Found existing installation: ruff 0.14.8\n",
            "    Uninstalling ruff-0.14.8:\n",
            "      Successfully uninstalled ruff-0.14.8\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 6.0.3\n",
            "    Uninstalling PyYAML-6.0.3:\n",
            "      Successfully uninstalled PyYAML-6.0.3\n",
            "  Attempting uninstall: httpx\n",
            "    Found existing installation: httpx 0.28.1\n",
            "    Uninstalling httpx-0.28.1:\n",
            "      Successfully uninstalled httpx-0.28.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "firebase-admin 6.9.0 requires httpx[http2]==0.28.1, but you have httpx 0.27.2 which is incompatible.\n",
            "gradio 5.50.0 requires ruff>=0.9.3, but you have ruff 0.3.5 which is incompatible.\n",
            "google-genai 1.53.0 requires httpx<1.0.0,>=0.28.1, but you have httpx 0.27.2 which is incompatible.\n",
            "google-adk 1.20.0 requires PyYAML<7.0.0,>=6.0.2, but you have pyyaml 6.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed PyMuPDF-1.24.1 PyMuPDFb-1.24.1 cornsnake-0.0.103 html2text-2024.2.26 httpx-0.27.2 json5-0.9.25 ollama-0.2.0 pyyaml-6.0.1 ruff-0.3.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install --upgrade ctransformers pymupdf"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLGJVemxh4zm",
        "outputId": "749ec16a-48b9-42bf-d4c3-a7d7a7d88177"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ctransformers\n",
            "  Downloading ctransformers-0.2.27-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: pymupdf in /usr/local/lib/python3.12/dist-packages (1.24.1)\n",
            "Collecting pymupdf\n",
            "  Downloading pymupdf-1.26.7-cp310-abi3-manylinux_2_28_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.12/dist-packages (from ctransformers) (0.36.0)\n",
            "Requirement already satisfied: py-cpuinfo<10.0.0,>=9.0.0 in /usr/local/lib/python3.12/dist-packages (from ctransformers) (9.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->ctransformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->ctransformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->ctransformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->ctransformers) (6.0.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->ctransformers) (2.32.4)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->ctransformers) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->ctransformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub->ctransformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->ctransformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->ctransformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->ctransformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->huggingface-hub->ctransformers) (2025.11.12)\n",
            "Downloading ctransformers-0.2.27-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pymupdf-1.26.7-cp310-abi3-manylinux_2_28_x86_64.whl (24.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.1/24.1 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pymupdf, ctransformers\n",
            "  Attempting uninstall: pymupdf\n",
            "    Found existing installation: PyMuPDF 1.24.1\n",
            "    Uninstalling PyMuPDF-1.24.1:\n",
            "      Successfully uninstalled PyMuPDF-1.24.1\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cornsnake 0.0.103 requires PyMuPDF~=1.24.0, but you have pymupdf 1.26.7 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed ctransformers-0.2.27 pymupdf-1.26.7\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gpt-summarizer\n",
        "!./go.sh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w16rD-pliB_M",
        "outputId": "86f9977d-e741-4d3d-a681-f1bc9689e8a3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gpt-summarizer\n",
            "./go.sh: line 3: poetry: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!poetry run python -m gpt_summarizer.main_cli"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cd0eBhFJik-s",
        "outputId": "b78dc7f2-9812-4c71-f77f-85d5a04522c4"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: poetry: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python /content/gpt-summarizer/gpt_summarizer/main_cli.py --help"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrLxBfywi5yk",
        "outputId": "b583c3f8-4a0f-490e-d7ea-f61449cbae1b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/gpt-summarizer/gpt_summarizer/main_cli.py\", line 11, in <module>\n",
            "    from . import summarizer\n",
            "ImportError: attempted relative import with no known parent package\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/gpt-summarizer/gpt_summarizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8uS15zRnjBmM",
        "outputId": "3a9ef4a1-7aaf-4d1f-d4e2-412adc04c24c"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gpt-summarizer/gpt_summarizer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python main_cli.py -h"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDw9PFZJjLkL",
        "outputId": "bd8178b5-39e1-4713-c90b-263a2853cb7c"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/gpt-summarizer/gpt_summarizer/main_cli.py\", line 11, in <module>\n",
            "    from . import summarizer\n",
            "ImportError: attempted relative import with no known parent package\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "# إنشاء ملف نصي تجريبي يحتوي على نص طويل (يمكنك تعديل النص)\n",
        "sample_text = \"\"\"\n",
        "Artificial intelligence (AI) is intelligence demonstrated by machines, as opposed to the natural intelligence displayed by animals including humans.\n",
        "AI research has been defined as the field of study of intelligent agents, which refers to any system that perceives its environment and takes actions that maximize its chance of achieving its goals.\n",
        "The term \"artificial intelligence\" had previously been used to describe machines that mimic and display \"human\" cognitive skills that are associated with the human mind, such as \"learning\" and \"problem-solving\".\n",
        "This definition has since been rejected by major AI researchers who now describe AI in terms of rationality and acting rationally, which does not limit how intelligence can be articulated.\n",
        "\"\"\"\n",
        "\n",
        "with open(\"text.txt\", \"w\") as f:\n",
        "    f.write(sample_text)\n",
        "\n",
        "print(\"تم إنشاء ملف text.txt بنجاح\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3bA-InmjPuM",
        "outputId": "4d998401-e0d3-4ba3-fb3e-c02c466a1fcd"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "تم إنشاء ملف text.txt بنجاح\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m gpt_summarizer --input_file text.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6EgA9i5jrPI",
        "outputId": "ee25591f-11f7-49e3-9b43-3e0fdc4e2690"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/bin/python3: No module named gpt_summarizer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd gpt-summarizer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9xPGBaGpjuNE",
        "outputId": "3b8a0e03-b04a-4ac6-e867-c34dde9afc88"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'gpt-summarizer'\n",
            "/content/gpt-summarizer/gpt_summarizer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ollama\n",
        "\n",
        "# قراءة الملف النصي\n",
        "with open(\"text.txt\", \"r\") as f:\n",
        "    text_content = f.read()\n",
        "\n",
        "# إرسال النص إلى Llama 3.1 للتلخيص\n",
        "response = ollama.chat(model='llama3.1:8b', messages=[\n",
        "  {\n",
        "    'role': 'user',\n",
        "    'content': f\"Summarize this text: {text_content}\",\n",
        "  },\n",
        "])\n",
        "\n",
        "print(\"--- الملخص من Llama 3.1 ---\")\n",
        "print(response['message']['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qtmiBCktkcs2",
        "outputId": "361f256c-d539-418c-a794-340069912936"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- الملخص من Llama 3.1 ---\n",
            "The text defines Artificial Intelligence (AI) as a type of machine-based intelligence that allows systems to perceive their environment and take actions to achieve their goals. It also notes that the traditional definition of AI as mimicking human cognitive skills has been rejected by researchers in favor of a broader understanding of AI as rationality and acting rationally, which encompasses a wider range of intelligent behaviors.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ollama\n",
        "\n",
        "# قراءة الملف النصي\n",
        "with open(\"text.txt\", \"r\") as f:\n",
        "    text_content = f.read()\n",
        "\n",
        "# إرسال النص إلى Llama 3.1 للتلخيص\n",
        "response = ollama.chat(model='llama3.1:8b', messages=[\n",
        "  {\n",
        "    'role': 'user',\n",
        "    'content': f\"لخص هذا النص باللغة العربية: {text_content}\",\n",
        "  },\n",
        "])\n",
        "\n",
        "print(\"--- الملخص من Llama 3.1 ---\")\n",
        "print(response['message']['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-do02UYJl4Lx",
        "outputId": "c37f1640-e5ed-4b56-acdc-ce4a7476dcbc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- الملخص من Llama 3.1 ---\n",
            "تُعرَف الذكاء الاصطناعي بأنه الذكاء الذي يظهره الماكينات، كما هو مختلف عن الذكاء الطبيعي المعروض من قبل الحيوانات بمن فيهم البشر.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ollama\n",
        "\n",
        "# قراءة الملف النصي\n",
        "with open(\"text.txt\", \"r\") as f:\n",
        "    text_content = f.read()\n",
        "\n",
        "# إرسال النص إلى Llama 3.1 للتلخيص\n",
        "response = ollama.chat(model='llama3.1:8b', messages=[\n",
        "  {\n",
        "    'role': 'user',\n",
        "    'content': f\"ترجم هذا النص الى اللغة العربية: {text_content}\",\n",
        "  },\n",
        "])\n",
        "\n",
        "print(\"--- الملخص من Llama 3.1 ---\")\n",
        "print(response['message']['content'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4de3AFS6mE-9",
        "outputId": "cf2b2a7f-bbe4-4157-8125-940b9b42c747"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- الملخص من Llama 3.1 ---\n",
            "الهندسة الجديدة (الذكاء الاصطناعي) هي الذكاء الذي يظهر في الآلات، وليس الذكاء الطبيعي المُظهَر في الحيوانات بما في ذلك الإنسان. وقد تم تعريف البحث عن الذكاء الاصطناعي على أنه مجال دراسة एजينتس ذكيه، ويشير إلى أي نظام يستطيع أن يدرك بيئته ويؤدي اكتساباته لتحقيق أهدافه.Using term \"artificial intelligence\" to describe machines that mimic and display \"human\" cognitive abilities, such as learning and problem-solving، تم رفض هذا التعريف بالفعل من قبل الأبحاث الرئيسية للذكاء الاصطناعي، الذين يقومون الآن بوصف الذكاء فيما يتعلق بنظريته وقدرته على اتخاذ القرارات المنطقية.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--- الملخص من Llama 3.1 ---\n",
        "الهندسة الجديدة (الذكاء الاصطناعي) هي الذكاء الذي يظهر في الآلات، وليس الذكاء الطبيعي المُظهَر في الحيوانات بما في ذلك الإنسان. وقد تم تعريف البحث عن الذكاء الاصطناعي على أنه مجال دراسة"
      ],
      "metadata": {
        "id": "jJyzxCttnRBT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "اكتب هذا (للحصول على ملخص عربي):\n",
        "code Python\n",
        "\n",
        "\n",
        "'content': f\"لخص هذا النص باللغة العربية: {text_content}\",\n",
        "\n",
        "\n",
        "\n",
        "أو إذا أردته كنقاط محددة:\n",
        "code Python\n",
        "\n",
        "\n",
        "'content': f\"Give me a summary of this text as bullet points: {text_content}\",\n",
        "\n",
        ""
      ],
      "metadata": {
        "id": "9crgJLlPl2pN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. ادخل إلى المجلد الرئيسي أولاً\n",
        "%cd /content/gpt-summarizer\n",
        "\n",
        "# 2. شغل الأمر واستدعي الموديول (لاحظ النقطة والشرطة السفلية)\n",
        "!python3 -m gpt_summarizer.main_cli text.txt --engine ollama --model llama3.1:8b"
      ],
      "metadata": {
        "id": "vNF2jB6xla-V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YdELtgd7mto7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AjqdzD0smtmp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zHfKHFjBmtjE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ollama\n",
        "\n",
        "# قراءة النص\n",
        "with open(\"text.txt\", \"r\") as f:\n",
        "    text_content = f.read()\n",
        "\n",
        "# إرسال النص مع تعليمات دقيقة (System Prompt)\n",
        "response = ollama.chat(model='llama3.1:8b', messages=[\n",
        "  {\n",
        "    'role': 'system',\n",
        "    'content': 'أنت خبير لغوي ومحلل نصوص. قم بتلخيص النص التالي باللغة العربية الفصحى السليمة، وركز على استخراج الأفكار الرئيسية بأسلوب مترابط وليس ترجمة حرفية.'\n",
        "  },\n",
        "  {\n",
        "    'role': 'user',\n",
        "    'content': text_content,\n",
        "  },\n",
        "])\n",
        "\n",
        "print(\"--- الملخص المحسن ---\")\n",
        "print(response['message']['content'])"
      ],
      "metadata": {
        "id": "yZKYj06zmtck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ollama\n",
        "import os\n",
        "\n",
        "# --- إعدادات ---\n",
        "input_file = \"text.txt\"           # اسم الملف الأصلي\n",
        "output_file = \"translated_text.txt\" # اسم ملف الترجمة\n",
        "chunk_size = 2000                 # عدد الأحرف في كل دفعة (حوالي 300-400 كلمة)\n",
        "model_name = \"llama3.1:8b\"        # الموديل المستخدم\n",
        "\n",
        "# 1. دالة لتقسيم النص إلى أجزاء\n",
        "def split_text(text, size):\n",
        "    # نقسم النص بحيث لا نقطع الكلمات (نحاول التقسيم عند المسافات)\n",
        "    chunks = []\n",
        "    while len(text) > size:\n",
        "        # نجد آخر مسافة قبل الحد الأقصى\n",
        "        split_index = text.rfind(' ', 0, size)\n",
        "        if split_index == -1: # إذا لم توجد مسافة (كلمة طويلة جداً)\n",
        "            split_index = size\n",
        "\n",
        "        chunks.append(text[:split_index])\n",
        "        text = text[split_index:]\n",
        "    chunks.append(text)\n",
        "    return chunks\n",
        "\n",
        "# 2. قراءة الملف الطويل\n",
        "if not os.path.exists(input_file):\n",
        "    print(f\"Error: File {input_file} not found!\")\n",
        "else:\n",
        "    with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        full_text = f.read()\n",
        "\n",
        "    # تقسيم النص\n",
        "    text_chunks = split_text(full_text, chunk_size)\n",
        "    print(f\"تم تقسيم الملف إلى {len(text_chunks)} جزء للترجمة...\")\n",
        "\n",
        "    # تنظيف ملف المخرجات القديم إن وجد\n",
        "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\")\n",
        "\n",
        "    # 3. البدء في الترجمة (جزء بجزء)\n",
        "    for i, chunk in enumerate(text_chunks):\n",
        "        print(f\"جاري ترجمة الجزء {i+1} من {len(text_chunks)}...\")\n",
        "\n",
        "        try:\n",
        "            response = ollama.chat(model=model_name, messages=[\n",
        "                {\n",
        "                    'role': 'system',\n",
        "                    'content': 'أنت مترجم محترف. قم بترجمة النص التالي إلى اللغة العربية بدقة وأسلوب أدبي متماسك. لا تضف أي مقدمات أو شروحات، فقط أعطني الترجمة.'\n",
        "                },\n",
        "                {\n",
        "                    'role': 'user',\n",
        "                    'content': chunk\n",
        "                },\n",
        "            ])\n",
        "\n",
        "            translated_chunk = response['message']['content']\n",
        "\n",
        "            # حفظ الترجمة مباشرة في الملف (append)\n",
        "            with open(output_file, \"a\", encoding=\"utf-8\") as f:\n",
        "                f.write(translated_chunk + \"\\n\\n\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"حدث خطأ في الجزء {i+1}: {e}\")\n",
        "\n",
        "    print(f\"\\n✅ تمت الترجمة بنجاح! الملف المحفوظ: {output_file}\")\n",
        "\n",
        "    # عرض أول جزء من الترجمة للتأكد\n",
        "    print(\"\\n--- مقتطف من الترجمة ---\")\n",
        "    with open(output_file, \"r\", encoding=\"utf-8\") as f:\n",
        "        print(f.read()[:500] + \"...\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kf5JdHv5nYDE",
        "outputId": "d0fbfee7-0874-46b5-d42c-d1403f1132d4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "تم تقسيم الملف إلى 1 جزء للترجمة...\n",
            "جاري ترجمة الجزء 1 من 1...\n",
            "\n",
            "✅ تمت الترجمة بنجاح! الملف المحفوظ: translated_text.txt\n",
            "\n",
            "--- مقتطف من الترجمة ---\n",
            "الذكاء الاصطناعي هو الذكاء الذي يظهره الآلات، على عكس الذكاء الطبيعي الذي يعرضه الحيوانات بما في ذلك البشر.  استُخدمت بحث الذكاء الاصطناعي للإشارة إلى مجال دراسة एजنتات ذكية، والتي تشير إلى أي نظام يدرك بيئته ويقوم بالاعتماد على إجراءات تؤدي إلى زيادة فرص تحقيق أهدافه.\n",
            "استُخدم المصطلح \"الذكاء الاصطناعي\" في السابق للإشارة إلى الآلات التي تقلد وتصور مهارات البشر المعرفية، مثل \"العلم\" والإجابة عن المسائل. \n",
            "تم رفض هذه التعريف منذ ذلك الحين من قبل بعض الباحثين الرئيسيين للذكاء الاصطناعي الذين يصفون ا...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# داخل حلقة الـ loop\n",
        "        response = ollama.chat(\n",
        "            model=model_name,\n",
        "            messages=[\n",
        "                {\n",
        "                    'role': 'system',\n",
        "                    'content': 'أنت مترجم تقني محترف. قم بترجمة النص إلى العربية بأسلوب علمي رصين. انتبه للمصطلحات التقنية (مثلاً: Agents = وكلاء ذكية، Learning = التعلم، Problem-solving = حل المشكلات). تجنب الترجمة الحرفية واكتب بلغة عربية سليمة نحوياً.'\n",
        "                },\n",
        "                {\n",
        "                    'role': 'user',\n",
        "                    'content': chunk\n",
        "                },\n",
        "            ],\n",
        "            # إضافة خيارات لتقليل العشوائية ومنع الهلوسة\n",
        "            options={\n",
        "                'temperature': 0.1,  # تقليل الحرارة لزيادة الدقة\n",
        "                'top_p': 0.9\n",
        "            }\n",
        "        )"
      ],
      "metadata": {
        "id": "O-03TeHBo5tY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ollama\n",
        "import os\n",
        "import time\n",
        "from tqdm.notebook import tqdm  # شريط تقدم لبيئة كولاب\n",
        "\n",
        "# ------------------- إعدادات المستخدم -------------------\n",
        "INPUT_FILE = \"text.txt\"           # اسم الملف الذي تريد ترجمته\n",
        "OUTPUT_FILE = \"translated_text.txt\" # اسم الملف الناتج\n",
        "MODEL_NAME = \"llama3.1:8b\"        # الموديل المستخدم\n",
        "CHUNK_SIZE = 1500                 # حجم الجزء (تم تقليله قليلاً لضمان جودة أعلى)\n",
        "# --------------------------------------------------------\n",
        "\n",
        "def split_text_smart(text, max_length):\n",
        "    \"\"\"\n",
        "    تقسيم النص بذكاء عند المسافات أو علامات الترقيم لعدم قطع الجمل.\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "    while len(text) > max_length:\n",
        "        # البحث عن أقرب نقطة أو مسافة قبل الحد الأقصى\n",
        "        # نحاول البحث عن نهاية جملة (.) أولاً ليكون التقسيم منطقياً\n",
        "        split_index = text.rfind('.', 0, max_length)\n",
        "\n",
        "        if split_index == -1: # إذا لم نجد نقطة، نبحث عن مسافة\n",
        "            split_index = text.rfind(' ', 0, max_length)\n",
        "\n",
        "        if split_index == -1: # إذا كانت جملة عملاقة جداً، نقطع عند الحد الأقصى\n",
        "            split_index = max_length\n",
        "\n",
        "        # إضافة الجزء للقائمة (مع احتساب النقطة في الجزء الأول)\n",
        "        chunks.append(text[:split_index + 1])\n",
        "        text = text[split_index + 1:].strip()\n",
        "\n",
        "    if text:\n",
        "        chunks.append(text)\n",
        "    return chunks\n",
        "\n",
        "def translate_segment(text_chunk, retries=3):\n",
        "    \"\"\"\n",
        "    دالة لترجمة جزء واحد مع محاولة إعادة المحاولة عند الفشل\n",
        "    \"\"\"\n",
        "    system_prompt = (\n",
        "        \"أنت مترجم تقني وأكاديمي محترف. \"\n",
        "        \"مهمتك: ترجمة النص التالي من الإنجليزية إلى العربية الفصحى السليمة. \"\n",
        "        \"القواعد: \"\n",
        "        \"1. استخدم مصطلحات علمية دقيقة (مثلاً: Agents = وكلاء، AI = الذكاء الاصطناعي). \"\n",
        "        \"2. حافظ على قواعد النحو (تذكير وتأنيث الأفعال بدقة). \"\n",
        "        \"3. لا تضف أي حوار جانبي، أعطني الترجمة فقط.\"\n",
        "    )\n",
        "\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = ollama.chat(\n",
        "                model=MODEL_NAME,\n",
        "                messages=[\n",
        "                    {'role': 'system', 'content': system_prompt},\n",
        "                    {'role': 'user', 'content': text_chunk},\n",
        "                ],\n",
        "                options={\n",
        "                    'temperature': 0.1,  # دقة عالية جداً ومنع الهلوسة\n",
        "                    'top_p': 0.9,\n",
        "                }\n",
        "            )\n",
        "            return response['message']['content']\n",
        "        except Exception as e:\n",
        "            print(f\"\\n⚠️ تحذير: فشلت المحاولة {attempt+1} لترجمة الجزء. الخطأ: {e}\")\n",
        "            time.sleep(2) # انتظار ثانيتين قبل المحاولة مرة أخرى\n",
        "\n",
        "    return \"[فشلت ترجمة هذا الجزء بعد عدة محاولات]\"\n",
        "\n",
        "# ------------------- التنفيذ الرئيسي -------------------\n",
        "\n",
        "# 1. التأكد من وجود الملف\n",
        "if not os.path.exists(INPUT_FILE):\n",
        "    print(f\"❌ خطأ: الملف {INPUT_FILE} غير موجود! تأكد من رفع الملف وتسميته بشكل صحيح.\")\n",
        "else:\n",
        "    print(\"🚀 جاري قراءة الملف وتقسيمه...\")\n",
        "    with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        full_text = f.read()\n",
        "\n",
        "    # تقسيم النص\n",
        "    chunks = split_text_smart(full_text, CHUNK_SIZE)\n",
        "    print(f\"📄 تم تقسيم النص إلى {len(chunks)} جزء. البدء في الترجمة...\\n\")\n",
        "\n",
        "    # مسح محتوى ملف المخرجات القديم لبدء كتابة جديدة\n",
        "    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\")\n",
        "\n",
        "    # 2. حلقة الترجمة مع شريط التقدم\n",
        "    # نستخدم tqdm لعرض شريط التحميل\n",
        "    for i, chunk in enumerate(tqdm(chunks, desc=\"جاري الترجمة\")):\n",
        "\n",
        "        translated_text = translate_segment(chunk)\n",
        "\n",
        "        # كتابة النتيجة فوراً للملف (حتى لا يضيع شيء لو انقطع الاتصال)\n",
        "        with open(OUTPUT_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(translated_text + \"\\n\\n\")\n",
        "\n",
        "    print(f\"\\n✅ تمت العملية بنجاح! تم حفظ الترجمة في: {OUTPUT_FILE}\")\n",
        "\n",
        "    # عرض عينة\n",
        "    print(\"-\" * 30)\n",
        "    print(\"👀 عينة من بداية الترجمة:\")\n",
        "    with open(OUTPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        print(f.read()[:500] + \"...\")\n",
        "    print(\"-\" * 30)"
      ],
      "metadata": {
        "id": "mjU4sHuRpIQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ollama\n",
        "import os\n",
        "import time\n",
        "from tqdm.notebook import tqdm # Colab Progress Bar\n",
        "\n",
        "# ------------------- User Settings -------------------\n",
        "INPUT_FILE = \"text.txt\" # Name of the file you want to translate\n",
        "OUTPUT_FILE = \"translated_text.txt\" # Name of the output file\n",
        "MODEL_NAME = \"llama3.1:8b\" # Model used\n",
        "CHUNK_SIZE = 1500 # Chunk size (slightly reduced for better quality)\n",
        "# --------------------------------------------------------\n",
        "\n",
        "def split_text_smart(text, max_length):\n",
        "\n",
        "\"\"\n",
        "Smartly splits text at spaces or punctuation marks to avoid breaking sentences.\n",
        "\n",
        "\"\"\n",
        "chunks = []\n",
        "\n",
        "while len(text) > max_length:\n",
        "\n",
        "# Finds the nearest point or space before the maximum\n",
        "\n",
        "# Attempts to find the end of a sentence (.) first for logical splitting\n",
        "split_index = text.rfind('.', 0, max_length)\n",
        "\n",
        "if split_index == -1: # If we don't find a point, we look for a distance\n",
        "\n",
        "split_index = text.rfind(' ', 0, max_length)\n",
        "\n",
        "if split_index == -1: # If the sentence is too long, we cut at the maximum length\n",
        "\n",
        "split_index = max_length\n",
        "\n",
        "# Add the segment to the list (counting the point in the first segment)\n",
        "\n",
        "chunks.append(text[:split_index + 1])\n",
        "\n",
        "text = text[split_index + 1:].strip()\n",
        "\n",
        "if text:\n",
        "\n",
        "chunks.append(text)\n",
        "\n",
        "return chunks\n",
        "\n",
        "def translate_segment(text_chunk, retries=3):\n",
        "\n",
        "\"\"\n",
        "\n",
        "A function to translate one segment and attempt to retry if it fails\n",
        "\n",
        "\"\"\n",
        "system_prompt = (\n",
        "\"You are a professional technical and academic translator.\"\n",
        "\n",
        "\"Your task: Translate the following text from English into Modern Standard Arabic.\"\n",
        "Rules:\n",
        "\n",
        "1. Use precise scientific terminology (e.g., Agents = Agents, AI = Artificial Intelligence).\n",
        "\n",
        "2. Maintain grammatical rules (precise gender agreement of verbs).\n",
        "\n",
        "3. Do not add any side dialogue; just provide the translation.\n",
        "\n",
        ")\n",
        "\n",
        "for attempt in range(retries):\n",
        "\n",
        "try:\n",
        "\n",
        "response = ollama.chat(\n",
        "model=MODEL_NAME,\n",
        "\n",
        "messages=[\n",
        "{'role': 'system', 'content': system_prompt},\n",
        "\n",
        "{'role': 'user', 'content': text_chunk},\n",
        "\n",
        "],\n",
        "\n",
        "options={\n",
        "'temperature': 0.1, # Very high accuracy and hallucination prevention\n",
        "\n",
        "'top_p': 0.9,\n",
        "\n",
        "}\n",
        ")\n",
        "\n",
        "return response['message']['content']\n",
        "\n",
        "except Exception as e:\n",
        "\n",
        "print(f\"\\n⚠️ Warning: The attempt to translate the part failed. Error: {e}\")\n",
        "\n",
        "time.sleep(2) # Wait two seconds before trying again\n",
        "\n",
        "return \"[This part failed to translate after several attempts]\"\n",
        "\n",
        "# ------------------- Main Implementation -------------------\n",
        "\n",
        "# 1. Check if the file exists\n",
        "if not os.path.exists(INPUT_FILE):\n",
        "\n",
        "print(f\"❌ Error: The file {INPUT_FILE} does not exist! Make sure you uploaded and named the file correctly.\")\n",
        "else:\n",
        "\n",
        "print(\"🚀 Reading and splitting the file...\")\n",
        "\n",
        "with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "\n",
        "full_text = f.read()\n",
        "\n",
        "# Split the text\n",
        "\n",
        "chunks = split_text_smart(full_text, CHUNK_SIZE)\n",
        "\n",
        "print(f\"📄 The text has been split into {len(chunks)} chunks. Starting to translate...\")\n",
        "\n",
        "# Clear the contents of the old output file to start writing a new one\n",
        "with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "\n",
        "f.write(\"\")\n",
        "\n",
        "# 2. Translation Loop with Progress Bar\n",
        "\n",
        "# We use tqdm to display the progress bar\n",
        "\n",
        "for i, chunk in enumerate(tqdm(chunks, desc=\"Translating in progress\")):\n",
        "\n",
        "translated_text = translate_segment(chunk)\n",
        "\n",
        "# Write the result immediately to the file (so nothing is lost if the connection is interrupted)\n",
        "\n",
        "with open(OUTPUT_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "\n",
        "f.write(translated_text + \"\\n\\n\")\n",
        "\n",
        "print(f\"\\n✅ Operation completed successfully! Translation saved in: {OUTPUT_FILE}\")\n",
        "\n",
        "# Display Sample\n",
        "\n",
        "print(\"-\" * 30)\n",
        "\n",
        "print(\"👀 Sample from the beginning of the translation:\")\n",
        "\n",
        "with open(OUTPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "\n",
        "print(f.read()[:500] + \"...\")\n",
        "\n",
        "print(\"-\" * 30)"
      ],
      "metadata": {
        "id": "rT1UvS4WpOB1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ollama\n",
        "import os\n",
        "import time\n",
        "from tqdm.notebook import tqdm  # Progress bar for Colab\n",
        "\n",
        "# ------------------- Configuration -------------------\n",
        "INPUT_FILE = \"/content/gpt-summarizer/gpt_summarizer/text.txt\"           # Source file name\n",
        "OUTPUT_FILE = \"translated_text.txt\" # Output file name\n",
        "MODEL_NAME = \"llama3.1:8b\"        # Model to use\n",
        "CHUNK_SIZE = 1500                 # Character limit per chunk\n",
        "# -----------------------------------------------------\n",
        "\n",
        "def split_text_smart(text, max_length):\n",
        "    \"\"\"\n",
        "    Splits text intelligently at periods (.) or spaces to avoid cutting sentences in half.\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "    while len(text) > max_length:\n",
        "        # Find the last period before the max length to keep sentences intact\n",
        "        split_index = text.rfind('.', 0, max_length)\n",
        "\n",
        "        # If no period is found, look for the last space\n",
        "        if split_index == -1:\n",
        "            split_index = text.rfind(' ', 0, max_length)\n",
        "\n",
        "        # If the chunk is one giant string (no spaces), cut at max_length\n",
        "        if split_index == -1:\n",
        "            split_index = max_length\n",
        "\n",
        "        # Append the chunk including the punctuation\n",
        "        chunks.append(text[:split_index + 1])\n",
        "        text = text[split_index + 1:].strip()\n",
        "\n",
        "    if text:\n",
        "        chunks.append(text)\n",
        "    return chunks\n",
        "\n",
        "def translate_segment(text_chunk, retries=3):\n",
        "    \"\"\"\n",
        "    Translates a single text chunk with retry logic in case of failure.\n",
        "    \"\"\"\n",
        "    # The system prompt is in English, but instructs the AI to output Arabic.\n",
        "    system_prompt = (\n",
        "        \"You are an expert technical and academic translator. \"\n",
        "        \"Your task is to translate the following English text into professional, high-quality Arabic. \"\n",
        "        \"Guidelines: \"\n",
        "        \"1. Use accurate technical terms (e.g., translate 'Agents' correctly in an AI context). \"\n",
        "        \"2. Ensure perfect Arabic grammar. \"\n",
        "        \"3. Output ONLY the translation, without any introductory or concluding remarks.\"\n",
        "    )\n",
        "\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = ollama.chat(\n",
        "                model=MODEL_NAME,\n",
        "                messages=[\n",
        "                    {'role': 'system', 'content': system_prompt},\n",
        "                    {'role': 'user', 'content': text_chunk},\n",
        "                ],\n",
        "                options={\n",
        "                    'temperature': 0.1,  # Low temperature for high accuracy/low hallucination\n",
        "                    'top_p': 0.9,\n",
        "                }\n",
        "            )\n",
        "            return response['message']['content']\n",
        "        except Exception as e:\n",
        "            print(f\"\\n[Warning] Attempt {attempt+1} failed. Error: {e}\")\n",
        "            time.sleep(2) # Wait 2 seconds before retrying\n",
        "\n",
        "    return \"[Failed to translate this segment after multiple attempts]\"\n",
        "\n",
        "# ------------------- Main Execution -------------------\n",
        "\n",
        "def main():\n",
        "    # 1. Check if input file exists\n",
        "    if not os.path.exists(INPUT_FILE):\n",
        "        print(f\"[Error] File '{INPUT_FILE}' not found! Please upload it first.\")\n",
        "        return\n",
        "\n",
        "    print(\"--- Starting Process ---\")\n",
        "    print(f\"Reading file: {INPUT_FILE}...\")\n",
        "\n",
        "    with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        full_text = f.read()\n",
        "\n",
        "    # 2. Split text into chunks\n",
        "    chunks = split_text_smart(full_text, CHUNK_SIZE)\n",
        "    print(f\"Document split into {len(chunks)} parts. Starting translation...\\n\")\n",
        "\n",
        "    # Clear previous output file content\n",
        "    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\")\n",
        "\n",
        "    # 3. Process chunks with a progress bar\n",
        "    for chunk in tqdm(chunks, desc=\"Translating\"):\n",
        "\n",
        "        translated_text = translate_segment(chunk)\n",
        "\n",
        "        # Append result immediately to file (safeguard against crashes)\n",
        "        with open(OUTPUT_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(translated_text + \"\\n\\n\")\n",
        "\n",
        "    print(f\"\\n[Success] Translation complete!\")\n",
        "    print(f\"Output saved to: {OUTPUT_FILE}\")\n",
        "\n",
        "    # 4. Show a sample\n",
        "    print(\"-\" * 30)\n",
        "    print(\"Sample of the output:\")\n",
        "    if os.path.exists(OUTPUT_FILE):\n",
        "        with open(OUTPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "            print(f.read()[:500] + \"...\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337,
          "referenced_widgets": [
            "bcab341dbb0b431586f96b10397ae8d1",
            "f2c77961dcb44c59bfb22a87a7d2eea5",
            "f48e8e4c64854ac9847af6dcc21aa3b7",
            "5aa10f6f15b24eeab1d6f710c031657c",
            "5b855eaed54e473ebd45017c1dce9147",
            "54fd3699332f4aa9b4ee81cb57302b91",
            "61bd50953b5c4d2799823068a7762889",
            "c23f7bdcef4e4facaa4e55b2337c74f2",
            "04b41bd28bce4be8abd2f6b1aa908c16",
            "be682891a717456baa18c5502a3f9c7f",
            "2e1e9acb21bd4bb8bb3014a6c12544ca"
          ]
        },
        "id": "5x1sez_Hp-wd",
        "outputId": "7ba9c566-d97e-47c8-d324-eed6c6a426bc"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Process ---\n",
            "Reading file: /content/gpt-summarizer/gpt_summarizer/text.txt...\n",
            "Document split into 1 parts. Starting translation...\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Translating:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "bcab341dbb0b431586f96b10397ae8d1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Success] Translation complete!\n",
            "Output saved to: translated_text.txt\n",
            "------------------------------\n",
            "Sample of the output:\n",
            "التعلم الإلكتروني (AI) هو الذكاء الذي يظهره الآلات، على عكس الذكاء الطبيعي الذي يتم.displayed من قبل الحيوانات بما في ذلك البشر.\n",
            "\n",
            "تم تعريف البحث عن التعلم الإلكتروني على أنه مجال دراسة एजنسيات ذكية، والتي تشير إلى أي نظام يمكنه أن يدرك بيئته ويتخذ إجراءات تزيد فرص تحقيق أهدافه.\n",
            "استخدم المصطلح \"التعلم الإلكتروني\" سابقًا لوصف الآلات التي تماثل وتظهر مهارات المعرفية البشرية، مثل \"تعلم\" و \" حل المشكلات\".\n",
            "تم رفض هذا التعريف منذ ذلك الحين من قبل باحثي AI الكبار الذين يصفون الآن التعلم الإلكتروني بال م...\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--- Starting Process ---\n",
        "Reading file: /content/gpt-summarizer/gpt_summarizer/text.txt...\n",
        "Document split into 1 parts. Starting translation...\n",
        "\n",
        "Translating: 100%\n",
        " 1/1 [04:48<00:00, 288.13s/it]\n",
        "\n",
        "\n",
        "[Success] Translation complete!\n",
        "Output saved to: translated_text.txt\n",
        "## ------------------------------\n",
        "Sample of the output:\n",
        "التعلم الإلكتروني (AI) هو الذكاء الذي يظهره الآلات، على عكس الذكاء الطبيعي الذي يتم.displayed من قبل الحيوانات بما في ذلك البشر.\n",
        "\n",
        "تم تعريف البحث عن التعلم الإلكتروني على أنه مجال دراسة एजنسيات ذكية، والتي تشير إلى أي نظام يمكنه أن يدرك بيئته ويتخذ إجراءات تزيد فرص تحقيق أهدافه.\n",
        "استخدم المصطلح \"التعلم الإلكتروني\" سابقًا لوصف الآلات التي تماثل وتظهر مهارات المعرفية البشرية، مثل \"تعلم\" و \" حل المشكلات\".\n",
        "تم رفض هذا التعريف منذ ذلك الحين من قبل باحثي AI الكبار الذين يصفون الآن التعلم الإلكتروني بال م...\n",
        "# ------------------"
      ],
      "metadata": {
        "id": "zJY1DBOWrg78"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ollama\n",
        "import os\n",
        "import time\n",
        "from tqdm.notebook import tqdm  # Progress bar\n",
        "\n",
        "# ------------------- Configuration -------------------\n",
        "INPUT_FILE = \"text.txt\"\n",
        "OUTPUT_FILE = \"translated_text.txt\"\n",
        "MODEL_NAME = \"llama3.1:8b\"\n",
        "CHUNK_SIZE = 1000                 # Reduced chunk size for better focus\n",
        "# -----------------------------------------------------\n",
        "\n",
        "def split_text_smart(text, max_length):\n",
        "    \"\"\"\n",
        "    Splits text intelligently at periods or newlines.\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "    while len(text) > max_length:\n",
        "        split_index = text.rfind('.', 0, max_length)\n",
        "        if split_index == -1:\n",
        "            split_index = text.rfind('\\n', 0, max_length)\n",
        "        if split_index == -1:\n",
        "            split_index = text.rfind(' ', 0, max_length)\n",
        "        if split_index == -1:\n",
        "            split_index = max_length\n",
        "\n",
        "        chunks.append(text[:split_index + 1])\n",
        "        text = text[split_index + 1:].strip()\n",
        "\n",
        "    if text:\n",
        "        chunks.append(text)\n",
        "    return chunks\n",
        "\n",
        "def translate_segment(text_chunk, retries=3):\n",
        "    \"\"\"\n",
        "    Translates text with strict enforcement of Arabic terminology.\n",
        "    \"\"\"\n",
        "\n",
        "    # STRICT SYSTEM PROMPT\n",
        "    # We explicitly map the terms that were failing (AI -> الذكاء الاصطناعي)\n",
        "    # We explicitly forbid non-Arabic characters.\n",
        "    system_prompt = (\n",
        "        \"You are a professional translator. Translate the text from English to Arabic. \\n\"\n",
        "        \"STRICT RULES:\\n\"\n",
        "        \"1. Terminology: Translate 'Artificial Intelligence' strictly as 'الذكاء الاصطناعي'. NEVER translate it as 'E-learning'.\\n\"\n",
        "        \"2. Terminology: Translate 'Agents' as 'وكلاء' or 'أنظمة'.\\n\"\n",
        "        \"3. Output Language: The output must be 100% Arabic. Do NOT use English words (like 'displayed').\\n\"\n",
        "        \"4. Character Set: Do NOT use Hindi or Urdu characters (like 'ए'). Use only standard Arabic letters.\\n\"\n",
        "        \"5. Grammar: Use formal, correct Arabic grammar.\"\n",
        "    )\n",
        "\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = ollama.chat(\n",
        "                model=MODEL_NAME,\n",
        "                messages=[\n",
        "                    {'role': 'system', 'content': system_prompt},\n",
        "                    {'role': 'user', 'content': text_chunk},\n",
        "                ],\n",
        "                options={\n",
        "                    'temperature': 0.1,  # Keep it deterministic\n",
        "                    'top_p': 0.85,       # Slightly stricter sampling\n",
        "                    'num_predict': 512,  # Ensure enough space for output\n",
        "                }\n",
        "            )\n",
        "            return response['message']['content']\n",
        "        except Exception as e:\n",
        "            print(f\"\\n[Warning] Attempt {attempt+1} failed. Error: {e}\")\n",
        "            time.sleep(2)\n",
        "\n",
        "    return \"[Translation Failed]\"\n",
        "\n",
        "# ------------------- Main Execution -------------------\n",
        "\n",
        "def main():\n",
        "    if not os.path.exists(INPUT_FILE):\n",
        "        print(f\"[Error] File '{INPUT_FILE}' not found.\")\n",
        "        return\n",
        "\n",
        "    print(\"--- Starting Process (Strict Mode) ---\")\n",
        "\n",
        "    with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        full_text = f.read()\n",
        "\n",
        "    chunks = split_text_smart(full_text, CHUNK_SIZE)\n",
        "    print(f\"Document split into {len(chunks)} parts.\")\n",
        "\n",
        "    # Reset output file\n",
        "    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\")\n",
        "\n",
        "    for chunk in tqdm(chunks, desc=\"Translating\"):\n",
        "        translated_text = translate_segment(chunk)\n",
        "\n",
        "        with open(OUTPUT_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(translated_text + \"\\n\\n\")\n",
        "\n",
        "    print(f\"\\n[Success] Translation complete!\")\n",
        "\n",
        "    # Show sample\n",
        "    print(\"-\" * 30)\n",
        "    if os.path.exists(OUTPUT_FILE):\n",
        "        with open(OUTPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "            print(f.read()[:500] + \"...\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "pSVNUMqxqSBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "عودة f\"\"\"SYSTEM: أنت محلل نصوص مفيد يعرف كيفية تلخيص النص إلى صحيح {get_output_format_name()}.\n",
        "المستخدم: تلخيص هذا النص المشار إليه بالخلفيات:"
      ],
      "metadata": {
        "id": "6xjJ8dlZsY8O"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1A96kkMmsghU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### https://github.com/mrseanryan/gpt-summarizer/blob/9dd9ae6bf70de990294a14d28d027821e5e530f1/gpt_summarizer/prompts.py#L5"
      ],
      "metadata": {
        "id": "SS6weBzCssOH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# yaml أرخص لتوليد\n",
        "OUTPUT_FORMAT_YAML = \"\"\"\n",
        "يجب أن يكون تنسيق الإخراج صالحًا YAML ، مع الحقول: title_in_quotes ، short_summary ، long_summary ، الفقرات.\n",
        "- لا تقم بتضمين أحرف YAML الخاصة في النص (على سبيل المثال: اقتباسات مفردة أو قولون)\n",
        "- لا تستخدم مشغل استمرار الخط '|'\n",
        "- للرصاص استخدام الواصلة '-'، لا تستخدم '*'\n",
        "- دلالة على الناتج الإجمالي مع \"\" لا \"--\"\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "plzo5jbashQe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OUTPUT_FORMAT_YAML_SHORTER = \"\"\"\n",
        "The output format must be valid YAML, with the fields: title_in_quotes, short_summary, long_summary.\n",
        "- do NOT include YAML special characters in the text (for example: single quotes or colons)\n",
        "- do NOT use the line-continuation operator '|'\n",
        "- for bullets use hyphen '-', do NOT use '*'\n",
        "- denote the overall output with ``` NOT '---'\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "SX0z_DcLsqAl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "OUTPUT_TEXT_STYLE = \"The output text preserve the original style and tone. The summary MUST summarize the contents of the text, this is NOT a commentary.\"\n",
        "\n",
        "SYSTEM_PROMPT__OPENAI = f\"You are a summary assistant, skilled in summarizing texts whilst preserving the main points and original style. The target language is {config.TARGET_LANGUAGE}.\"\n"
      ],
      "metadata": {
        "id": "R7Y3nn5dsw5m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "OUTPUT_TEXT_STYLE = \"نص الإخراج يحافظ على النمط الأصلي والنغمة. يجب أن يلخص الملخص محتويات النص، هذا ليس تعليقا\".\n",
        "\n",
        "SYSTEM_PROMPT__OPENAI = f\"You مساعد ملخص، ماهر في تلخيص النصوص مع الحفاظ على النقاط الرئيسية والأسلوب الأصلي. اللغة المستهدفة هي {config.TARGET_LANGUAGE}."
      ],
      "metadata": {
        "id": "ldl_rnPWs36k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "        PARAGRAPHS = \"\\n    - paragraphs should be an array of one sentence summaries: one sentence for each paragraph.\\n\"\n",
        "\n",
        "    return f\"\"\""
      ],
      "metadata": {
        "id": "Ug0J-IFdtAqo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "    return f\"\"\"\n",
        "1. Analyze the given input text.\n",
        "    - The input text is delimited by triple backticks.\n",
        "2. {_get_output_format(include_paragraphs=include_paragraphs)}\n",
        "3. Create a title and a short and long summary in the target language {target_language}.\n",
        "    - {OUTPUT_TEXT_STYLE}\n",
        "    - short_summary should be {config.SHORT_SUMMARY_WORD_COUNT} words long.\n",
        "    - long_summary should be {config.LONG_SUMMARY_WORD_COUNT} words long.{PARAGRAPHS}\n",
        "4. After generating the summaries, stop and check that the output is valid {get_output_format_name()}.\n"
      ],
      "metadata": {
        "id": "cinWBdkftJdN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "العودة f \"\"\"\n",
        "1. تحليل نص الإدخال المعطى.\n",
        "    - يتم تعيين نص الإدخال بواسطة خلفيات ثلاثية.\n",
        "2. {_get_output_format(include_paragraphs=include_paragraphs)}\n",
        "3. إنشاء عنوان وملخص قصير وطويل في اللغة الهدف {target_language}.\n",
        "    - {OUTPUT_TEXT_STYLE}\n",
        "    - يجب أن يكون {config.SHORT_SUMMARY_WORD_COUNT} كلمات طويلة.\n",
        "    - يجب أن يكون {config.LONG_SUMMARY_WORD_COUNT} كلمات طويلة.{ الفقرات}\n",
        "4. بعد إنشاء الملخصات ، توقف وتحقق من أن الإخراج صالح {get_output_format_name()}."
      ],
      "metadata": {
        "id": "P47TydqJtNRk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H8iza4-Isx-t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "        PARAGRAPHS = \"\\n        - paragraphs should be an array of one sentence shortened-versions: one sentence for each paragraph.\\n\"\n",
        "\n",
        "    print(\"[complex prompt]\")\n",
        "    return f\"\"\"Examine the provided user prompt, the text input, noting its style of writing and tone.\n",
        "\n",
        "RULES:\n",
        "R1. Do NOT mention the text, study, document or paper.\n",
        "R2. Create shorted versions of the original text, in the same style.\n",
        "R3. Preserve the 'person' or 'narrator' of the text: for example, if the text is written in first-person, then also output in first-person.\n",
        "\n",
        "PROCESS TO FOLLOW:\n",
        "    1. Analyze the given input text, noting its style and tone.\n",
        "        - The input text is delimited by triple backticks.\n",
        "    2. {_get_output_format(include_paragraphs=include_paragraphs)}\n",
        "    3. Create a title and a short and long version in the target language {target_language}.\n",
        "        - {OUTPUT_TEXT_STYLE}\n",
        "        - short_summary should be {config.SHORT_SUMMARY_WORD_COUNT} words long, in the original style.\n",
        "        - long_summary should be {config.LONG_SUMMARY_WORD_COUNT} words long, in the original style.{PARAGRAPHS}\n",
        "        - shortened texts should be as if excerpts of the original text. example: 'While recent language models have the ability to take long contexts as input, relatively little is known about how well the language models\n",
        "use longer context.' -> 'Whilst recent language models can accept long contexts, little is know about the quality of output'.\n",
        "    4. Do NOT output a commentary - do NOT use phrases such as 'The paper examines...', 'It is noted...' or 'This text'.\n",
        "    5. After generating the shortened texts, stop and check that the output is valid {get_output_format_name()}.\n",
        "\n",
        "<thinking>\n",
        "For each generated shortened text:\n",
        "- check does the generated text follow the RULES.\n",
        "- check is the writing style and tone same as original.\n",
        "- check is the shortened text as if part of the original text NOT a commentary.\n",
        "- check the shortened text is direct, as a primary source, and not a commentary.\n",
        "- do NOT mention the text, study, document or paper.\n",
        "- do NOT output a commentary - do NOT use phrases such as 'The paper examines...', 'It is noted...' or 'The text discusses...' or 'This text lists...' or 'The text...' or 'The study...'.\n",
        "- avoid duplication\n",
        "[Continue for all items]\n",
        "</thinking>\n",
        "\n",
        "text: ```{input_text}```\n"
      ],
      "metadata": {
        "id": "iryg-5ZQtT8O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ollama\n",
        "import os\n",
        "import time\n",
        "from tqdm.notebook import tqdm  # Progress bar\n",
        "\n",
        "# ------------------- Configuration -------------------\n",
        "INPUT_FILE = \"/content/gpt-summarizer/gpt_summarizer/text.txt\"\n",
        "OUTPUT_FILE = \"translated_text.txt\"\n",
        "MODEL_NAME = \"llama3.1:8b\"\n",
        "CHUNK_SIZE = 1000                 # Reduced chunk size for better focus\n",
        "# -----------------------------------------------------\n",
        "\n",
        "def split_text_smart(text, max_length):\n",
        "    \"\"\"\n",
        "    Splits text intelligently at periods or newlines.\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "    while len(text) > max_length:\n",
        "        split_index = text.rfind('.', 0, max_length)\n",
        "        if split_index == -1:\n",
        "            split_index = text.rfind('\\n', 0, max_length)\n",
        "        if split_index == -1:\n",
        "            split_index = text.rfind(' ', 0, max_length)\n",
        "        if split_index == -1:\n",
        "            split_index = max_length\n",
        "\n",
        "        chunks.append(text[:split_index + 1])\n",
        "        text = text[split_index + 1:].strip()\n",
        "\n",
        "    if text:\n",
        "        chunks.append(text)\n",
        "    return chunks\n",
        "\n",
        "def translate_segment(text_chunk, retries=3):\n",
        "    \"\"\"\n",
        "    Translates text with strict enforcement of Arabic terminology.\n",
        "    \"\"\"\n",
        "\n",
        "    # STRICT SYSTEM PROMPT\n",
        "    # We explicitly map the terms that were failing (AI -> الذكاء الاصطناعي)\n",
        "    # We explicitly forbid non-Arabic characters.\n",
        "    system_prompt = (\n",
        "        \"You are a professional translator. Translate the text from English to Arabic. \\n\"\n",
        "        \"STRICT RULES:\\n\"\n",
        "        \"1. Terminology: Translate 'Artificial Intelligence' strictly as 'الذكاء الاصطناعي'. NEVER translate it as 'E-learning'.\\n\"\n",
        "        \"2. Terminology: Translate 'Agents' as 'وكلاء' or 'أنظمة'.\\n\"\n",
        "        \"3. Output Language: The output must be 100% Arabic. Do NOT use English words (like 'displayed').\\n\"\n",
        "        \"4. Character Set: Do NOT use Hindi or Urdu characters (like 'ए'). Use only standard Arabic letters.\\n\"\n",
        "        \"5. Grammar: Use formal, correct Arabic grammar.\"\n",
        "    )\n",
        "\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = ollama.chat(\n",
        "                model=MODEL_NAME,\n",
        "                messages=[\n",
        "                    {'role': 'system', 'content': system_prompt},\n",
        "                    {'role': 'user', 'content': text_chunk},\n",
        "                ],\n",
        "                options={\n",
        "                    'temperature': 0.1,  # Keep it deterministic\n",
        "                    'top_p': 0.85,       # Slightly stricter sampling\n",
        "                    'num_predict': 512,  # Ensure enough space for output\n",
        "                }\n",
        "            )\n",
        "            return response['message']['content']\n",
        "        except Exception as e:\n",
        "            print(f\"\\n[Warning] Attempt {attempt+1} failed. Error: {e}\")\n",
        "            time.sleep(2)\n",
        "\n",
        "    return \"[Translation Failed]\"\n",
        "\n",
        "# ------------------- Main Execution -------------------\n",
        "\n",
        "def main():\n",
        "    if not os.path.exists(INPUT_FILE):\n",
        "        print(f\"[Error] File '{INPUT_FILE}' not found.\")\n",
        "        return\n",
        "\n",
        "    print(\"--- Starting Process (Strict Mode) ---\")\n",
        "\n",
        "    with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        full_text = f.read()\n",
        "\n",
        "    chunks = split_text_smart(full_text, CHUNK_SIZE)\n",
        "    print(f\"Document split into {len(chunks)} parts.\")\n",
        "\n",
        "    # Reset output file\n",
        "    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\")\n",
        "\n",
        "    for chunk in tqdm(chunks, desc=\"Translating\"):\n",
        "        translated_text = translate_segment(chunk)\n",
        "\n",
        "        with open(OUTPUT_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(translated_text + \"\\n\\n\")\n",
        "\n",
        "    print(f\"\\n[Success] Translation complete!\")\n",
        "\n",
        "    # Show sample\n",
        "    print(\"-\" * 30)\n",
        "    if os.path.exists(OUTPUT_FILE):\n",
        "        with open(OUTPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "            print(f.read()[:500] + \"...\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266,
          "referenced_widgets": [
            "3826b35d8b8d4d1cb9c0386ba9c4adf1",
            "239428a71b3f452d9f5cc08984666019",
            "21c5cd1aba90477495e41f3f323c3e22",
            "6ab9f91ddbfe4b0792bfe8c1da280927",
            "eb3699e2966d4f9ca704c68ee1131d6f",
            "0953cdc9c9b442529bcf3b8e33cc5f81",
            "77122b9b36014b4e90dc5672caf8a865",
            "06916acf24f94e5e9002bfa35b3dc475",
            "0d9812c513c94cab84941d18f625e0fc",
            "add38c9df6394be29d849f0b3750b92d",
            "66b611a5804841a983a6cc75c989db04"
          ]
        },
        "id": "70JT3Q0ztUpY",
        "outputId": "af9a34d3-e58f-4f91-ffec-32afb5ce1612"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Process (Strict Mode) ---\n",
            "Document split into 1 parts.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Translating:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3826b35d8b8d4d1cb9c0386ba9c4adf1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Success] Translation complete!\n",
            "------------------------------\n",
            "الذكاء الاصطناعي هو الذكاء الذي يظهره الآلات، على عكس الذكاء الطبيعي الذي يتم.displaying بواسطة الحيوانات بما في ذلك البشر.\n",
            "\n",
            "وقد تم تعريف البحث عن الذكاء الاصطناعي كمجال دراسة الوكلاء الذكيين، والذي يشير إلى أي نظام يمكنه أن يدرك بيئته ويتخذ إجراءات تزيد من فرص تحقيق أهدافه.\n",
            "تم استخدام مصطلح \"الذكاء الاصطناعي\" سابقًا لوصف الآلات التي تمثل وتظهر مهارات المعرفية البشرية، مثل \"التعلم\" وال \" حل المشكلات\".\n",
            "وقد تم رفض هذا التعريف منذ ذلك الحين من قبل باحثي الذكاء الاصطناعي الأبرز الذين يصفون الآن الذك...\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--- Starting Process (Strict Mode) ---\n",
        "Document split into 1 parts.\n",
        "\n",
        "Translating: 100%\n",
        " 1/1 [05:26<00:00, 326.45s/it]\n",
        "\n",
        "\n",
        "[Success] Translation complete!\n",
        "------------------------------\n",
        "الذكاء الاصطناعي هو الذكاء الذي يظهره الآلات، على عكس الذكاء الطبيعي الذي يتم.displaying بواسطة الحيوانات بما في ذلك البشر.\n",
        "\n",
        "وقد تم تعريف البحث عن الذكاء الاصطناعي كمجال دراسة الوكلاء الذكيين، والذي يشير إلى أي نظام يمكنه أن يدرك بيئته ويتخذ إجراءات تزيد من فرص تحقيق أهدافه.\n",
        "تم استخدام مصطلح \"الذكاء الاصطناعي\" سابقًا لوصف الآلات التي تمثل وتظهر مهارات المعرفية البشرية، مثل \"التعلم\" وال \" حل المشكلات\".\n",
        "وقد تم رفض هذا التعريف منذ ذلك الحين من قبل باحثي الذكاء الاصطناعي الأبرز الذين يصفون الآن الذك...\n",
        "------------------------------\n",
        "### New Section"
      ],
      "metadata": {
        "id": "S-r-eSKJw49B"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AnOEPM30uCDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9NRZYdDGw4Yn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ollama\n",
        "import os\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# ------------------- Configuration -------------------\n",
        "INPUT_FILE = \"/content/gpt-summarizer/gpt_summarizer/text.txt\"\n",
        "OUTPUT_FILE = \"output_arabic.txt\"\n",
        "MODEL_NAME = \"llama3.1:8b\"\n",
        "CHUNK_SIZE = 1200  # Good balance for Llama 3 context\n",
        "\n",
        "# ------------------- PROMPT ENGINEERING -------------------\n",
        "# Inspired by: mrseanryan/gpt-summarizer/prompts.py\n",
        "# We define \"Personas\" to guide the model strictly.\n",
        "\n",
        "PROMPT_TEMPLATES = {\n",
        "    \"tech_translation\": (\n",
        "        \"ROLE: You are an expert Technical Writer and Linguist.\\n\"\n",
        "        \"TASK: Translate the following English text into professional Arabic.\\n\"\n",
        "        \"GUIDELINES:\\n\"\n",
        "        \"1. Accuracy: Preserve technical meaning. Do not dumb it down.\\n\"\n",
        "        \"2. Terminology: Use standard Arabic technical terms (e.g., 'Artificial Intelligence' -> 'الذكاء الاصطناعي').\\n\"\n",
        "        \"3. Tone: Formal, objective, and concise. No marketing fluff.\\n\"\n",
        "        \"4. STRICT OUTPUT: Output ONLY the Arabic translation. No English words. No Hindi characters.\\n\"\n",
        "        \"5. Formatting: Maintain the original paragraph structure.\"\n",
        "    ),\n",
        "    \"tech_summary\": (\n",
        "        \"ROLE: You are an expert Technical Writer.\\n\"\n",
        "        \"TASK: Summarize the key points of the following text in Arabic.\\n\"\n",
        "        \"GUIDELINES:\\n\"\n",
        "        \"1. Format: Use bullet points (-).\\n\"\n",
        "        \"2. Focus: Extract the most important technical facts and arguments.\\n\"\n",
        "        \"3. Brevity: Be concise but comprehensive.\\n\"\n",
        "        \"4. Language: The output must be 100% in Arabic.\"\n",
        "    )\n",
        "}\n",
        "\n",
        "# Select which prompt to use here:\n",
        "SELECTED_PROMPT = PROMPT_TEMPLATES[\"tech_translation\"]\n",
        "# Change to PROMPT_TEMPLATES[\"tech_summary\"] if you want a summary instead.\n",
        "\n",
        "# --------------------------------------------------------\n",
        "\n",
        "def split_text_smart(text, max_length):\n",
        "    \"\"\"Splits text without breaking sentences.\"\"\"\n",
        "    chunks = []\n",
        "    while len(text) > max_length:\n",
        "        # Try splitting at paragraph end\n",
        "        split_index = text.rfind('\\n', 0, max_length)\n",
        "        # If no paragraph, try sentence end\n",
        "        if split_index == -1:\n",
        "            split_index = text.rfind('.', 0, max_length)\n",
        "        # If no sentence, try space\n",
        "        if split_index == -1:\n",
        "            split_index = text.rfind(' ', 0, max_length)\n",
        "        # Last resort: hard cut\n",
        "        if split_index == -1:\n",
        "            split_index = max_length\n",
        "\n",
        "        chunks.append(text[:split_index + 1])\n",
        "        text = text[split_index + 1:].strip()\n",
        "\n",
        "    if text:\n",
        "        chunks.append(text)\n",
        "    return chunks\n",
        "\n",
        "def process_segment(text_chunk, retries=3):\n",
        "    \"\"\"Sends the chunk to Ollama with the selected engineering prompt.\"\"\"\n",
        "\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = ollama.chat(\n",
        "                model=MODEL_NAME,\n",
        "                messages=[\n",
        "                    {'role': 'system', 'content': SELECTED_PROMPT},\n",
        "                    {'role': 'user', 'content': text_chunk},\n",
        "                ],\n",
        "                options={\n",
        "                    'temperature': 0.1, # Low temp = High precision (Technical Writer style)\n",
        "                    'top_p': 0.9,\n",
        "                }\n",
        "            )\n",
        "            return response['message']['content']\n",
        "        except Exception as e:\n",
        "            print(f\"\\n[Warning] Attempt {attempt+1} failed. Error: {e}\")\n",
        "            time.sleep(2)\n",
        "\n",
        "    return \"[Error: Segment failed to process]\"\n",
        "\n",
        "def main():\n",
        "    if not os.path.exists(INPUT_FILE):\n",
        "        print(f\"[Error] File '{INPUT_FILE}' not found.\")\n",
        "        return\n",
        "\n",
        "    print(\"--- Starting Process with 'Expert Technical Writer' Persona ---\")\n",
        "\n",
        "    with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        full_text = f.read()\n",
        "\n",
        "    chunks = split_text_smart(full_text, CHUNK_SIZE)\n",
        "    print(f\"Document split into {len(chunks)} parts.\")\n",
        "\n",
        "    # Clear output file\n",
        "    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\")\n",
        "\n",
        "    # Process chunks\n",
        "    for chunk in tqdm(chunks, desc=\"Processing\"):\n",
        "        result = process_segment(chunk)\n",
        "\n",
        "        with open(OUTPUT_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(result + \"\\n\\n\")\n",
        "\n",
        "    print(f\"\\n[Success] Task complete! Saved to: {OUTPUT_FILE}\")\n",
        "\n",
        "    # Preview\n",
        "    print(\"-\" * 30)\n",
        "    print(\"Output Preview:\")\n",
        "    if os.path.exists(OUTPUT_FILE):\n",
        "        with open(OUTPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "            print(f.read()[:500] + \"...\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337,
          "referenced_widgets": [
            "4e07d1f17afa453ca2edde8ac3afd437",
            "76916399dc254ae1867b499137039ee7",
            "3ffbd86ef40049e8887176ca4d1daae0",
            "96049fa987f5441c84760e094d7a7698",
            "2e10bd670c1340f491d2b4dceb4423a6",
            "a4de9a95f8a649f5b0ea5e92e4838acc",
            "d84d52d247b64ea08cbfec8d59bd912d",
            "b05afa04cdf542fd810a0e5716904429",
            "5d5cad7b6a2844b09486b819bc6c915d",
            "64da4cd22e3647dc84df0ee3f2b5f5fe",
            "aa113e316ee948abac24b65155cdc0fe"
          ]
        },
        "id": "B9mEhIqOxPGg",
        "outputId": "e4c38ec1-4eda-4758-93ac-4f56464dfa61"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting Process with 'Expert Technical Writer' Persona ---\n",
            "Document split into 1 parts.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4e07d1f17afa453ca2edde8ac3afd437"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Success] Task complete! Saved to: output_arabic.txt\n",
            "------------------------------\n",
            "Output Preview:\n",
            "يُعرَف الذكاء الاصطناعي (AI) بأنه الحكمة التي تظهرها الآلات، على عكس الحكمة الطبيعية التي تتميز بها الحيوانات بما في ذلك البشر.\n",
            "\n",
            "وقد تم تعريف البحث عن الذكاء الاصطناعي بأنها مجال دراسة एजنسيات ذكية، والتي تشير إلى أي نظام يدرك بيئته ويتخذ إجراءات تزيد من فرص تحقيق أهدافه.\n",
            "\n",
            "ومع ذلك، استُخدم المصطلح \"ذكاء اصطناعي\" سابقًا لوصف الآلات التي تقلد وتظهر مهارات认مية بشرية مرتبطة بالدماغ البشري، مثل \"التعلم\" و \" حل المشكلات\".\n",
            "\n",
            "وقد تم رفض هذا التعريف منذ ذلك الحين من قبل باحثي الذكاء الاصطناعي البارزين الذ...\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--- Starting Process with 'Expert Technical Writer' Persona ---\n",
        "Document split into 1 parts.\n",
        "\n",
        "Processing: 100%\n",
        " 1/1 [05:35<00:00, 335.66s/it]\n",
        "\n",
        "\n",
        "[Success] Task complete! Saved to: output_arabic.txt\n",
        "------------------------------\n",
        "Output Preview:\n",
        "يُعرَف الذكاء الاصطناعي (AI) بأنه الحكمة التي تظهرها الآلات، على عكس الحكمة الطبيعية التي تتميز بها الحيوانات بما في ذلك البشر.\n",
        "\n",
        "وقد تم تعريف البحث عن الذكاء الاصطناعي بأنها مجال دراسة एजنسيات ذكية، والتي تشير إلى أي نظام يدرك بيئته ويتخذ إجراءات تزيد من فرص تحقيق أهدافه.\n",
        "\n",
        "ومع ذلك، استُخدم المصطلح \"ذكاء اصطناعي\" سابقًا لوصف الآلات التي تقلد وتظهر مهارات认مية بشرية مرتبطة بالدماغ البشري، مثل \"التعلم\" و \" حل المشكلات\".\n",
        "\n",
        "وقد تم رفض هذا التعريف منذ ذلك الحين من قبل باحثي الذكاء الاصطناعي البارزين الذ...\n",
        "------------------------------\n",
        "### New Section"
      ],
      "metadata": {
        "id": "4IVIJ2p-zBbV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://github.com/mrseanryan/gpt-summarizer/blob/9dd9ae6bf70de990294a14d28d027821e5e530f1/gpt_summarizer/prompts.py#L5"
      ],
      "metadata": {
        "id": "ZqQdpI_jxndt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ollama\n",
        "import os\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# ------------------- Configuration -------------------\n",
        "INPUT_FILE = \"text.txt\"\n",
        "OUTPUT_FILE = \"output_arabic_v2.txt\"\n",
        "MODEL_NAME = \"llama3.1:8b\"\n",
        "CHUNK_SIZE = 1000  # Smaller chunks reduce hallucination risks\n",
        "\n",
        "# ------------------- ENGINEERED PROMPT (Anti-Glitch) -------------------\n",
        "# Updated based on your request + constraints to fix Hindi/Chinese glitches\n",
        "\n",
        "PROMPT_TEMPLATES = {\n",
        "    \"strict_tech_translation\": (\n",
        "        \"ROLE: You are an expert Computer Science Translator specialized in AI.\\n\"\n",
        "        \"TASK: Translate the text to Modern Standard Arabic (Fusha).\\n\\n\"\n",
        "        \"CRITICAL RULES (MUST FOLLOW):\\n\"\n",
        "        \"1. NO GLITCHES: Do NOT use Hindi characters (e.g., 'ए'), Chinese (e.g., '认'), or mixed scripts. Use ONLY Arabic letters.\\n\"\n",
        "        \"2. TERMINOLOGY MAPPING:\\n\"\n",
        "        \"   - 'Artificial Intelligence' -> 'الذكاء الاصطناعي' (NOT 'حكمة' or 'تعلم إلكتروني').\\n\"\n",
        "        \"   - 'Intelligence' -> 'ذكاء'.\\n\"\n",
        "        \"   - 'Agents' -> 'وكلاء' or 'كيانات' (NOT 'एजنسيات').\\n\"\n",
        "        \"   - 'Cognitive' -> 'إدراكية' or 'معرفية'.\\n\"\n",
        "        \"3. STYLE: Academic, concise, and professional.\\n\"\n",
        "        \"4. OUTPUT: Provide the Arabic translation ONLY. No intro, no notes.\"\n",
        "    )\n",
        "}\n",
        "\n",
        "SELECTED_PROMPT = PROMPT_TEMPLATES[\"strict_tech_translation\"]\n",
        "\n",
        "# --------------------------------------------------------\n",
        "\n",
        "def split_text_smart(text, max_length):\n",
        "    chunks = []\n",
        "    while len(text) > max_length:\n",
        "        split_index = text.rfind('\\n', 0, max_length)\n",
        "        if split_index == -1: split_index = text.rfind('.', 0, max_length)\n",
        "        if split_index == -1: split_index = text.rfind(' ', 0, max_length)\n",
        "        if split_index == -1: split_index = max_length\n",
        "\n",
        "        chunks.append(text[:split_index + 1])\n",
        "        text = text[split_index + 1:].strip()\n",
        "\n",
        "    if text:\n",
        "        chunks.append(text)\n",
        "    return chunks\n",
        "\n",
        "def process_segment(text_chunk, retries=3):\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = ollama.chat(\n",
        "                model=MODEL_NAME,\n",
        "                messages=[\n",
        "                    {'role': 'system', 'content': SELECTED_PROMPT},\n",
        "                    {'role': 'user', 'content': text_chunk},\n",
        "                ],\n",
        "                options={\n",
        "                    'temperature': 0.3,  # Increased slightly to avoid \"Hindi token trap\"\n",
        "                    'top_p': 0.9,\n",
        "                    'repeat_penalty': 1.1, # Penalize repetition/glitches\n",
        "                    'num_predict': 2048,   # Allow enough space for output\n",
        "                }\n",
        "            )\n",
        "\n",
        "            content = response['message']['content']\n",
        "\n",
        "            # Simple check: If output contains Hindi/Chinese chars, retry\n",
        "            # (Very basic filter logic)\n",
        "            if any(\"\\u0900\" <= c <= \"\\u097F\" for c in content): # Hindi block\n",
        "                raise ValueError(\"Detected Hindi characters in output!\")\n",
        "\n",
        "            return content\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n[Warning] Attempt {attempt+1} failed/rejected. Reason: {e}\")\n",
        "            time.sleep(1)\n",
        "\n",
        "    return \"[Error: Translation Failed]\"\n",
        "\n",
        "def main():\n",
        "    if not os.path.exists(INPUT_FILE):\n",
        "        print(f\"[Error] File '{INPUT_FILE}' not found.\")\n",
        "        return\n",
        "\n",
        "    print(\"--- Starting Process (Strict Anti-Glitch Mode) ---\")\n",
        "\n",
        "    with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        full_text = f.read()\n",
        "\n",
        "    chunks = split_text_smart(full_text, CHUNK_SIZE)\n",
        "    print(f\"Document split into {len(chunks)} parts.\")\n",
        "\n",
        "    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\")\n",
        "\n",
        "    for chunk in tqdm(chunks, desc=\"Translating\"):\n",
        "        result = process_segment(chunk)\n",
        "        with open(OUTPUT_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(result + \"\\n\\n\")\n",
        "\n",
        "    print(f\"\\n[Success] Task complete! Saved to: {OUTPUT_FILE}\")\n",
        "\n",
        "    print(\"-\" * 30)\n",
        "    if os.path.exists(OUTPUT_FILE):\n",
        "        with open(OUTPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "            print(f.read()[:600] + \"...\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "TXSoP4IAxoae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r4ZdV9ZH1ZwZ",
        "outputId": "b8ec52a2-ce00-4787-bc5c-4ae8f1af956b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pdfplumber\n",
            "  Downloading pdfplumber-0.11.8-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pdfminer.six==20251107 (from pdfplumber)\n",
            "  Downloading pdfminer_six-20251107-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.12/dist-packages (from pdfplumber) (11.3.0)\n",
            "Collecting pypdfium2>=4.18.0 (from pdfplumber)\n",
            "  Downloading pypdfium2-5.2.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (67 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.8/67.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20251107->pdfplumber) (3.4.4)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.12/dist-packages (from pdfminer.six==20251107->pdfplumber) (43.0.3)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.0.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20251107->pdfplumber) (2.23)\n",
            "Downloading pdfplumber-0.11.8-py3-none-any.whl (60 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.0/60.0 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pdfminer_six-20251107-py3-none-any.whl (5.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m46.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdfium2-5.2.0-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m61.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdfium2, pdfminer.six, pdfplumber\n",
            "Successfully installed pdfminer.six-20251107 pdfplumber-0.11.8 pypdfium2-5.2.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pdfplumber\n",
        "\n",
        "def extract_text_from_pdf(pdf_path, output_txt_path):\n",
        "    \"\"\"\n",
        "    تستخرج النص من ملف PDF وتحفظه في ملف نصي بنفس التنسيق التقريبي (فقرات وسطور).\n",
        "\n",
        "    :param pdf_path: مسار ملف PDF الأصلي\n",
        "    :param output_txt_path: مسار الملف النصي المطلوب حفظه\n",
        "    \"\"\"\n",
        "    extracted_text = []\n",
        "\n",
        "    try:\n",
        "        with pdfplumber.open(pdf_path) as pdf:\n",
        "            for page_num, page in enumerate(pdf.pages, start=1):\n",
        "                # استخراج النص مع الحفاظ على السطور\n",
        "                text = page.extract_text()\n",
        "                if text:\n",
        "                    extracted_text.append(text)\n",
        "                    # إضافة فاصل بين الصفحات (اختياري)\n",
        "                    extracted_text.append(\"\\n\" + \"=\"*50 + f\" صفحة {page_num} \" + \"=\"*50 + \"\\n\")\n",
        "                else:\n",
        "                    print(f\"لم يتم العثور على نص في الصفحة {page_num}\")\n",
        "\n",
        "        # كتابة النص المستخرج في ملف نصي\n",
        "        with open(output_txt_path, 'w', encoding='utf-8') as f:\n",
        "            f.write(\"\\n\".join(extracted_text))\n",
        "\n",
        "        print(f\"✅ تم حفظ النص المستخرج في: {output_txt_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ حدث خطأ أثناء معالجة الملف: {e}\")\n",
        "\n",
        "# === الاستخدام ===\n",
        "if __name__ == \"__main__\":\n",
        "    # غيّر هذه المسارات حسب ملفك\n",
        "    pdf_file = \"/content/Dracula (Novel)_1-5.pdf\"          # اسم ملف PDF (يجب أن يكون في نفس المجلد أو أعط مسارًا كاملاً)\n",
        "    output_file = \"النص_المستخرج.txt\"  # اسم الملف النصي الناتج\n",
        "\n",
        "    extract_text_from_pdf(pdf_file, output_file)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WILUxCXl1WoJ",
        "outputId": "06a2acea-6313-414f-cf32-1c454dbfb1c6"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ تم حفظ النص المستخرج في: النص_المستخرج.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ollama\n",
        "import os\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# ------------------- Configuration -------------------\n",
        "INPUT_FILE = \"/content/1.txt\"\n",
        "OUTPUT_FILE = \"translated_general.txt\"\n",
        "MODEL_NAME = \"llama3.1:8b\"\n",
        "# Chunk size: 1000-1200 is safe for maintaining context without memory issues\n",
        "CHUNK_SIZE = 1200\n",
        "\n",
        "# ------------------- GENERAL SYSTEM PROMPT -------------------\n",
        "# This prompt is designed to work with ANY text type (technical, story, news, etc.)\n",
        "# It focuses on language purity and style, not specific words.\n",
        "\n",
        "SYSTEM_PROMPT = (\n",
        "    \"ROLE: You are an expert linguist and professional translator.\\n\"\n",
        "    \"TASK: Translate the provided text into fluent, high-quality Modern Standard Arabic (Fusha).\\n\"\n",
        "    \"GUIDELINES:\\n\"\n",
        "    \"1. Context Awareness: Analyze the text to understand the context. If it is technical, use technical Arabic terms. If it is narrative, use descriptive Arabic.\\n\"\n",
        "    \"2. Language Purity: The output must be strictly in Arabic script. Do NOT use Hindi, Urdu, or Chinese characters under any circumstances.\\n\"\n",
        "    \"3. Grammar: Ensure correct grammar and sentence structure.\\n\"\n",
        "    \"4. formatting: Maintain the original paragraph structure.\\n\"\n",
        "    \"5. OUTPUT: Provide ONLY the Arabic translation. Do not add notes like 'Here is the translation'.\"\n",
        ")\n",
        "\n",
        "# -------------------------------------------------------------\n",
        "\n",
        "def split_text_smart(text, max_length):\n",
        "    \"\"\"\n",
        "    Splits text efficiently without breaking sentences.\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "    while len(text) > max_length:\n",
        "        # Priority 1: Split at a newline (paragraph end)\n",
        "        split_index = text.rfind('\\n', 0, max_length)\n",
        "\n",
        "        # Priority 2: Split at a period (sentence end)\n",
        "        if split_index == -1:\n",
        "            split_index = text.rfind('.', 0, max_length)\n",
        "\n",
        "        # Priority 3: Split at a space (word end)\n",
        "        if split_index == -1:\n",
        "            split_index = text.rfind(' ', 0, max_length)\n",
        "\n",
        "        # Fallback: Hard split if no separators found\n",
        "        if split_index == -1:\n",
        "            split_index = max_length\n",
        "\n",
        "        chunks.append(text[:split_index + 1])\n",
        "        text = text[split_index + 1:].strip()\n",
        "\n",
        "    if text:\n",
        "        chunks.append(text)\n",
        "    return chunks\n",
        "\n",
        "def translate_segment(text_chunk, retries=3):\n",
        "    \"\"\"\n",
        "    Translates a chunk with retry logic to handle potential model hiccups.\n",
        "    \"\"\"\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = ollama.chat(\n",
        "                model=MODEL_NAME,\n",
        "                messages=[\n",
        "                    {'role': 'system', 'content': SYSTEM_PROMPT},\n",
        "                    {'role': 'user', 'content': text_chunk},\n",
        "                ],\n",
        "                options={\n",
        "                    # Temperature 0.3 allows the model enough flexibility to pick\n",
        "                    # the right Arabic words without getting stuck in a loop (which causes glitches).\n",
        "                    'temperature': 0.3,\n",
        "                    'top_p': 0.9,\n",
        "                    'num_predict': 2048,\n",
        "                }\n",
        "            )\n",
        "\n",
        "            content = response['message']['content']\n",
        "            return content\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n[Warning] Attempt {attempt+1} failed. Error: {e}\")\n",
        "            time.sleep(1)\n",
        "\n",
        "    return \"[Error: Segment translation failed]\"\n",
        "\n",
        "def main():\n",
        "    # 1. Validation\n",
        "    if not os.path.exists(INPUT_FILE):\n",
        "        print(f\"[Error] File '{INPUT_FILE}' not found. Please upload it.\")\n",
        "        return\n",
        "\n",
        "    print(\"--- Starting General Translation ---\")\n",
        "\n",
        "    # 2. Read File\n",
        "    with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        full_text = f.read()\n",
        "\n",
        "    # 3. Split\n",
        "    chunks = split_text_smart(full_text, CHUNK_SIZE)\n",
        "    print(f\"Document split into {len(chunks)} parts.\")\n",
        "\n",
        "    # 4. Prepare Output File\n",
        "    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\")\n",
        "\n",
        "    # 5. Process\n",
        "    for chunk in tqdm(chunks, desc=\"Translating\"):\n",
        "        result = translate_segment(chunk)\n",
        "\n",
        "        # Append immediately to file\n",
        "        with open(OUTPUT_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(result + \"\\n\\n\")\n",
        "\n",
        "    print(f\"\\n[Success] Process complete! File saved as: {OUTPUT_FILE}\")\n",
        "\n",
        "    # 6. Preview\n",
        "    print(\"-\" * 30)\n",
        "    print(\"Preview of the result:\")\n",
        "    if os.path.exists(OUTPUT_FILE):\n",
        "        with open(OUTPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "            print(f.read()[:600] + \"...\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 337,
          "referenced_widgets": [
            "f9431f3833d64504828fd6054d20c4a6",
            "835b1f3bd2194361b4c2da7afc99859f",
            "9928dd8889c04edc9b28afed7f14537d",
            "20ae3146e3f14b2c8f657dfc1e21b613",
            "6af04b13bcb64e6988526f534ce7fe20",
            "85927480ea444ffbabdf2ec621ff616a",
            "252f6ce6e0b84872bb90a478b9a71135",
            "70e130e58cfb4964acce486ec2bfddd4",
            "95236a32f0a5453b936d439fd3552c7c",
            "5f5124a726b24f73a9d4f94fa7cdded1",
            "4cecd4be6499487a9a62cc576631700e"
          ]
        },
        "id": "Ub2gBkhi1ML8",
        "outputId": "a5718e8a-87da-43e3-c8f1-8ec375c13c6f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting General Translation ---\n",
            "Document split into 9 parts.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Translating:   0%|          | 0/9 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f9431f3833d64504828fd6054d20c4a6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Success] Process complete! File saved as: translated_general.txt\n",
            "------------------------------\n",
            "Preview of the result:\n",
            "دراكولا\n",
            "من تأليف برام ستوكر\n",
            "المطبوعة عام 1897\n",
            "الفصل الأول\n",
            "سجل جوناثان هاركر\n",
            "3 مايو. بستريتسيز. - غادرنا ميونخ في الساعة الثامنة والثلاثين مساءً في اليوم الأول من مايو، ووصلنا إلى فيينا في الصباح الباكر التالي، كان يجب أن نصل في الساعة السادسة وأربعين دقيقة، لكن القطار تأخر ساعة واحدة.\n",
            "يبدو أن بودابست مكان رائع، من نظرة ما استطعتها من خلال القطار وبالقرب الذي استطعت المشي فيه من الشوارع. خفت أن آتي بعيدًا عن المحطة، لأننا وصلنا متأخرون وسوف نغادر في الوقت الصحيح قدر الإمكان.\n",
            "ال impression التي حصلت عليها هي أننا نترك الغرب وندخل الشرق؛ أكثر الجسور المذهلة في العالم يربط بين الغرب والشرق، وهي جس...\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VyL4z7nA1_Tn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/CohereLabs/aya-expanse-8b"
      ],
      "metadata": {
        "id": "6YqwhM1IQnvB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/CohereLabs/c4ai-command-r7b-arabic-02-2025"
      ],
      "metadata": {
        "id": "kSgLKRxLQa8z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/bartowski/aya-expanse-32b-GGUF/tree/main"
      ],
      "metadata": {
        "id": "Nk3JeRBaP_kS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/bartowski/aya-expanse-8b-GGUF/tree/main"
      ],
      "metadata": {
        "id": "iiEiJeiq26Vn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/spaces/CohereLabs/command-a-translate"
      ],
      "metadata": {
        "id": "NMhjm1oN4y9w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/bartowski/CohereLabs_command-a-reasoning-08-2025-GGUF/tree/main/CohereLabs_command-a-reasoning-08-2025-Q3_K_M"
      ],
      "metadata": {
        "id": "Kc0Fcigc53ll"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://huggingface.co/bartowski/c4ai-command-r7b-12-2024-GGUF/tree/main"
      ],
      "metadata": {
        "id": "eWeICMqq6U2Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "https://huggingface.co/CohereLabs/c4ai-command-r7b-12-2024\n",
        "\n",
        "\n",
        "Languages covered: The model has been trained on 23 languages: English, French, Spanish, Italian, German, Portuguese, Japanese, Korean, Arabic, Chinese, Russian, Polish, Turkish, Vietnamese, Dutch, Czech, Indonesian, Ukrainian, Romanian, Greek, Hindi, Hebrew, and Persian.\n",
        "\n",
        "Context length: Command R7B supports a context length of 128K."
      ],
      "metadata": {
        "id": "468xwIFQ26_P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fa9AQJhs6sAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "/content/translated_general.txt\n",
        "دراكولا\n",
        "من تأليف برام ستوكر\n",
        "المطبوعة عام 1897\n",
        "الفصل الأول\n",
        "سجل جوناثان هاركر\n",
        "3 مايو. بستريتسيز. - غادرنا ميونخ في الساعة الثامنة والثلاثين مساءً في اليوم الأول من مايو، ووصلنا إلى فيينا في الصباح الباكر التالي، كان يجب أن نصل في الساعة السادسة وأربعين دقيقة، لكن القطار تأخر ساعة واحدة.\n",
        "يبدو أن بودابست مكان رائع، من نظرة ما استطعتها من خلال القطار وبالقرب الذي استطعت المشي فيه من الشوارع. خفت أن آتي بعيدًا عن المحطة، لأننا وصلنا متأخرون وسوف نغادر في الوقت الصحيح قدر الإمكان.\n",
        "ال impression التي حصلت عليها هي أننا نترك الغرب وندخل الشرق؛ أكثر الجسور المذهلة في العالم يربط بين الغرب والشرق، وهي جسر على النهر الدانوب الذي هنا من العرض الكبير والمعمولة العميقة، ووضعنا بين التقاليد الحاكمة للعثمانيين.\n",
        "\n",
        "================================================== صفحة 1 ==================================================\n",
        "\n",
        "غادرنا في وقت جيد نسبيًا، ووصلنا بعد غروب الشمس إلى كلوسنبرج. توقفت هنا ليلتني في فندق رويال. أكلت في الغداء أو بالأحرى العشاء، دجاجة مطهورة بطريقة معينة باستخدام الفلفل الأحمر، كانت طعمة جيدة ولكن جفيفة. (ملاحظة: احصل على وصفة لها ل مينا). سألت الخادم، وقال لي إنها تسمى \"باپريكا هينديل\"، وأنها طعام وطني، nên يجب أن أستطيع الحصول عليها في أي مكان.\n",
        "\n",
        "في أي مكان على جبال الكاربات.\n",
        "وجدت أن معرفتي القليله باللغة الألمانية كانت مفيدة هنا، و لا أعرف كيف سأستطيع العيش بدونها.\n",
        "بعدما استفدت من بعض الوقت الذي كان متوفرا لي في لندن، زرت المتحف البريطاني وقمت ببحث بين الكتب والخرائط في المكتبة حول تراسيا، و أدركت أن المعرفة القليله بالبلاد سوف تكون مهمة في التعامل مع نبيل من تلك البلاد.\n",
        "أنا وجدت أن المنطقة التي ذكرها هو في أقصى شرق البلاد، على حدود ثلاث دول: تراسيا ومولدافيا وبوكوفينا، وسط جبال الكاربات؛ واحدة من أجزاء أوروبا الأشد غموضاً وعدم المعرفة.\n",
        "لم استطع العثور على أي خريطة أو عمل يبين موقع قصر دراكولا بدقة، لأن لا توجد خرائط لهذه البلاد حتى الآن مثل خرائط المسح الجغرافي البريطاني التي نستخدمها؛ لكنني وجدت أن بستريتز، المدينة المحلية المذكورة من قبل كونت دراكولا، هي بلدة معروفة.\n",
        "سأضيف هنا بعض ملاحظاتي، حيث قد تساعدني في استعادة الذاكرة عندما أتحدث مع مينا عن رحلاتي.\n",
        "في سكان تراسيا هناك أربع طوائف مختلفة:\n",
        "(Translation continues...)\n",
        "\n",
        "في الجنوب Saxons، وهم مخلوطون بWallachs، الذين هم أحفاد Dacians؛ في الغرب Magyars، وفي الشرق والشمال Szekelys. أنا أذهب إلى الأخير، الذين يزعمون أنهم من ذريّة Attila وحمّال الهواء. قد يكون هذا صحيحًا، فإذا كان Magyars قد غزوا البلاد في القرن الحادي عشر، وجدوا حمّال الهواء مقيمين فيها.\n",
        "\n",
        "قرأت أن كل ما يعرف من خرافات العالم جمع في حلق Carpathians، كأنها مركز لبعض نوع من الدوران الخيالي؛ فربما تكون إقامتاي مثيرة. (ملاحظة، يجب أن أسأل الكونت عنهم.)\n",
        "\n",
        "لم أستريح جيدًا، رغم أن سريري كان مريحًا، لأنني كنت أفكر في كل sorts of أحلام غريبة. كانت هناك كلبة تصرخ طوال الليل تحت نافذتي، وقد يكون لها علاقة بهذا؛ أو قد يكون من paprika، لأنني يجب أن أسقي كأس الماء في كاريفي، وازدادت جفافي. نحو الفجر استيقظت وأصبحت مستيقظًا بسبب الضرب المستمر على بابي، فبدو لي أنه كنت أستريح جيدًا آنذاك.\n",
        "\n",
        "كنت قد أكلت في الصباح أكثر من البابريكا، و نوعًا من الحساء من دقيق الذرة الذي قالوا إنه \"ماماليكا\"، و خضروة فلفل ممتلئة بالدجاج المقطّع، وجدت هذه الأكل جيدًا جدًا، ويسمونها \"إمبلاتا\". (مذكورة: حصل على الوصفة لهذا الأكل أيضًا.)\n",
        "\n",
        "كان علي أن أسرع في الأكل لأن القطار بدأ قبل الساعة الثامنة قليلًا، أو ربما كان يجب أن يبدأ بذلك، لأنه بعد سير إلى المحطة في الساعة 7:30 كان علي أن أجلس في القطار لمدة أكثر من ساعة قبل أن نبدء الحركة.\n",
        "\n",
        "يبدو لي أن كلما اتجهنا شرقًا زادت عدم دقتهم. ماذا يجب أن يكونوا في الصين؟\n",
        "\n",
        "كانت اليوم تبدو وكأننا نستغرق في بلاد مليئة بالجمال من كل نوع. أحيانًا رأينا مدن صغيرة أو قصور على قمم جبال مرتفعة مثل تلك التي نراها في القصص المقدسة; وأحيانًا سرعنا بسهولة عبر أنهار وآبار يبدو من الحافة الحجريّة الواسعة من كل جانب منها أنها عرضة للفيضانات الكبيرة. تحتاج كمية كبيرة من الماء، وسرعة قوية، لتنظيف حافة خارجيّة لنهر.\n",
        "\n",
        "في كل محطة كان هناك مجموعات من الناس، أحيانًا تجمعات، وجميعهم يرتدون ملابس مختلفة. بعضهم كانوا مثل الفلاحين في الوطن أو الذين رأيت في...\n",
        "\n",
        "كانوا يأتون من فرنسا وألمانيا، يرتدون قميص قصير وطربوش مستدير وقمصان أصلية، ولكن كان بعضهم يبدو متميزاً.\n",
        "\n",
        "نظروا إلى النساء جميلات في البداية، لكن عندما اقتربنا منهن، اكتشفنا أنهن يبدون غير متكاملات في الخصر. كانت جميعهن تلبسن كُمّين بيض آملين من نوع ما، وبالإضافة إلى ذلك، كان معظمهن يرتدين حزاماً كبيراً مزخرفاً بالعديد من الأشرطة التي تتمايل مثل القصبات في الباليه، ولكن طبعاً، كانت هناك قميصات تحتها.\n",
        "\n",
        "أكبر ما شاهدناه من حيث الظرافة كان السلوڤاكيون، الذين كانوا أكثر بربرية من باقي الناس، مع أطواد رفيعة كبيرة وقمصان بيضاء متسخة كبيرة وقمصان من القطن البيض الأبيض، وحزامين كبيرين من الجلد الثقيل، يبلغ عرضهم تقريباً قدمًا واحدًا، مزخرفين بالعديد من النجوم البرونزية. كانوا يرتدون حذاء عالي، مع تثبيت قميصهم في داخله، وكان لديهم شعر أسود طويل ومستأصلة أسود ثقيلة. هم أصحاب مظهر جميل، لكن لا يبدو أنهن يمتلكن المظهر الجذب. لو كانوا على المسرح، لكانوا تمثيلًا قديمًا من فرقة عرائس بربرية شرقية. ومع ذلك، وفقاً للمعلومات التي تلقيتها، هم أفراد هينون ومتخاذلون في التعبير عن احتياجاتهم الطبيعية.\n",
        "\n",
        "كانت الساعة متأخرة من الليل عندما وصلنا إلى بستريتز، وهي مدينة قديمة ومثيرة. يقع على حافة البلاد -فإن ممر بورغو يؤدي منها إلى بوكوفينا- ولذلك فقد عانى من وجودها حياة مضطربة ويتضح ذلك في كل مكان. قبل خمسين عاماً حدثت سلسلة من الحوادث الكبرى، مما أسفر عن دمار كبير على خمس مناسبات مختلفة. وفي بداية القرن السابع عشر تعرضت للاستيلاء على مدة ثلاثة أسابيع ووقعت 13 ألف قتيل، وكانت الخسائر في الحرب تزداد سوءاً بسبب الجوع والمرض.\n",
        "\n",
        "أمرني كونت دراكولا بالذهاب إلى فندق الكرون الذهبي، الذي وجدته بمظهر قديم جداً، لأنني كنت أريد أن أرى كل ما يمكنني من شئون البلد.\n",
        "\n",
        "كانت هناك توقعات بأنني سأكون هناك، فعندما اقتربت من الباب واجهت امرأة مسنة طيبة المظهر ترتدي ملابس الفلاحين العادية -لباس أسفل أبيض مع فستان ضخم ملون يصل إلى الكتف- عندما اقتربت منها قالت: \"هو السيد الإنجليزي؟\" قلت لها: \"نعم، جوناثان هاركر.\" ابتسمت وارسلت رسالة إلى رجل مسن يرتدي قميصاً أبيضاً، كان قد اتبعها إلى الباب.\n",
        "\n",
        "ذهب، لكنه عاد فورًا مع بريد:\n",
        "\"أحِبُّكَ.--أُرحَبُ لكَ بالكاربات. أَنظُرُ إلى وصولكِ بِالحرص. استَرقِ اللَيْلَ جيدًا. في الثَلاثِة غدًا سيبدأ السَفَرُ لِبُكوڤينا، ويُحتفظُ بمكانٍ لكَ فيهِ. في مضيق البورغو سوف ينتظركِ عربتي وسيأخذكِ إلَيَّ. أَتمنى أن يكونَ سفرك من لندن سعيدًا، وأن تُسَرَّعَ في إقامتك في أرضي الجميلة.--أَحِبُّكَ,\n",
        "دروكولا.\"\n",
        "4 مايو- وجدتُ أنّ أهلي قد حصلوا على بريدٍ من القسيس، يَدِرُهُم بِحجز أفضل مكانٍ في السَفَرِ لي؛ لكن عند سؤالهم عن تفاصيلِهِ بدوا مترددين، واعترفوا بأَنَّهم لا يستطيعون فهمي الألمانية.\n",
        "\n",
        "================================================== صفحة 4 ==================================================\n",
        "\n",
        "لا يمكن أن يكونَ هذا الصادقًا، لأنّهم كانوا يفهمونها جيدًا حتى ذلك الحين؛ على الأقل، كانت الإجاباتُ لأسئلتي دقيقةً كأَنَّهم يستطيعون فهمها. وقف الزعيم وزوجته، السيدة المسنة التي استقبلتني، ينظران بعضهما البعض بِنظرة خوفية نوعًا ما. بدا أنّ المال قد أُرسِلَ في بريدٍ، وأَنَّ ذلك كل ما يعرفونه. عندما سألتهُم إن كانوا يعرفون القسيس دروكولا،\n",
        "\n",
        "وكانا يصليان الصلاة، ويتقادمان، وقالوا إنهما لا يعرفان شيئًا من ذلك، فلم يتحدثا أيًا. كان الوقت قريبًا من انطلاق السفر، ولم أكن لدي وقت لأسألใครًا آخر، لأن كل شيء كان غامضًا ومثيرًا للخوف.\n",
        "\n",
        "وكانت السيدة العجوز تحضر إلى غرفتي قبل أن أخرج، وقالت بحركة هستيرية: \"لا يجب أن تذهب؟ يا شاب الهير! لا يجب أن تذهب؟\" كانت في حالة من التهيج الشديد حتى فقدت سيطرتها على اللغة الألمانية التي تعلمتها، وخلطت بينها وبين لغة أخرى لا أعرفها.\n",
        "\n",
        "كنت أستطيع اتباع كلامها عن طريق أسئلة متكررة. عندما قلت لها إنني يجب أن أذهب الآن، وأنني مشغول في شؤون مهمة، سألت مرة أخرى: \"هل تعرف ما اليوم؟\" أجبتها أن اليوم الرابع من مايو.\n",
        "\n",
        "وقفت رأسها وهي تقول مرة أخرى: \"نعم! أعرف ذلك! أعرف ذلك، ولكن هل تعرف ما اليوم؟\" عندما قلت لها إنني لا أفهم،続ت على قائلي:\n",
        "\n",
        "\"هو اليوم قبل عيد القديس جورج. هل لا تعرف أن هذا الليل، عند صوت الساعة في منتصف الليل، سيتمتع كل الشرور في العالم بالسيطرة؟ هل لا تعرف ذلك؟\n",
        "\n",
        "عرفت مكانك و ما ستذهب إليه؟ كانت في حالة من القلق البليغ، فحاولت أن أطمئنها دون جدوى. ثم انهضت على ركبتي وتوسلت إليّ أن لا أغادرها؛ على الأقل أن آخُذ يومًا أو اثنين قبل الخروج.\n",
        "\n",
        "كان كل ذلك أمرًا سخيفًا، لكنني لم أشعر بالراحة. ومع ذلك، كان هناك أعمال يجب إنجازها، ولا يمكن لي أن ألوي أي شيء من أجلها.\n",
        "\n",
        "حاولت أن أعيدها على قوتها وقولت لها بجدية ما شكرتها، لكن واجبي كان أمرًا لازمًا، وبالتالي يجب عليّ الخروج.\n",
        "\n",
        "ثم站ت وأسقمت عينيها، ثم أخذت صليبًا من حول عنقها وقدمته لي.\n",
        "\n"
      ],
      "metadata": {
        "id": "S3TxTA1PGa5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "رأيي بصراحة: الترجمة جيدة كبداية لفهم المعنى العام، ولكنها \"غير صالحة للنشر\" أو القراءة الممتعة بوضعها الحالي.\n",
        "\n",
        "إليك تحليلي التفصيلي للمشاكل الموجودة في النص (ولماذا حدثت)، وكيفية إصلاحها برمجياً:\n",
        "1. المشاكل الكبرى في النص (The Issues):\n",
        "\n",
        "    بقايا الإنجليزية (Code Switching):\n",
        "    النموذج \"تكسال\" عن ترجمة بعض الكلمات وتركها كما هي:\n",
        "\n",
        "        أمثلة: impression، Saxons، Wallachs، Magyars، paprika، sorts of.\n",
        "\n",
        "        السبب: النموذج اعتبر هذه الكلمات مصطلحات علمية أو أسماء لا تترجم، أو أن \"الموجه\" لم يكن صارماً بما يكفي لتعريب الأسماء.\n",
        "\n",
        "    \"خلل\" الحروف الصينية (Glitches):\n",
        "    ظهرت حروف صينية وسط الكلمات العربية!\n",
        "\n",
        "        أمثلة: 续ت (بدلاً من \"استمرت\")، 站ت (بدلاً من \"وقفت\").\n",
        "\n",
        "        السبب: هذا يحدث غالباً عندما تكون الـ Temperature منخفضة جداً (0.1)، مما يجعل النموذج يعلق في \"توكن\" خاطئ ولا يستطيع الخروج منه.\n",
        "\n",
        "    أخطاء في المعنى (كارثية ومضحكة):\n",
        "\n",
        "        \"أحبك... دراكولا\": ختم الرسالة بكلمة \"أحبك\"! (في النص الأصلي \"Your friend\" أو \"Yours\"). دراكولا وحش مرعب وليس محباً ولهاناً! 😂\n",
        "\n",
        "        \"القسيس\": يبدو أنه ترجم كلمة ما خطأً، فدراكولا هو \"كونت\" (Count) وليس قسيساً.\n",
        "\n",
        "    الركاكة اللغوية:\n",
        "\n",
        "        جمل مثل \"ال impression التي حصلت عليها\" ضعيفة جداً."
      ],
      "metadata": {
        "id": "ET1TSmDoGXcs"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LC8lITvNGm0L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kdND6icEGmwd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5I8UiqVoGmqL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ollama\n",
        "import os\n",
        "import time\n",
        "import re\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# ------------------- Configuration -------------------\n",
        "INPUT_FILE = \"text.txt\"\n",
        "OUTPUT_FILE = \"dracula_arabic.txt\"\n",
        "MODEL_NAME = \"llama3.1:8b\"\n",
        "CHUNK_SIZE = 1200\n",
        "\n",
        "# ------------------- LITERARY SYSTEM PROMPT -------------------\n",
        "# هذا الموجه مخصص للأدب والروايات\n",
        "# يطلب من النموذج تعريب الأسماء (Transliteration) بدلاً من تركها بالإنجليزية\n",
        "SYSTEM_PROMPT = (\n",
        "    \"ROLE: You are a professional literary translator aimed at Arab readers.\\n\"\n",
        "    \"TASK: Translate the novel text into fluent, captivating Modern Standard Arabic.\\n\"\n",
        "    \"CRITICAL RULES:\\n\"\n",
        "    \"1. NO ENGLISH LEFT: You must transliterate proper names (e.g., 'Saxons' -> 'السكسونيون', 'Bistritz' -> 'بيستريتز'). Do not leave ANY Latin characters.\\n\"\n",
        "    \"2. NO GLITCHES: Do NOT use Chinese characters (like 站, 续) or Hindi characters. Use ONLY Arabic script.\\n\"\n",
        "    \"3. TONE: Maintain the atmospheric, gothic, and antique tone of the novel (19th century style).\\n\"\n",
        "    \"4. ACCURACY: Translate 'Count' as 'الكونت' (not priest). Translate endings like 'Yours' as 'المخلص لك' (not I love you).\\n\"\n",
        "    \"5. OUTPUT: Provide ONLY the Arabic text.\"\n",
        ")\n",
        "\n",
        "def remove_foreign_chars(text):\n",
        "    \"\"\"\n",
        "    دالة تنظيف إضافية لإزالة أي حروف صينية أو إنجليزية قد تتسرب\n",
        "    \"\"\"\n",
        "    # السماح فقط بالعربية، الأرقام، وعلامات الترقيم الأساسية\n",
        "    # هذا التعبير النمطي (Regex) يحذف الحروف اللاتينية والصينية والهندية\n",
        "    clean_text = re.sub(r'[a-zA-Z]', '', text) # حذف الإنجليزية\n",
        "    # (يمكن إضافة المزيد من الفلاتر هنا إذا لزم الأمر)\n",
        "    return clean_text\n",
        "\n",
        "def split_text_smart(text, max_length):\n",
        "    chunks = []\n",
        "    while len(text) > max_length:\n",
        "        split_index = text.rfind('\\n', 0, max_length)\n",
        "        if split_index == -1: split_index = text.rfind('.', 0, max_length)\n",
        "        if split_index == -1: split_index = text.rfind(' ', 0, max_length)\n",
        "        if split_index == -1: split_index = max_length\n",
        "        chunks.append(text[:split_index + 1])\n",
        "        text = text[split_index + 1:].strip()\n",
        "    if text: chunks.append(text)\n",
        "    return chunks\n",
        "\n",
        "def translate_segment(text_chunk, retries=3):\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = ollama.chat(\n",
        "                model=MODEL_NAME,\n",
        "                messages=[\n",
        "                    {'role': 'system', 'content': SYSTEM_PROMPT},\n",
        "                    {'role': 'user', 'content': text_chunk},\n",
        "                ],\n",
        "                options={\n",
        "                    'temperature': 0.4, # رفعنا الحرارة قليلاً للسرد القصصي\n",
        "                    'top_p': 0.9,\n",
        "                    'num_predict': 2048,\n",
        "                }\n",
        "            )\n",
        "\n",
        "            content = response['message']['content']\n",
        "\n",
        "            # التحقق من وجود حروف صينية شائعة في الأخطاء\n",
        "            if any(char in content for char in ['站', '续', '是', '的']):\n",
        "                 raise ValueError(\"Detected Chinese glitch characters!\")\n",
        "\n",
        "            return content\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n[Warning] Attempt {attempt+1} failed. Reason: {e}\")\n",
        "            time.sleep(1)\n",
        "\n",
        "    return \"[Translation Failed for this part]\"\n",
        "\n",
        "def main():\n",
        "    if not os.path.exists(INPUT_FILE):\n",
        "        print(\"File not found.\")\n",
        "        return\n",
        "\n",
        "    print(\"--- Starting Literary Translation (Dracula Mode) ---\")\n",
        "\n",
        "    with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        full_text = f.read()\n",
        "\n",
        "    chunks = split_text_smart(full_text, CHUNK_SIZE)\n",
        "    print(f\"Split into {len(chunks)} parts.\")\n",
        "\n",
        "    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\")\n",
        "\n",
        "    for chunk in tqdm(chunks, desc=\"Translating\"):\n",
        "        result = translate_segment(chunk)\n",
        "\n",
        "        # تنظيف إضافي (اختياري)\n",
        "        # result = remove_foreign_chars(result)\n",
        "\n",
        "        with open(OUTPUT_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(result + \"\\n\\n\")\n",
        "\n",
        "    print(f\"\\n[Done] Check {OUTPUT_FILE}\")\n",
        "\n",
        "    # عرض عينة\n",
        "    if os.path.exists(OUTPUT_FILE):\n",
        "        with open(OUTPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "            print(f.read()[:500] + \"...\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "DEzJXFTjGmmf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dac6tys0HVAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fx8q_QNIHU-Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "S1Bo_WsYHU7Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ollama\n",
        "import os\n",
        "import time\n",
        "import re\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# ------------------- Settings -------------------\n",
        "INPUT_FILE = \"/content/1.txt\"           # The file you want to translate\n",
        "OUTPUT_FILE = \"translated_general.txt\"\n",
        "MODEL_NAME = \"llama3.1:8b\"\n",
        "CHUNK_SIZE = 1000                 # Safe size to keep context\n",
        "\n",
        "# ------------------- UNIVERSAL SYSTEM PROMPT -------------------\n",
        "# This prompt guides the AI to behave like a human translator\n",
        "# regardless of the text type (Fiction, Non-fiction, etc.)\n",
        "\n",
        "SYSTEM_PROMPT = (\n",
        "    \"ROLE: You are an expert professional translator proficient in English and Modern Standard Arabic (Fusha).\\n\"\n",
        "    \"TASK: Translate the provided text into natural, fluent Arabic.\\n\"\n",
        "    \"GUIDELINES:\\n\"\n",
        "    \"1. TRANSFORMATION: Do not translate word-for-word. Translate the *meaning* and *context*.\\n\"\n",
        "    \"2. PROPER NOUNS: Transliterate names and places into Arabic phonetically (e.g., 'London' -> 'لندن', 'Harker' -> 'هاركر'). Do NOT leave them in English.\\n\"\n",
        "    \"3. IDIOMS: Adapt idioms and letter closings to their Arabic equivalents (e.g., 'Yours' at end of letter -> 'المخلص لك').\\n\"\n",
        "    \"4. SCRIPT: The output must be 100% Arabic script. Do not use Hindi/Chinese characters.\\n\"\n",
        "    \"5. PUNCTUATION: Adjust punctuation to suit Arabic writing standards.\"\n",
        ")\n",
        "\n",
        "# ---------------------------------------------------------------\n",
        "\n",
        "def split_text_smart(text, max_length):\n",
        "    \"\"\"\n",
        "    Splits text attempting to keep paragraphs and sentences intact.\n",
        "    \"\"\"\n",
        "    chunks = []\n",
        "    while len(text) > max_length:\n",
        "        # Priority 1: Newline (Paragraph)\n",
        "        split_index = text.rfind('\\n', 0, max_length)\n",
        "\n",
        "        # Priority 2: Period (Sentence)\n",
        "        if split_index == -1:\n",
        "            split_index = text.rfind('.', 0, max_length)\n",
        "\n",
        "        # Priority 3: Space (Word)\n",
        "        if split_index == -1:\n",
        "            split_index = text.rfind(' ', 0, max_length)\n",
        "\n",
        "        # Fallback\n",
        "        if split_index == -1:\n",
        "            split_index = max_length\n",
        "\n",
        "        chunks.append(text[:split_index + 1])\n",
        "        text = text[split_index + 1:].strip()\n",
        "\n",
        "    if text:\n",
        "        chunks.append(text)\n",
        "    return chunks\n",
        "\n",
        "def translate_segment(text_chunk, retries=3):\n",
        "    \"\"\"\n",
        "    Translates a chunk with retry logic to handle model instability.\n",
        "    \"\"\"\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = ollama.chat(\n",
        "                model=MODEL_NAME,\n",
        "                messages=[\n",
        "                    {'role': 'system', 'content': SYSTEM_PROMPT},\n",
        "                    {'role': 'user', 'content': text_chunk},\n",
        "                ],\n",
        "                options={\n",
        "                    # Temperature 0.35 is balanced:\n",
        "                    # High enough to be creative with literary text.\n",
        "                    # Low enough to avoid hallucinations/glitches.\n",
        "                    'temperature': 0.35,\n",
        "                    'top_p': 0.9,\n",
        "                    'num_predict': 2048,\n",
        "                }\n",
        "            )\n",
        "\n",
        "            content = response['message']['content']\n",
        "\n",
        "            # Basic sanity check: If output is empty, retry\n",
        "            if not content.strip():\n",
        "                raise ValueError(\"Empty response received\")\n",
        "\n",
        "            return content\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n[Warning] Attempt {attempt+1} failed. Reason: {e}\")\n",
        "            time.sleep(1)\n",
        "\n",
        "    return \"[Error: Could not translate this segment]\"\n",
        "\n",
        "def main():\n",
        "    if not os.path.exists(INPUT_FILE):\n",
        "        print(f\"[Error] File '{INPUT_FILE}' not found.\")\n",
        "        return\n",
        "\n",
        "    print(\"--- Starting General Translation ---\")\n",
        "\n",
        "    with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        full_text = f.read()\n",
        "\n",
        "    chunks = split_text_smart(full_text, CHUNK_SIZE)\n",
        "    print(f\"Document split into {len(chunks)} parts.\")\n",
        "\n",
        "    # Clear output file\n",
        "    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\")\n",
        "\n",
        "    # Process\n",
        "    for chunk in tqdm(chunks, desc=\"Translating\"):\n",
        "        result = translate_segment(chunk)\n",
        "\n",
        "        with open(OUTPUT_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(result + \"\\n\\n\")\n",
        "\n",
        "    print(f\"\\n[Success] Translation saved to: {OUTPUT_FILE}\")\n",
        "\n",
        "    # Preview\n",
        "    print(\"-\" * 30)\n",
        "    print(\"Preview:\")\n",
        "    if os.path.exists(OUTPUT_FILE):\n",
        "        with open(OUTPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "            print(f.read()[:600] + \"...\")\n",
        "    print(\"-\" * 30)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391,
          "referenced_widgets": [
            "afde0c3d92384a638f5eb4910faab878",
            "c8b51b1dd3f54ca2b1542e960f2c0b3f",
            "9f306a87336d4a3b8a61eae6c98c71ed",
            "667413fc7ae144dda1f9a3a5d20a2b58",
            "2734297110764a27ae7508c76f54f197",
            "f9990d62da1f47d691fcde94ec4fcc4a",
            "63e1ffe83d59402db3571bc476d36415",
            "767fb82e377e42ebb8841eb7a5f09216",
            "8c2b61a8f9854ff48bf99d9db191108a",
            "bc7aed2928cc45c5a85a0f773c87032a",
            "b8111ddaa9ee44d4a6c5cd0222f67980"
          ]
        },
        "id": "lmuyD75kHU3y",
        "outputId": "3d7d7d64-0856-42be-fd95-3350ebc3633d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Starting General Translation ---\n",
            "Document split into 11 parts.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Translating:   0%|          | 0/11 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "afde0c3d92384a638f5eb4910faab878"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[Success] Translation saved to: translated_general.txt\n",
            "------------------------------\n",
            "Preview:\n",
            "دكتوراكولا\n",
            "من تأليف\n",
            "برام ستوكر\n",
            "المطبوعة عام 1897\n",
            "الفصل الأول\n",
            "سجل جوناثان هاركر\n",
            "3 مايو. بستريتسه.--ترك ميونخ في الساعة 8:35 مساءً في 1 مايو، ووصل إلى فيينا في الصباح التالي مبكرًا؛ كان يجب أن نصل في الساعة 6:46، لكن القطار تأخر ساعة واحدة.\n",
            "بدا لي أن بودابست أمر رائع، من نظرة سريعة لها من القطار والقليل الذي استطعت المشي به خلال الشوارع. خشي أن أذهب بعيدًا عن المحطة، لأننا وصلنا متأخرون وسيبدأون في وقت تقريبًا من الوقت الصحيح.\n",
            "ال impression التي حصلت عليها كانت أننا نغادر الغرب وندخل الشرق؛ أحد الجسور المذهلة على النهر الدانوب، الذي هنا يمتلك عرضًا عريضًا ومعمولًا، حملنا بين الآثار التركية.\n",
            "\n",
            "===...\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ":\n",
        "1. التحسن الملحوظ (الجيد):\n",
        "\n",
        "    اختفاء الأحرف الصينية والهندية: لم تعد تظهر رموز غريبة مثل 续 أو एज، وهذا إنجاز كبير بفضل الفلاتر وضبط الحرارة.\n",
        "\n",
        "    القضاء على الإنجليزية: معظم الكلمات تُرجمت أو عُرّبت (مثل \"بستريتز\"، \"ببريكا هانديل\").\n",
        "\n",
        "    السرد مفهوم: يمكنك قراءة النص وفهم القصة وتتابع الأحداث.\n",
        "\n",
        "2. المشاكل المستمرة (التي تحتاج لحل):\n",
        "\n",
        "    بطء شديد: ذكرت أن الترجمة \"استمرت وقت كبير\". هذا متوقع لأن النموذج Llama 3.1 8B ثقيل نسبياً على المعالج (CPU).\n",
        "\n",
        "    بقايا تعليمات (Leaking): النموذج ما زال يطبع ملاحظات بالإنجليزية وسط النص العربي!\n",
        "\n",
        "        مثال: Note: I followed the guidelines...\n",
        "\n",
        "        مثال: Note: Transliteration of \"St. George's Day\"...\n",
        "\n",
        "        مثال: appearanceاً (كلمة إنجليزية مدمجة بتنوين عربي! هذا \"Glitch\" جديد ومضحك).\n",
        "\n",
        "    أخطاء ترجمة السياق:\n",
        "\n",
        "        \"أحبك... دراكولا\": ما زالت موجودة! لم يفهم أن Yours تعني \"المخلص لك\".\n",
        "\n",
        "        \"أجنبي إنجليزي؟\": (The English Herr?) ترجمها \"الأجنبي\" بدلاً من \"السيد\".\n",
        "\n",
        "        \"الدوقة دراكولا\": حول الكونت إلى أنثى (دوقة)!\n",
        "\n",
        "الحل لمشكلة البطء + الجودة (The Final Optimization)\n",
        "\n",
        "لحل مشكلة البطء، سنقوم بخطوتين:\n",
        "\n",
        "    تفعيل تسريع GPU (إجباري): ذكرت في البداية أنك تستخدم Colab. يجب التأكد من أنك تستخدم T4 GPU وليس المعالج فقط.\n",
        "\n",
        "    استخدام نموذج أصغر (Phi-3 Mini): إذا كنت مصراً على السرعة الفائقة، فنموذج Phi-3 من مايكروسوفت (3.8 مليار بارامتر) أسرع بـ 3 أضعاف من Llama 3 (8 مليار)، وجيد جداً في الترجمة.\n",
        "\n",
        "إليك الكود المحدث بالكامل الذي:\n",
        "\n",
        "    يستخدم نموذج Phi-3 (الأسرع والأخف).\n",
        "\n",
        "    يحتوي على فلتر صارم (Strict Cleaner) يحذف أي ملاحظات إنجليزية (Note: ...) من الناتج النهائي قبل حفظه.\n",
        "\n"
      ],
      "metadata": {
        "id": "ceMdZhZ7X1KL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import ollama\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# ------------------- Settings -------------------\n",
        "INPUT_FILE = \"text.txt\"\n",
        "OUTPUT_FILE = \"translated_generic.txt\"\n",
        "MODEL_NAME = \"llama3.1:8b\"  # Best balance for quality\n",
        "CHUNK_SIZE = 1000           # Optimal size for context\n",
        "\n",
        "# ------------------- GENERIC SYSTEM PROMPT -------------------\n",
        "# No hardcoded dictionaries. Relies on the model's intelligence.\n",
        "SYSTEM_PROMPT = (\n",
        "    \"ROLE: You are a professional translator proficient in English and Modern Standard Arabic.\\n\"\n",
        "    \"TASK: Translate the provided text into fluent, natural Arabic based on the context.\\n\"\n",
        "    \"RULES:\\n\"\n",
        "    \"1. CONTEXT: Analyze the text style (narrative, technical, or formal) and translate accordingly.\\n\"\n",
        "    \"2. NO NOTES: Do NOT provide explanations, headers, or notes like 'Here is the translation'. Output ONLY the Arabic text.\\n\"\n",
        "    \"3. SCRIPT: Use only Arabic script. Transliterate proper names if necessary.\\n\"\n",
        "    \"4. PUNCTUATION: Adjust punctuation to match Arabic standards.\"\n",
        ")\n",
        "\n",
        "def remove_english_artifacts(text):\n",
        "    \"\"\"\n",
        "    Filters out lines that contain mostly English characters.\n",
        "    This solves the issue of 'Note: I followed the guidelines...' appearing in the output.\n",
        "    \"\"\"\n",
        "    lines = text.split('\\n')\n",
        "    cleaned_lines = []\n",
        "\n",
        "    for line in lines:\n",
        "        # Check if the line has English letters\n",
        "        if re.search(r'[a-zA-Z]', line):\n",
        "            # Count English vs Arabic chars\n",
        "            english_char_count = len(re.findall(r'[a-zA-Z]', line))\n",
        "            total_len = len(line.strip())\n",
        "\n",
        "            # If a line is more than 30% English, discard it (it's likely a note)\n",
        "            if total_len > 0 and (english_char_count / total_len) > 0.3:\n",
        "                continue\n",
        "\n",
        "        cleaned_lines.append(line)\n",
        "\n",
        "    return '\\n'.join(cleaned_lines).strip()\n",
        "\n",
        "def split_text_smart(text, max_length):\n",
        "    chunks = []\n",
        "    while len(text) > max_length:\n",
        "        # Split by paragraph first\n",
        "        split_index = text.rfind('\\n', 0, max_length)\n",
        "        if split_index == -1: split_index = text.rfind('.', 0, max_length)\n",
        "        if split_index == -1: split_index = text.rfind(' ', 0, max_length)\n",
        "        if split_index == -1: split_index = max_length\n",
        "\n",
        "        chunks.append(text[:split_index + 1])\n",
        "        text = text[split_index + 1:].strip()\n",
        "\n",
        "    if text:\n",
        "        chunks.append(text)\n",
        "    return chunks\n",
        "\n",
        "def translate_segment(text_chunk, retries=3):\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = ollama.chat(\n",
        "                model=MODEL_NAME,\n",
        "                messages=[\n",
        "                    {'role': 'system', 'content': SYSTEM_PROMPT},\n",
        "                    {'role': 'user', 'content': text_chunk},\n",
        "                ],\n",
        "                options={\n",
        "                    'temperature': 0.3, # Balanced for general use\n",
        "                    'num_predict': 2048,\n",
        "                }\n",
        "            )\n",
        "\n",
        "            content = response['message']['content']\n",
        "\n",
        "            # Apply the filter to remove English notes immediately\n",
        "            cleaned_content = remove_english_artifacts(content)\n",
        "\n",
        "            return cleaned_content\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\n[Warning] Attempt {attempt+1} failed. Error: {e}\")\n",
        "            time.sleep(1)\n",
        "\n",
        "    return \"\" # Return empty string on failure rather than error message\n",
        "\n",
        "def main():\n",
        "    if not os.path.exists(INPUT_FILE):\n",
        "        print(f\"[Error] File '{INPUT_FILE}' not found.\")\n",
        "        return\n",
        "\n",
        "    print(\"--- Starting General Translation ---\")\n",
        "\n",
        "    with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        full_text = f.read()\n",
        "\n",
        "    chunks = split_text_smart(full_text, CHUNK_SIZE)\n",
        "    print(f\"Document split into {len(chunks)} parts.\")\n",
        "\n",
        "    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\")\n",
        "\n",
        "    for chunk in tqdm(chunks, desc=\"Translating\"):\n",
        "        result = translate_segment(chunk)\n",
        "\n",
        "        if result:\n",
        "            with open(OUTPUT_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "                f.write(result + \"\\n\\n\")\n",
        "\n",
        "    print(f\"\\n[Success] Done! Saved to: {OUTPUT_FILE}\")\n",
        "\n",
        "    # Preview\n",
        "    if os.path.exists(OUTPUT_FILE):\n",
        "        print(\"-\" * 30)\n",
        "        with open(OUTPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "            print(f.read()[:500] + \"...\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "pP07gbwMX2Vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "للحصول على سرعة عالية جداً ومساحة صغيرة (أقل من 2 جيجا) مع دعم جيد للعربية، أنصحك باستخدام موديل Gemma 2 (نسخة 2B) من جوجل، أو Qwen 2.5 (نسخة 1.5B).\n",
        "\n",
        "إليك الخيارات الأخف على الإطلاق:\n",
        "\n",
        "    gemma2:2b (الحجم: 1.6 جيجا فقط) -> ممتاز وسريع جداً.\n",
        "\n",
        "    qwen2.5:1.5b (الحجم: 1.1 جيجا فقط) -> طيارة (أسرع شيء ممكن).\n",
        "\n",
        "    phi3:mini (الحجم: 2.3 جيجا) -> النسخة الصغيرة من Phi3 (أخف بكثير من Medium)."
      ],
      "metadata": {
        "id": "s9x5kqX-Y0Ds"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama pull gemma2:2b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sn6LSLKlZgTf",
        "outputId": "46431442-262f-4057-a692-3e7b9063d6e1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[1G\u001b[?25h\u001b[?2026l\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ollama list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "coCTh8l9Zr3M",
        "outputId": "f15e287a-7394-4561-d044-bb76130edeb0"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NAME           ID              SIZE      MODIFIED       \n",
            "gemma2:2b      8ccf136fdd52    1.6 GB    10 seconds ago    \n",
            "phi3:latest    4f2222927938    2.2 GB    4 hours ago       \n",
            "llama3.1:8b    46e0c10c039e    4.9 GB    4 hours ago       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import ollama\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# ------------------- Settings -------------------\n",
        "INPUT_FILE = \"/content/1.txt\"\n",
        "OUTPUT_FILE = \"translated_fast.txt\"\n",
        "\n",
        "# اخترنا هذا الموديل لأنه خفيف جداً (1.6 GB) ويدعم العربية\n",
        "MODEL_NAME = \"gemma2:2b\"\n",
        "# بديل آخر سريع جداً: \"qwen2.5:1.5b\"\n",
        "\n",
        "CHUNK_SIZE = 800  # حجم أصغر لسرعة أكبر\n",
        "\n",
        "# ------------------- SYSTEM PROMPT -------------------\n",
        "SYSTEM_PROMPT = (\n",
        "    \"ROLE: Professional Translator.\\n\"\n",
        "    \"TASK: Translate text to Arabic.\\n\"\n",
        "    \"RULES:\\n\"\n",
        "    \"1. OUTPUT: Arabic only. No English characters. No Notes.\\n\"\n",
        "    \"2. NAMES: Transliterate names (e.g., Jonathan -> جوناثان).\\n\"\n",
        "    \"3. MEANING: Translate context, not word-for-word.\"\n",
        ")\n",
        "\n",
        "def remove_english_artifacts(text):\n",
        "    \"\"\"Filter out English noise\"\"\"\n",
        "    lines = text.split('\\n')\n",
        "    cleaned_lines = []\n",
        "    for line in lines:\n",
        "        if re.search(r'[a-zA-Z]', line):\n",
        "            english_char_count = len(re.findall(r'[a-zA-Z]', line))\n",
        "            if len(line) > 0 and (english_char_count / len(line)) > 0.4:\n",
        "                continue\n",
        "        cleaned_lines.append(line)\n",
        "    return '\\n'.join(cleaned_lines).strip()\n",
        "\n",
        "def split_text_smart(text, max_length):\n",
        "    chunks = []\n",
        "    while len(text) > max_length:\n",
        "        split_index = text.rfind('\\n', 0, max_length)\n",
        "        if split_index == -1: split_index = text.rfind('.', 0, max_length)\n",
        "        if split_index == -1: split_index = max_length\n",
        "        chunks.append(text[:split_index + 1])\n",
        "        text = text[split_index + 1:].strip()\n",
        "    if text: chunks.append(text)\n",
        "    return chunks\n",
        "\n",
        "def translate_segment(text_chunk, retries=3):\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = ollama.chat(\n",
        "                model=MODEL_NAME,\n",
        "                messages=[\n",
        "                    {'role': 'system', 'content': SYSTEM_PROMPT},\n",
        "                    {'role': 'user', 'content': text_chunk},\n",
        "                ],\n",
        "                options={\n",
        "                    'temperature': 0.2, # حرارة منخفضة للسرعة والدقة\n",
        "                    'num_predict': 1024,\n",
        "                }\n",
        "            )\n",
        "            content = response['message']['content']\n",
        "            return remove_english_artifacts(content)\n",
        "        except Exception as e:\n",
        "            time.sleep(1)\n",
        "    return \"\"\n",
        "\n",
        "def main():\n",
        "    if not os.path.exists(INPUT_FILE):\n",
        "        print(\"File not found.\")\n",
        "        return\n",
        "\n",
        "    # تحميل الموديل تلقائياً إذا لم يكن موجوداً\n",
        "    print(f\"🚀 Pulling model {MODEL_NAME} (Size: ~1.6 GB)...\")\n",
        "    try:\n",
        "        ollama.pull(MODEL_NAME)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        full_text = f.read()\n",
        "\n",
        "    chunks = split_text_smart(full_text, CHUNK_SIZE)\n",
        "    print(f\"⚡ Processing {len(chunks)} chunks with {MODEL_NAME}...\")\n",
        "\n",
        "    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\")\n",
        "\n",
        "    for chunk in tqdm(chunks, desc=\"Translating\"):\n",
        "        result = translate_segment(chunk)\n",
        "        if result:\n",
        "            with open(OUTPUT_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "                f.write(result + \"\\n\\n\")\n",
        "\n",
        "    print(f\"✅ Done! Saved to: {OUTPUT_FILE}\")\n",
        "\n",
        "    # عرض عينة\n",
        "    if os.path.exists(OUTPUT_FILE):\n",
        "        print(\"-\" * 30)\n",
        "        with open(OUTPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "            print(f.read()[:500] + \"...\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301,
          "referenced_widgets": [
            "cb8e935b833a4ad9a98931674345d6ba",
            "1560891b509047e383bdae6966632662",
            "a17b305ee4ed461bbeec10fc9e79cb30",
            "75eb9b6c7cef41738b3dfbf322f1e7cf",
            "a7ff0b1787f94fd793270ec93e091743",
            "0c97a5bd08b041928cb726049f6d7231",
            "99b512c11ca94bfebc75c0de335fe5cc",
            "5d08b33542be4ec99068b6c12fc968fd",
            "0a58ad0273734128bc5da168e31dc4a6",
            "162a77462be04d51a92d1ec432289673",
            "ad27d7475bfd418bb7bd0f1e79014d43"
          ]
        },
        "id": "z58nNR0nY1z3",
        "outputId": "30a2f0c1-8ec2-4ab5-96d0-e4c4d0766c7a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "🚀 Pulling model gemma2:2b (Size: ~1.6 GB)...\n",
            "⚡ Processing 4 chunks with gemma2:2b...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Translating:   0%|          | 0/4 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cb8e935b833a4ad9a98931674345d6ba"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Done! Saved to: translated_fast.txt\n",
            "------------------------------\n",
            "##  ドラcula \n",
            "**3 ماي.** **بِستريتس**--   **إنتقل من مUNCH**  **في الساعة 8:35 مساءً، يوم 1 ماي، ووصل إلى فيينا**\n",
            "**الصباح**\n",
            "**يجب أن أصل إلى فيينا قبل الساعة 6:46، لكن السكّو كان متأخراً بـ ساعة.**\n",
            "**الظاهر أننا نغادر الغرب ونتجه إلى الشرق;  أقدم من أقدم على جسرٌ غربيّ، يقع على نهر الدانوب، الذي هنا واسع وعميق،  ويسلبنا من عادات الحكم التركي.**\n",
            "\n",
            "فُرِطْنَ  أَنْ أُخْلِقْ فِي بَعْضٍ مِنْ حَدِّ الْوَلْدَاء، وَجَانَّتْ نَشْرًا لِلْهَرَبْ وَلَا تَمْسْكَ.\n",
            "أَطْلَعْتُ سِحْرَ  مِنْ فِيْلْبَيْنٍ بِوَاءْ، وَ...\n",
            "------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "gemmaa2 baaaaaaaaaaaaaaaaaaad"
      ],
      "metadata": {
        "id": "kF6j5kxqktKu"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C2AEkKUek0QB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3ESYf5oIk0Mz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wd9BX65nk0Ja"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4nZ_IN2Jk0Fq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import ollama\n",
        "import os\n",
        "import re\n",
        "import time\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# ------------------- Settings -------------------\n",
        "INPUT_FILE = \"text.txt\"\n",
        "OUTPUT_FILE = \"dracula_final.txt\"\n",
        "\n",
        "# سنعود لـ Phi-3 Medium لأنه أفضل توازن بين الذكاء والسرعة للنصوص الأدبية\n",
        "# النماذج الصغيرة (2B) تفشل في الروايات وتكرر الكلام\n",
        "MODEL_NAME = \"phi3:medium\"\n",
        "\n",
        "CHUNK_SIZE = 1000\n",
        "\n",
        "# ------------------- SYSTEM PROMPT -------------------\n",
        "SYSTEM_PROMPT = (\n",
        "    \"ROLE: Expert Arabic Translator for Novels.\\n\"\n",
        "    \"TASK: Translate the English text to Arabic.\\n\"\n",
        "    \"RULES:\\n\"\n",
        "    \"1. NO REPETITION: Never repeat sentences. If you get stuck, move to the next sentence.\\n\"\n",
        "    \"2. TRANSLITERATE: 'Munich' -> 'ميونخ', 'Dracula' -> 'دراكولا'.\\n\"\n",
        "    \"3. SCRIPT: Arabic only. No English/Japanese characters.\\n\"\n",
        "    \"4. STYLE: Narrative and descriptive.\"\n",
        ")\n",
        "\n",
        "def remove_garbage(text):\n",
        "    \"\"\"حذف التكرار والأسطر الغريبة\"\"\"\n",
        "    lines = text.split('\\n')\n",
        "    cleaned = []\n",
        "    seen = set()\n",
        "    for line in lines:\n",
        "        line = line.strip()\n",
        "        # حذف السطر إذا كان تكراراً لسطر سابق\n",
        "        if line in seen:\n",
        "            continue\n",
        "        # حذف الأسطر التي تحتوي تشكيل مبالغ فيه (علامة الهلوسة)\n",
        "        if len(re.findall(r'[\\u064B-\\u065F]', line)) > len(line) * 0.4:\n",
        "            continue\n",
        "\n",
        "        if line:\n",
        "            seen.add(line)\n",
        "            cleaned.append(line)\n",
        "    return '\\n'.join(cleaned)\n",
        "\n",
        "def split_text_smart(text, max_length):\n",
        "    chunks = []\n",
        "    while len(text) > max_length:\n",
        "        split_index = text.rfind('\\n', 0, max_length)\n",
        "        if split_index == -1: split_index = text.rfind('.', 0, max_length)\n",
        "        if split_index == -1: split_index = max_length\n",
        "        chunks.append(text[:split_index + 1])\n",
        "        text = text[split_index + 1:].strip()\n",
        "    if text: chunks.append(text)\n",
        "    return chunks\n",
        "\n",
        "def translate_segment(text_chunk, retries=3):\n",
        "    for attempt in range(retries):\n",
        "        try:\n",
        "            response = ollama.chat(\n",
        "                model=MODEL_NAME,\n",
        "                messages=[\n",
        "                    {'role': 'system', 'content': SYSTEM_PROMPT},\n",
        "                    {'role': 'user', 'content': text_chunk},\n",
        "                ],\n",
        "                options={\n",
        "                    'temperature': 0.4,     # حرارة متوسطة للإبداع بدون جنون\n",
        "                    'repeat_penalty': 1.3,  # هام جداً: يمنع تكرار الجمل (الحل لمشكلتك)\n",
        "                    'top_k': 50,            # تنويع الكلمات\n",
        "                    'num_predict': 1500,\n",
        "                }\n",
        "            )\n",
        "            content = response['message']['content']\n",
        "            return remove_garbage(content)\n",
        "        except Exception as e:\n",
        "            print(f\"Error: {e}\")\n",
        "            time.sleep(1)\n",
        "    return \"[Translation Failed]\"\n",
        "\n",
        "def main():\n",
        "    if not os.path.exists(INPUT_FILE):\n",
        "        print(\"File not found.\")\n",
        "        return\n",
        "\n",
        "    print(f\"🚀 Pulling {MODEL_NAME}...\")\n",
        "    try:\n",
        "        ollama.pull(MODEL_NAME)\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    with open(INPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "        full_text = f.read()\n",
        "\n",
        "    chunks = split_text_smart(full_text, CHUNK_SIZE)\n",
        "    print(f\"⚡ Translating {len(chunks)} parts...\")\n",
        "\n",
        "    with open(OUTPUT_FILE, \"w\", encoding=\"utf-8\") as f:\n",
        "        f.write(\"\")\n",
        "\n",
        "    for chunk in tqdm(chunks, desc=\"Translating\"):\n",
        "        result = translate_segment(chunk)\n",
        "        with open(OUTPUT_FILE, \"a\", encoding=\"utf-8\") as f:\n",
        "            f.write(result + \"\\n\\n\")\n",
        "\n",
        "    print(f\"✅ Done! Check {OUTPUT_FILE}\")\n",
        "\n",
        "    if os.path.exists(OUTPUT_FILE):\n",
        "        print(\"-\" * 30)\n",
        "        with open(OUTPUT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
        "            print(f.read()[:500] + \"...\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "Gd1iaSckhKox"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}